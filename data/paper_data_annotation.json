[
  {
    "title": "LLaVA-OneVision: Easy Visual Task Transfer",
    "authors": "Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li",
    "abstract": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.",
    "arxiv_id": "2408.03326v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03326v1",
    "abstract_url": "http://arxiv.org/abs/2408.03326v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "ClassiFIM: An Unsupervised Method To Detect Phase Transitions",
    "authors": "Victor Kasatkin, Evgeny Mozgunov, Nicholas Ezzell, Utkarsh Mishra, Itay Hen, Daniel Lidar",
    "abstract": "Estimation of the Fisher Information Metric (FIM-estimation) is an important\ntask that arises in unsupervised learning of phase transitions, a problem\nproposed by physicists. This work completes the definition of the task by\ndefining rigorous evaluation metrics distMSE, distMSEPS, and distRE and\nintroduces ClassiFIM, a novel machine learning method designed to solve the\nFIM-estimation task. Unlike existing methods for unsupervised learning of phase\ntransitions, ClassiFIM directly estimates a well-defined quantity (the FIM),\nallowing it to be rigorously compared to any present and future other methods\nthat estimate the same. ClassiFIM transforms a dataset for the FIM-estimation\ntask into a dataset for an auxiliary binary classification task and involves\nselecting and training a model for the latter. We prove that the output of\nClassiFIM approaches the exact FIM in the limit of infinite dataset size and\nunder certain regularity conditions. We implement ClassiFIM on multiple\ndatasets, including datasets describing classical and quantum phase\ntransitions, and find that it achieves a good ground truth approximation with\nmodest computational resources. Furthermore, we independently implement two\nalternative state-of-the-art methods for unsupervised estimation of phase\ntransition locations on the same datasets and find that ClassiFIM predicts such\nlocations at least as well as these other methods. To emphasize the generality\nof our method, we also propose and generate the MNIST-CNN dataset, which\nconsists of the output of CNNs trained on MNIST for different hyperparameter\nchoices. Using ClassiFIM on this dataset suggests there is a phase transition\nin the distribution of image-prediction pairs for CNNs trained on MNIST,\ndemonstrating the broad scope of FIM-estimation beyond physics.",
    "arxiv_id": "2408.03323v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03323v1",
    "abstract_url": "http://arxiv.org/abs/2408.03323v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Hedge Fund Portfolio Construction Using PolyModel Theory and iTransformer",
    "authors": "Siqiao Zhao, Zhikang Dong, Zeyu Cao, Raphael Douady",
    "abstract": "When constructing portfolios, a key problem is that a lot of financial time\nseries data are sparse, making it challenging to apply machine learning\nmethods. Polymodel theory can solve this issue and demonstrate superiority in\nportfolio construction from various aspects. To implement the PolyModel theory\nfor constructing a hedge fund portfolio, we begin by identifying an asset pool,\nutilizing over 10,000 hedge funds for the past 29 years' data. PolyModel theory\nalso involves choosing a wide-ranging set of risk factors, which includes\nvarious financial indices, currencies, and commodity prices. This comprehensive\nselection mirrors the complexities of the real-world environment. Leveraging on\nthe PolyModel theory, we create quantitative measures such as Long-term Alpha,\nLong-term Ratio, and SVaR. We also use more classical measures like the Sharpe\nratio or Morningstar's MRAR. To enhance the performance of the constructed\nportfolio, we also employ the latest deep learning techniques (iTransformer) to\ncapture the upward trend, while efficiently controlling the downside, using all\nthe features. The iTransformer model is specifically designed to address the\nchallenges in high-dimensional time series forecasting and could largely\nimprove our strategies. More precisely, our strategies achieve better Sharpe\nratio and annualized return. The above process enables us to create multiple\nportfolio strategies aiming for high returns and low risks when compared to\nvarious benchmarks.",
    "arxiv_id": "2408.03320v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03320v1",
    "abstract_url": "http://arxiv.org/abs/2408.03320v1",
    "primary_category": "q-fin.PM",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Training LLMs to Recognize Hedges in Spontaneous Narratives",
    "authors": "Amie J. Paige, Adil Soubki, John Murzaku, Owen Rambow, Susan E. Brennan",
    "abstract": "Hedges allow speakers to mark utterances as provisional, whether to signal\nnon-prototypicality or \"fuzziness\", to indicate a lack of commitment to an\nutterance, to attribute responsibility for a statement to someone else, to\ninvite input from a partner, or to soften critical feedback in the service of\nface-management needs. Here we focus on hedges in an experimentally\nparameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced\nfrom memory by 21 speakers for co-present addressees, transcribed to text\n(Galati and Brennan, 2010). We created a gold standard of hedges annotated by\nhuman coders (the Roadrunner-Hedge corpus) and compared three LLM-based\napproaches for hedge detection: fine-tuning BERT, and zero and few-shot\nprompting with GPT-4o and LLaMA-3. The best-performing approach was a\nfine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on\nthe top performing approaches, we used an LLM-in-the-Loop approach to improve\nthe gold standard coding, as well as to highlight cases in which hedges are\nambiguous in linguistically interesting ways that will guide future research.\nThis is the first step in our research program to train LLMs to interpret and\ngenerate collateral signals appropriately and meaningfully in conversation.",
    "arxiv_id": "2408.03319v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03319v1",
    "abstract_url": "http://arxiv.org/abs/2408.03319v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
    "authors": "Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar",
    "abstract": "Enabling LLMs to improve their outputs by using more test-time computation is\na critical step towards building generally self-improving agents that can\noperate on open-ended natural language. In this paper, we study the scaling of\ninference-time computation in LLMs, with a focus on answering the question: if\nan LLM is allowed to use a fixed but non-trivial amount of inference-time\ncompute, how much can it improve its performance on a challenging prompt?\nAnswering this question has implications not only on the achievable performance\nof LLMs, but also on the future of LLM pretraining and how one should tradeoff\ninference-time and pre-training compute. Despite its importance, little\nresearch attempted to understand the scaling behaviors of various test-time\ninference methods. Moreover, current work largely provides negative results for\na number of these strategies. In this work, we analyze two primary mechanisms\nto scale test-time computation: (1) searching against dense, process-based\nverifier reward models; and (2) updating the model's distribution over a\nresponse adaptively, given the prompt at test time. We find that in both cases,\nthe effectiveness of different approaches to scaling test-time compute\ncritically varies depending on the difficulty of the prompt. This observation\nmotivates applying a \"compute-optimal\" scaling strategy, which acts to most\neffectively allocate test-time compute adaptively per prompt. Using this\ncompute-optimal strategy, we can improve the efficiency of test-time compute\nscaling by more than 4x compared to a best-of-N baseline. Additionally, in a\nFLOPs-matched evaluation, we find that on problems where a smaller base model\nattains somewhat non-trivial success rates, test-time compute can be used to\noutperform a 14x larger model.",
    "arxiv_id": "2408.03314v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03314v1",
    "abstract_url": "http://arxiv.org/abs/2408.03314v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Pre-training and in-context learning IS Bayesian inference a la De Finetti",
    "authors": "Naimeng Ye, Hanming Yang, Andrew Siah, Hongseok Namkoong",
    "abstract": "Accurately gauging uncertainty on the underlying environment is a\nlongstanding goal of intelligent systems. We characterize which latent concepts\npre-trained sequence models are naturally able to reason with. We go back to De\nFinetti's predictive view of Bayesian reasoning: instead of modeling latent\nparameters through priors and likelihoods like topic models do, De Finetti has\nlong advocated for modeling exchangeable (permutation invariant) sequences of\nobservables. According to this view, pre-training autoregressive models\nformulates informed beliefs based on prior observations (\"empirical Bayes\"),\nand forward generation is a simulated instantiation of an environment\n(\"posterior inference\"). This connection allows extending in-context learning\n(ICL) beyond predictive settings, highlighting sequence models' ability to\nperform explicit statistical inference. In particular, we show the sequence\nprediction loss over exchangeable documents controls performance on downstream\ntasks where uncertainty quantification is key. Empirically, we propose and\ndemonstrate several approaches for encoding exchangeability in sequence model\narchitectures: data augmentation, regularization, and causal masking.",
    "arxiv_id": "2408.03307v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03307v1",
    "abstract_url": "http://arxiv.org/abs/2408.03307v1",
    "primary_category": "stat.ML",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks",
    "authors": "Rafael Sterzinger, Christian Stippel, Robert Sablatnig",
    "abstract": "Etruscan mirrors constitute a significant category in Etruscan art,\ncharacterized by elaborate figurative illustrations featured on their backside.\nA laborious and costly aspect of their analysis and documentation is the task\nof manually tracing these illustrations. In previous work, a methodology has\nbeen proposed to automate this process, involving photometric-stereo scanning\nin combination with deep neural networks. While achieving quantitative\nperformance akin to an expert annotator, some results still lack qualitative\nprecision and, thus, require annotators for inspection and potential\ncorrection, maintaining resource intensity. In response, we propose a deep\nneural network trained to interactively refine existing annotations based on\nhuman guidance. Our human-in-the-loop approach streamlines annotation,\nachieving equal quality with up to 75% less manual input required. Moreover,\nduring the refinement process, the relative improvement of our methodology over\npure manual labeling reaches peak values of up to 26%, attaining drastically\nbetter quality quicker. By being tailored to the complex task of segmenting\nintricate lines, specifically distinguishing it from previous methods, our\napproach offers drastic improvements in efficacy, transferable to a broad\nspectrum of applications beyond Etruscan mirrors.",
    "arxiv_id": "2408.03304v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03304v1",
    "abstract_url": "http://arxiv.org/abs/2408.03304v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Understanding How Blind Users Handle Object Recognition Errors: Strategies and Challenges",
    "authors": "Jonggi Hong, Hernisa Kacorri",
    "abstract": "Object recognition technologies hold the potential to support blind and\nlow-vision people in navigating the world around them. However, the gap between\nbenchmark performances and practical usability remains a significant challenge.\nThis paper presents a study aimed at understanding blind users' interaction\nwith object recognition systems for identifying and avoiding errors. Leveraging\na pre-existing object recognition system, URCam, fine-tuned for our experiment,\nwe conducted a user study involving 12 blind and low-vision participants.\nThrough in-depth interviews and hands-on error identification tasks, we gained\ninsights into users' experiences, challenges, and strategies for identifying\nerrors in camera-based assistive technologies and object recognition systems.\nDuring interviews, many participants preferred independent error review, while\nexpressing apprehension toward misrecognitions. In the error identification\ntask, participants varied viewpoints, backgrounds, and object sizes in their\nimages to avoid and overcome errors. Even after repeating the task,\nparticipants identified only half of the errors, and the proportion of errors\nidentified did not significantly differ from their first attempts. Based on\nthese insights, we offer implications for designing accessible interfaces\ntailored to the needs of blind and low-vision users in identifying object\nrecognition errors.",
    "arxiv_id": "2408.03303v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03303v1",
    "abstract_url": "http://arxiv.org/abs/2408.03303v1",
    "primary_category": "cs.HC",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models",
    "authors": "Ruizhe Zhang, Yongxin Xu, Yuzhen Xiao, Runchuan Zhu, Xinke Jiang, Xu Chu, Junfeng Zhao, Yasha Wang",
    "abstract": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has\nbecome an effective strategy for mitigating the hallucination problems that\nlarge language models (LLMs) encounter when dealing with knowledge-intensive\ntasks. However, in the process of integrating external non-parametric\nsupporting evidence with internal parametric knowledge, inevitable knowledge\nconflicts may arise, leading to confusion in the model's responses. To enhance\nthe knowledge selection of LLMs in various contexts, some research has focused\non refining their behavior patterns through instruction-tuning. Nonetheless,\ndue to the absence of explicit negative signals and comparative objectives,\nmodels fine-tuned in this manner may still exhibit undesirable behaviors in the\nintricate and realistic retrieval scenarios. To this end, we propose a\nKnowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving\ncontrollable knowledge selection in real retrieval scenarios. Concretely, we\nexplore and simulate error types across diverse context combinations and learn\nhow to avoid these negative signals through preference optimization methods.\nSimultaneously, by adjusting the balance between response length and the\nproportion of preference data representing different behavior patterns, we\nenhance the adherence capabilities and noise robustness of LLMs in a balanced\nmanner. Experimental results show that KaPO outperforms previous methods for\nhandling knowledge conflicts by over 37%, while also exhibiting robust\ngeneralization across various out-of-distribution datasets.",
    "arxiv_id": "2408.03297v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03297v1",
    "abstract_url": "http://arxiv.org/abs/2408.03297v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Static IR Drop Prediction with Attention U-Net and Saliency-Based Explainability",
    "authors": "Lizi Zhang, Azadeh Davoodi",
    "abstract": "There has been significant recent progress to reduce the computational effort\nof static IR drop analysis using neural networks, and modeling as an\nimage-to-image translation task. A crucial issue is the lack of sufficient data\nfrom real industry designs to train these networks. Additionally, there is no\nmethodology to explain a high-drop pixel in a predicted IR drop image to its\nspecific root-causes. In this work, we first propose a U-Net neural network\nmodel with attention gates which is specifically tailored to achieve fast and\naccurate image-based static IR drop prediction. Attention gates allow selective\nemphasis on relevant parts of the input data without supervision which is\ndesired because of the often sparse nature of the IR drop map. We propose a\ntwo-phase training process which utilizes a mix of artificially-generated data\nand a limited number of points from real designs. The results are, on-average,\n18% (53%) better in MAE and 14% (113%) in F1 score compared to the winner of\nthe ICCAD 2023 contest (and U-Net only) when tested on real designs. Second, we\npropose a fast method using saliency maps which can explain a predicted IR drop\nin terms of specific input pixels contributing the most to a drop. In our\nexperiments, we show the number of high IR drop pixels can be reduced\non-average by 18% by mimicking upsize of a tiny portion of PDN's resistive\nedges.",
    "arxiv_id": "2408.03292v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03292v1",
    "abstract_url": "http://arxiv.org/abs/2408.03292v1",
    "primary_category": "cs.AR",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "SARA: Singular-Value Based Adaptive Low-Rank Adaption",
    "authors": "Jihao Gu, Shuai Chen, Zelin Wang, Yibo Zhang, Ping Gong",
    "abstract": "With the increasing number of parameters in large pre-trained models, LoRA as\na parameter-efficient fine-tuning(PEFT) method is widely used for not adding\ninference overhead. The LoRA method assumes that weight changes during\nfine-tuning can be approximated by low-rank matrices. However, the rank values\nneed to be manually verified to match different downstream tasks, and they\ncannot accommodate the varying importance of different layers in the model. In\nthis work, we first analyze the relationship between the performance of\ndifferent layers and their ranks using SVD. Based on this, we design the\nSingular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds\nthe rank during initialization by performing SVD on the pre-trained weights.\nAdditionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly\nreduces the number of parameters by fine-tuning only multiple parallel sets of\nsingular values controlled by a router. Extensive experiments on various\ncomplex tasks demonstrate the simplicity and parameter efficiency of our\nmethods. They can effectively and adaptively find the most suitable rank for\neach layer of each model.",
    "arxiv_id": "2408.03290v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03290v1",
    "abstract_url": "http://arxiv.org/abs/2408.03290v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Malicious Internet Entity Detection Using Local Graph Inference",
    "authors": "Simon Mandlik, Tomas Pevny, Vaclav Smidl, Lukas Bajer",
    "abstract": "Detection of malicious behavior in a large network is a challenging problem\nfor machine learning in computer security, since it requires a model with high\nexpressive power and scalable inference. Existing solutions struggle to achieve\nthis feat -- current cybersec-tailored approaches are still limited in\nexpressivity, and methods successful in other domains do not scale well for\nlarge volumes of data, rendering frequent retraining impossible. This work\nproposes a new perspective for learning from graph data that is modeling\nnetwork entity interactions as a large heterogeneous graph. High expressivity\nof the method is achieved with neural network architecture HMILnet that\nnaturally models this type of data and provides theoretical guarantees. The\nscalability is achieved by pursuing local graph inference, i.e., classifying\nindividual vertices and their neighborhood as independent samples. Our\nexperiments exhibit improvement over the state-of-the-art Probabilistic Threat\nPropagation (PTP) algorithm, show a further threefold accuracy improvement when\nadditional data is used, which is not possible with the PTP algorithm, and\ndemonstrate the generalization capabilities of the method to new, previously\nunseen entities.",
    "arxiv_id": "2408.03287v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03287v1",
    "abstract_url": "http://arxiv.org/abs/2408.03287v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation",
    "authors": "Boxi Cao, Mengjie Ren, Hongyu Lin, Xianpei Han, Feng Zhang, Junfeng Zhan, Le Sun",
    "abstract": "Evaluation is the baton for the development of large language models. Current\nevaluations typically employ a single-item assessment paradigm for each atomic\ntest objective, which struggles to discern whether a model genuinely possesses\nthe required capabilities or merely memorizes/guesses the answers to specific\nquestions. To this end, we propose a novel evaluation framework referred to as\nStructEval. Starting from an atomic test objective, StructEval deepens and\nbroadens the evaluation by conducting a structured assessment across multiple\ncognitive levels and critical concepts, and therefore offers a comprehensive,\nrobust and consistent evaluation for LLMs. Experiments on three widely-used\nbenchmarks demonstrate that StructEval serves as a reliable tool for resisting\nthe risk of data contamination and reducing the interference of potential\nbiases, thereby providing more reliable and consistent conclusions regarding\nmodel capabilities. Our framework also sheds light on the design of future\nprincipled and trustworthy LLM evaluation protocols.",
    "arxiv_id": "2408.03281v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03281v1",
    "abstract_url": "http://arxiv.org/abs/2408.03281v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments",
    "authors": "Angie Boggust, Venkatesh Sivaraman, Yannick Assogba, Donghao Ren, Dominik Moritz, Fred Hohman",
    "abstract": "To deploy machine learning models on-device, practitioners use compression\nalgorithms to shrink and speed up models while maintaining their high-quality\noutput. A critical aspect of compression in practice is model comparison,\nincluding tracking many compression experiments, identifying subtle changes in\nmodel behavior, and negotiating complex accuracy-efficiency trade-offs.\nHowever, existing compression tools poorly support comparison, leading to\ntedious and, sometimes, incomplete analyses spread across disjoint tools. To\nsupport real-world comparative workflows, we develop an interactive visual\nsystem called Compress and Compare. Within a single interface, Compress and\nCompare surfaces promising compression strategies by visualizing provenance\nrelationships between compressed models and reveals compression-induced\nbehavior changes by comparing models' predictions, weights, and activations. We\ndemonstrate how Compress and Compare supports common compression analysis tasks\nthrough two case studies, debugging failed compression on generative language\nmodels and identifying compression artifacts in image classification models. We\nfurther evaluate Compress and Compare in a user study with eight compression\nexperts, illustrating its potential to provide structure to compression\nworkflows, help practitioners build intuition about compression, and encourage\nthorough analysis of compression's effect on model behavior. Through these\nevaluations, we identify compression-specific challenges that future visual\nanalytics tools should consider and Compress and Compare visualizations that\nmay generalize to broader model comparison tasks.",
    "arxiv_id": "2408.03274v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03274v1",
    "abstract_url": "http://arxiv.org/abs/2408.03274v1",
    "primary_category": "cs.HC",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons",
    "authors": "Yifei Wang, Yuheng Chen, Wanting Wen, Yu Sheng, Linjing Li, Daniel Dajun Zeng",
    "abstract": "In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon.",
    "arxiv_id": "2408.03247v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03247v1",
    "abstract_url": "http://arxiv.org/abs/2408.03247v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Analysis of Partially-Calibrated Sparse Subarrays for Direction Finding with Extended Degrees of Freedom",
    "authors": "W. S. Leite, R. C. de Lamare",
    "abstract": "This paper investigates the problem of direction-of-arrival (DOA) estimation\nusing multiple partially-calibrated sparse subarrays. In particular, we present\nthe Generalized Coarray Multiple Signal Classification (GCA-MUSIC) DOA\nestimation algorithm to scenarios with partially-calibrated sparse subarrays.\nThe proposed GCA-MUSIC algorithm exploits the difference coarray for each\nsubarray, followed by a specific pseudo-spectrum merging rule that is based on\nthe intersection of the signal subspaces associated to each subarray. This rule\nassumes that there is no a priori knowledge about the cross-covariance between\nsubarrays. In that way, only the second-order statistics of each subarray are\nused to estimate the directions with increased degrees of freedom, i.e., the\nestimation procedure preserves the coarray Multiple Signal Classification and\nsparse arrays properties to estimate more sources than the number of physical\nsensors in each subarray. Numerical simulations show that the proposed\nGCA-MUSIC has better performance than other similar strategies.",
    "arxiv_id": "2408.03236v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03236v1",
    "abstract_url": "http://arxiv.org/abs/2408.03236v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Don't Think It Twice: Exploit Shift Invariance for Efficient Online Streaming Inference of CNNs",
    "authors": "Christodoulos Kechris, Jonathan Dan, Jose Miranda, David Atienza",
    "abstract": "Deep learning time-series processing often relies on convolutional neural\nnetworks with overlapping windows. This overlap allows the network to produce\nan output faster than the window length. However, it introduces additional\ncomputations. This work explores the potential to optimize computational\nefficiency during inference by exploiting convolution's shift-invariance\nproperties to skip the calculation of layer activations between successive\noverlapping windows. Although convolutions are shift-invariant, zero-padding\nand pooling operations, widely used in such networks, are not efficient and\ncomplicate efficient streaming inference. We introduce StreamiNNC, a strategy\nto deploy Convolutional Neural Networks for online streaming inference. We\nexplore the adverse effects of zero padding and pooling on the accuracy of\nstreaming inference, deriving theoretical error upper bounds for pooling during\nstreaming. We address these limitations by proposing signal padding and pooling\nalignment and provide guidelines for designing and deploying models for\nStreamiNNC. We validate our method in simulated data and on three real-world\nbiomedical signal processing applications. StreamiNNC achieves a low deviation\nbetween streaming output and normal inference for all three networks (2.03 -\n3.55% NRMSE). This work demonstrates that it is possible to linearly speed up\nthe inference of streaming CNNs processing overlapping windows, negating the\nadditional computation typically incurred by overlapping windows.",
    "arxiv_id": "2408.03223v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03223v1",
    "abstract_url": "http://arxiv.org/abs/2408.03223v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Masked Random Noise for Communication Efficient Federaetd Learning",
    "authors": "Shiwei Li, Yingyi Cheng, Haozhao Wang, Xing Tang, Shijie Xu, Weihong Luo, Yuhua Li, Dugang Liu, Xiuqiang He, and Ruixuan Li",
    "abstract": "Federated learning is a promising distributed training paradigm that\neffectively safeguards data privacy. However, it may involve significant\ncommunication costs, which hinders training efficiency. In this paper, we aim\nto enhance communication efficiency from a new perspective. Specifically, we\nrequest the distributed clients to find optimal model updates relative to\nglobal model parameters within predefined random noise. For this purpose, we\npropose Federated Masked Random Noise (FedMRN), a novel framework that enables\nclients to learn a 1-bit mask for each model parameter and apply masked random\nnoise (i.e., the Hadamard product of random noise and masks) to represent model\nupdates. To make FedMRN feasible, we propose an advanced mask training\nstrategy, called progressive stochastic masking (PSM). After local training,\neach client only need to transmit local masks and a random seed to the server.\nAdditionally, we provide theoretical guarantees for the convergence of FedMRN\nunder both strongly convex and non-convex assumptions. Extensive experiments\nare conducted on four popular datasets. The results show that FedMRN exhibits\nsuperior convergence speed and test accuracy compared to relevant baselines,\nwhile attaining a similar level of accuracy as FedAvg.",
    "arxiv_id": "2408.03220v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03220v1",
    "abstract_url": "http://arxiv.org/abs/2408.03220v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Learning to Learn without Forgetting using Attention",
    "authors": "Anna Vettoruzzo, Joaquin Vanschoren, Mohamed-Rafik Bouguelia, Thorsteinn R\u00f6gnvaldsson",
    "abstract": "Continual learning (CL) refers to the ability to continually learn over time\nby accommodating new knowledge while retaining previously learned experience.\nWhile this concept is inherent in human learning, current machine learning\nmethods are highly prone to overwrite previously learned patterns and thus\nforget past experience. Instead, model parameters should be updated selectively\nand carefully, avoiding unnecessary forgetting while optimally leveraging\npreviously learned patterns to accelerate future learning. Since hand-crafting\neffective update mechanisms is difficult, we propose meta-learning a\ntransformer-based optimizer to enhance CL. This meta-learned optimizer uses\nattention to learn the complex relationships between model parameters across a\nstream of tasks, and is designed to generate effective weight updates for the\ncurrent task while preventing catastrophic forgetting on previously encountered\ntasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and\nSplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both\nforward and backward transfer, even on small sets of labeled data, highlighting\nthe advantages of integrating a meta-learned optimizer within the continual\nlearning framework.",
    "arxiv_id": "2408.03219v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03219v1",
    "abstract_url": "http://arxiv.org/abs/2408.03219v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "FedBAT: Communication-Efficient Federated Learning via Learnable Binarization",
    "authors": "Shiwei Li, Wenchao Xu, Haozhao Wang, Xing Tang, Yining Qi, Shijie Xu, Weihong Luo, Yuhua Li, Xiuqiang He, Ruixuan Li",
    "abstract": "Federated learning is a promising distributed machine learning paradigm that\ncan effectively exploit large-scale data without exposing users' privacy.\nHowever, it may incur significant communication overhead, thereby potentially\nimpairing the training efficiency. To address this challenge, numerous studies\nsuggest binarizing the model updates. Nonetheless, traditional methods usually\nbinarize model updates in a post-training manner, resulting in significant\napproximation errors and consequent degradation in model accuracy. To this end,\nwe propose Federated Binarization-Aware Training (FedBAT), a novel framework\nthat directly learns binary model updates during the local training process,\nthus inherently reducing the approximation errors. FedBAT incorporates an\ninnovative binarization operator, along with meticulously designed derivatives\nto facilitate efficient learning. In addition, we establish theoretical\nguarantees regarding the convergence of FedBAT. Extensive experiments are\nconducted on four popular datasets. The results show that FedBAT significantly\naccelerates the convergence and exceeds the accuracy of baselines by up to 9\\%,\neven surpassing that of FedAvg in some cases.",
    "arxiv_id": "2408.03215v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03215v1",
    "abstract_url": "http://arxiv.org/abs/2408.03215v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Personalizing Federated Instrument Segmentation with Visual Trait Priors in Robotic Surgery",
    "authors": "Jialang Xu, Jiacheng Wang, Lequan Yu, Danail Stoyanov, Yueming Jin, Evangelos B. Mazomenos",
    "abstract": "Personalized federated learning (PFL) for surgical instrument segmentation\n(SIS) is a promising approach. It enables multiple clinical sites to\ncollaboratively train a series of models in privacy, with each model tailored\nto the individual distribution of each site. Existing PFL methods rarely\nconsider the personalization of multi-headed self-attention, and do not account\nfor appearance diversity and instrument shape similarity, both inherent in\nsurgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait\npriors for SIS, incorporating global-personalized disentanglement (GPD),\nappearance-regulation personalized enhancement (APE), and shape-similarity\nglobal enhancement (SGE), to boost SIS performance in each site. GPD represents\nthe first attempt at head-wise assignment for multi-headed self-attention\npersonalization. To preserve the unique appearance representation of each site\nand gradually leverage the inter-site difference, APE introduces appearance\nregulation and provides customized layer-wise aggregation solutions via\nhypernetworks for each site's personalized parameters. The mutual shape\ninformation of instruments is maintained and shared via SGE, which enhances the\ncross-style shape consistency on the image level and computes the\nshape-similarity contribution of each site on the prediction level for updating\nthe global parameters. PFedSIS outperforms state-of-the-art methods with +1.51%\nDice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding\ncode and models will be released at https://github.com/wzjialang/PFedSIS.",
    "arxiv_id": "2408.03208v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03208v1",
    "abstract_url": "http://arxiv.org/abs/2408.03208v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Adversarial Safety-Critical Scenario Generation using Naturalistic Human Driving Priors",
    "authors": "Kunkun Hao, Yonggang Luo, Wen Cui, Yuqiao Bai, Jucheng Yang, Songyang Yan, Yuxi Pan, Zijiang Yang",
    "abstract": "Evaluating the decision-making system is indispensable in developing\nautonomous vehicles, while realistic and challenging safety-critical test\nscenarios play a crucial role. Obtaining these scenarios is non-trivial, thanks\nto the long-tailed distribution, sparsity, and rarity in real-world data sets.\nTo tackle this problem, in this paper, we introduce a natural adversarial\nscenario generation solution using naturalistic human driving priors and\nreinforcement learning techniques. By doing this, we can obtain large-scale\ntest scenarios that are both diverse and realistic. Specifically, we build a\nsimulation environment that mimics natural traffic interaction scenarios.\nInformed by this environment, we implement a two-stage procedure. The first\nstage incorporates conventional rule-based models, e.g., IDM~(Intelligent\nDriver Model) and MOBIL~(Minimizing Overall Braking Induced by Lane changes)\nmodel, to coarsely and discretely capture and calibrate key control parameters\nfrom the real-world dataset. Next, we leverage GAIL~(Generative Adversarial\nImitation Learning) to represent driver behaviors continuously. The derived\nGAIL can be further used to design a PPO~(Proximal Policy Optimization)-based\nactor-critic network framework to fine-tune the reward function, and then\noptimizes our natural adversarial scenario generation solution. Extensive\nexperiments have been conducted in the NGSIM dataset including the trajectory\nof 3,000 vehicles. Essential traffic parameters were measured in comparison\nwith the baseline model, e.g., the collision rate, accelerations, steering, and\nthe number of lane changes. Our findings demonstrate that the proposed model\ncan generate realistic safety-critical test scenarios covering both naturalness\nand adversariality, which can be a cornerstone for the development of\nautonomous vehicles.",
    "arxiv_id": "2408.03200v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03200v1",
    "abstract_url": "http://arxiv.org/abs/2408.03200v1",
    "primary_category": "cs.RO",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Convergence Conditions for Stochastic Line Search Based Optimization of Over-parametrized Models",
    "authors": "Matteo Lapucci, Davide Pucci",
    "abstract": "In this paper, we deal with algorithms to solve the finite-sum problems\nrelated to fitting over-parametrized models, that typically satisfy the\ninterpolation condition. In particular, we focus on approaches based on\nstochastic line searches and employing general search directions. We define\nconditions on the sequence of search directions that guarantee finite\ntermination and bounds for the backtracking procedure. Moreover, we shed light\non the additional property of directions needed to prove fast (linear)\nconvergence of the general class of algorithms when applied to PL functions in\nthe interpolation regime. From the point of view of algorithms design, the\nproposed analysis identifies safeguarding conditions that could be employed in\nrelevant algorithmic framework. In particular, it could be of interest to\nintegrate stochastic line searches within momentum, conjugate gradient or\nadaptive preconditioning methods.",
    "arxiv_id": "2408.03199v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03199v1",
    "abstract_url": "http://arxiv.org/abs/2408.03199v1",
    "primary_category": "math.OC",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "RELIEF: Reinforcement Learning Empowered Graph Feature Prompt Tuning",
    "authors": "Jiapeng Zhu, Zichen Ding, Jianxiang Yu, Jiaqi Tan, Xiang Li, Weining Qian",
    "abstract": "The advent of the \"pre-train, prompt\" paradigm has recently extended its\ngeneralization ability and data efficiency to graph representation learning,\nfollowing its achievements in Natural Language Processing (NLP). Initial graph\nprompt tuning approaches tailored specialized prompting functions for Graph\nNeural Network (GNN) models pre-trained with specific strategies, such as edge\nprediction, thus limiting their applicability. In contrast, another pioneering\nline of research has explored universal prompting via adding prompts to the\ninput graph's feature space, thereby removing the reliance on specific\npre-training strategies. However, the necessity to add feature prompts to all\nnodes remains an open question. Motivated by findings from prompt tuning\nresearch in the NLP domain, which suggest that highly capable pre-trained\nmodels need less conditioning signal to achieve desired behaviors, we advocate\nfor strategically incorporating necessary and lightweight feature prompts to\ncertain graph nodes to enhance downstream task performance. This introduces a\ncombinatorial optimization problem, requiring a policy to decide 1) which nodes\nto prompt and 2) what specific feature prompts to attach. We then address the\nproblem by framing the prompt incorporation process as a sequential\ndecision-making problem and propose our method, RELIEF, which employs\nReinforcement Learning (RL) to optimize it. At each step, the RL agent selects\na node (discrete action) and determines the prompt content (continuous action),\naiming to maximize cumulative performance gain. Extensive experiments on graph\nand node-level tasks with various pre-training strategies in few-shot scenarios\ndemonstrate that our RELIEF outperforms fine-tuning and other prompt-based\napproaches in classification performance and data efficiency.",
    "arxiv_id": "2408.03195v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03195v1",
    "abstract_url": "http://arxiv.org/abs/2408.03195v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "An Object is Worth 64x64 Pixels: Generating 3D Object via Image Diffusion",
    "authors": "Xingguang Yan, Han-Hung Lee, Ziyu Wan, Angel X. Chang",
    "abstract": "We introduce a new approach for generating realistic 3D models with UV maps\nthrough a representation termed \"Object Images.\" This approach encapsulates\nsurface geometry, appearance, and patch structures within a 64x64 pixel image,\neffectively converting complex 3D shapes into a more manageable 2D format. By\ndoing so, we address the challenges of both geometric and semantic irregularity\ninherent in polygonal meshes. This method allows us to use image generation\nmodels, such as Diffusion Transformers, directly for 3D shape generation.\nEvaluated on the ABO dataset, our generated shapes with patch structures\nachieve point cloud FID comparable to recent 3D generative models, while\nnaturally supporting PBR material generation.",
    "arxiv_id": "2408.03178v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03178v1",
    "abstract_url": "http://arxiv.org/abs/2408.03178v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Leveraging Parameter Efficient Training Methods for Low Resource Text Classification: A Case Study in Marathi",
    "authors": "Pranita Deshmukh, Nikita Kulkarni, Sanhita Kulkarni, Kareena Manghani, Raviraj Joshi",
    "abstract": "With the surge in digital content in low-resource languages, there is an\nescalating demand for advanced Natural Language Processing (NLP) techniques\ntailored to these languages. BERT (Bidirectional Encoder Representations from\nTransformers), serving as the foundational framework for numerous NLP\narchitectures and language models, is increasingly employed for the development\nof low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a method\nfor fine-tuning Large Language Models (LLMs) and reducing the training\nparameters to some extent to decrease the computational costs needed for\ntraining the model and achieve results comparable to a fully fine-tuned model.\nIn this work, we present a study of PEFT methods for the Indic low-resource\nlanguage Marathi. We conduct a comprehensive analysis of PEFT methods applied\nto various monolingual and multilingual Marathi BERT models. These approaches\nare evaluated on prominent text classification datasets like MahaSent,\nMahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated to\nsignificantly expedite the training speed of the models, addressing a critical\naspect of model development and deployment. In this study, we explore Low-Rank\nAdaptation of Large Language Models (LoRA) and adapter methods for low-resource\ntext classification. We show that these methods are competitive with full\nfine-tuning and can be used without loss in accuracy. This study contributes\nvaluable insights into the effectiveness of Marathi BERT models, offering a\nfoundation for the continued advancement of NLP capabilities in Marathi and\nsimilar Indic languages.",
    "arxiv_id": "2408.03172v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03172v1",
    "abstract_url": "http://arxiv.org/abs/2408.03172v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Training on the Fly: On-device Self-supervised Learning aboard Nano-drones within 20 mW",
    "authors": "Elia Cereda, Alessandro Giusti, Daniele Palossi",
    "abstract": "Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning\n(TinyML), such as nano-drones, are becoming an increasingly attractive\ntechnology. Their small form factor (i.e., ~10cm diameter) ensures vast\napplicability, ranging from the exploration of narrow disaster scenarios to\nsafe human-robot interaction. Simple electronics make these CPSes inexpensive,\nbut strongly limit the computational, memory, and sensing resources available\non board. In real-world applications, these limitations are further exacerbated\nby domain shift. This fundamental machine learning problem implies that model\nperception performance drops when moving from the training domain to a\ndifferent deployment one. To cope with and mitigate this general problem, we\npresent a novel on-device fine-tuning approach that relies only on the limited\nultra-low power resources available aboard nano-drones. Then, to overcome the\nlack of ground-truth training labels aboard our CPS, we also employ a\nself-supervised method based on ego-motion consistency. Albeit our work builds\non top of a specific real-world vision-based human pose estimation task, it is\nwidely applicable for many embedded TinyML use cases. Our 512-image on-device\ntraining procedure is fully deployed aboard an ultra-low power GWT GAP9\nSystem-on-Chip and requires only 1MB of memory while consuming as low as 19mW\nor running in just 510ms (at 38mW). Finally, we demonstrate the benefits of our\non-device learning approach by field-testing our closed-loop CPS, showing a\nreduction in horizontal position error of up to 26% vs. a non-fine-tuned\nstate-of-the-art baseline. In the most challenging never-seen-before\nenvironment, our on-device learning procedure makes the difference between\nsucceeding or failing the mission.",
    "arxiv_id": "2408.03168v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03168v1",
    "abstract_url": "http://arxiv.org/abs/2408.03168v1",
    "primary_category": "cs.RO",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Dilated Convolution with Learnable Spacings makes visual models more aligned with humans: a Grad-CAM study",
    "authors": "Rabih Chamas, Ismail Khalfaoui-Hassani, Timothee Masquelier",
    "abstract": "Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced\nconvolution method that allows enlarging the receptive fields (RF) without\nincreasing the number of parameters, like the dilated convolution, yet without\nimposing a regular grid. DCLS has been shown to outperform the standard and\ndilated convolutions on several computer vision benchmarks. Here, we show that,\nin addition, DCLS increases the models' interpretability, defined as the\nalignment with human visual strategies. To quantify it, we use the Spearman\ncorrelation between the models' GradCAM heatmaps and the ClickMe dataset\nheatmaps, which reflect human visual attention. We took eight reference models\n- ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and\n36) - and drop-in replaced the standard convolution layers with DCLS ones. This\nimproved the interpretability score in seven of them. Moreover, we observed\nthat Grad-CAM generated random heatmaps for two models in our study: CAFormer\nand ConvFormer models, leading to low interpretability scores. We addressed\nthis issue by introducing Threshold-Grad-CAM, a modification built on top of\nGrad-CAM that enhanced interpretability across nearly all models. The code and\ncheckpoints to reproduce this study are available at:\nhttps://github.com/rabihchamas/DCLS-GradCAM-Eval.",
    "arxiv_id": "2408.03164v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03164v1",
    "abstract_url": "http://arxiv.org/abs/2408.03164v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Iterative CT Reconstruction via Latent Variable Optimization of Shallow Diffusion Models",
    "authors": "Sho Ozaki, Shizuo Kaji, Toshikazu Imae, Kanabu Nawa, Hideomi Yamashita, Keiichi Nakagawa",
    "abstract": "Image generative AI has garnered significant attention in recent years. In\nparticular, the diffusion model, a core component of recent generative AI,\nproduces high-quality images with rich diversity. In this study, we propose a\nnovel CT reconstruction method by combining the denoising diffusion\nprobabilistic model with iterative CT reconstruction. In sharp contrast to\nprevious studies, we optimize the fidelity loss of CT reconstruction with\nrespect to the latent variable of the diffusion model, instead of the image and\nmodel parameters. To suppress anatomical structure changes produced by the\ndiffusion model, we shallow the diffusion and reverse processes, and fix a set\nof added noises in the reverse process to make it deterministic during\ninference. We demonstrate the effectiveness of the proposed method through\nsparse view CT reconstruction of 1/10 view projection data. Despite the\nsimplicity of the implementation, the proposed method shows the capability of\nreconstructing high-quality images while preserving the patient's anatomical\nstructure, and outperforms existing methods including iterative reconstruction,\niterative reconstruction with total variation, and the diffusion model alone in\nterms of quantitative indices such as SSIM and PSNR. We also explore further\nsparse view CT using 1/20 view projection data with the same trained diffusion\nmodel. As the number of iterations increases, image quality improvement\ncomparable to that of 1/10 sparse view CT reconstruction is achieved. In\nprinciple, the proposed method can be widely applied not only to CT but also to\nother imaging modalities such as MRI, PET, and SPECT.",
    "arxiv_id": "2408.03156v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03156v1",
    "abstract_url": "http://arxiv.org/abs/2408.03156v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "TSC: A Simple Two-Sided Constraint against Over-Smoothing",
    "authors": "Furong Peng, Kang Liu, Xuan Lu, Yuhua Qian, Hongren Yan, Chao Ma",
    "abstract": "Graph Convolutional Neural Network (GCN), a widely adopted method for\nanalyzing relational data, enhances node discriminability through the\naggregation of neighboring information. Usually, stacking multiple layers can\nimprove the performance of GCN by leveraging information from high-order\nneighbors. However, the increase of the network depth will induce the\nover-smoothing problem, which can be attributed to the quality and quantity of\nneighbors changing: (a) neighbor quality, node's neighbors become overlapping\nin high order, leading to aggregated information becoming indistinguishable,\n(b) neighbor quantity, the exponentially growing aggregated neighbors submerges\nthe node's initial feature by recursively aggregating operations. Current\nsolutions mainly focus on one of the above causes and seldom consider both at\nonce.\n  Aiming at tackling both causes of over-smoothing in one shot, we introduce a\nsimple Two-Sided Constraint (TSC) for GCNs, comprising two straightforward yet\npotent techniques: random masking and contrastive constraint. The random\nmasking acts on the representation matrix's columns to regulate the degree of\ninformation aggregation from neighbors, thus preventing the convergence of node\nrepresentations. Meanwhile, the contrastive constraint, applied to the\nrepresentation matrix's rows, enhances the discriminability of the nodes.\nDesigned as a plug-in module, TSC can be easily coupled with GCN or SGC\narchitectures. Experimental analyses on diverse real-world graph datasets\nverify that our approach markedly reduces the convergence of node's\nrepresentation and the performance degradation in deeper GCN.",
    "arxiv_id": "2408.03152v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03152v1",
    "abstract_url": "http://arxiv.org/abs/2408.03152v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Conditioning LLMs with Emotion in Neural Machine Translation",
    "authors": "Charles Brazier, Jean-Luc Rouas",
    "abstract": "Large Language Models (LLMs) have shown remarkable performance in Natural\nLanguage Processing tasks, including Machine Translation (MT). In this work, we\npropose a novel MT pipeline that integrates emotion information extracted from\na Speech Emotion Recognition (SER) model into LLMs to enhance translation\nquality. We first fine-tune five existing LLMs on the Libri-trans dataset and\nselect the most performant model. Subsequently, we augment LLM prompts with\ndifferent dimensional emotions and train the selected LLM under these different\nconfigurations. Our experiments reveal that integrating emotion information,\nespecially arousal, into LLM prompts leads to notable improvements in\ntranslation quality.",
    "arxiv_id": "2408.03150v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03150v1",
    "abstract_url": "http://arxiv.org/abs/2408.03150v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Active Learning for Level Set Estimation Using Randomized Straddle Algorithms",
    "authors": "Yu Inatsu, Shion Takeno, Kentaro Kutsukake, Ichiro Takeuchi",
    "abstract": "Level set estimation (LSE), the problem of identifying the set of input\npoints where a function takes value above (or below) a given threshold, is\nimportant in practical applications. When the function is expensive-to-evaluate\nand black-box, the \\textit{straddle} algorithm, which is a representative\nheuristic for LSE based on Gaussian process models, and its extensions having\ntheoretical guarantees have been developed. However, many of existing methods\ninclude a confidence parameter $\\beta^{1/2}_t$ that must be specified by the\nuser, and methods that choose $\\beta^{1/2}_t$ heuristically do not provide\ntheoretical guarantees. In contrast, theoretically guaranteed values of\n$\\beta^{1/2}_t$ need to be increased depending on the number of iterations and\ncandidate points, and are conservative and not good for practical performance.\nIn this study, we propose a novel method, the \\textit{randomized straddle}\nalgorithm, in which $\\beta_t$ in the straddle algorithm is replaced by a random\nsample from the chi-squared distribution with two degrees of freedom. The\nconfidence parameter in the proposed method has the advantages of not needing\nadjustment, not depending on the number of iterations and candidate points, and\nnot being conservative. Furthermore, we show that the proposed method has\ntheoretical guarantees that depend on the sample complexity and the number of\niterations. Finally, we confirm the usefulness of the proposed method through\nnumerical experiments using synthetic and real data.",
    "arxiv_id": "2408.03144v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03144v1",
    "abstract_url": "http://arxiv.org/abs/2408.03144v1",
    "primary_category": "stat.ML",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "COMMENTATOR: A Code-mixed Multilingual Text Annotation Framework",
    "authors": "Rajvee Sheth, Shubh Nisar, Heenaben Prajapati, Himanshu Beniwal, Mayank Singh",
    "abstract": "As the NLP community increasingly addresses challenges associated with\nmultilingualism, robust annotation tools are essential to handle multilingual\ndatasets efficiently. In this paper, we introduce a code-mixed multilingual\ntext annotation framework, COMMENTATOR, specifically designed for annotating\ncode-mixed text. The tool demonstrates its effectiveness in token-level and\nsentence-level language annotation tasks for Hinglish text. We perform robust\nqualitative human-based evaluations to showcase COMMENTATOR led to 5x faster\nannotations than the best baseline. Our code is publicly available at\n\\url{https://github.com/lingo-iitgn/commentator}. The demonstration video is\navailable at \\url{https://bit.ly/commentator_video}.",
    "arxiv_id": "2408.03125v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03125v1",
    "abstract_url": "http://arxiv.org/abs/2408.03125v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Evaluating the Translation Performance of Large Language Models Based on Euas-20",
    "authors": "Yan Huang, Wei Liu",
    "abstract": "In recent years, with the rapid development of deep learning technology,\nlarge language models (LLMs) such as BERT and GPT have achieved breakthrough\nresults in natural language processing tasks. Machine translation (MT), as one\nof the core tasks of natural language processing, has also benefited from the\ndevelopment of large language models and achieved a qualitative leap. Despite\nthe significant progress in translation performance achieved by large language\nmodels, machine translation still faces many challenges. Therefore, in this\npaper, we construct the dataset Euas-20 to evaluate the performance of large\nlanguage models on translation tasks, the translation ability on different\nlanguages, and the effect of pre-training data on the translation ability of\nLLMs for researchers and developers.",
    "arxiv_id": "2408.03119v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03119v1",
    "abstract_url": "http://arxiv.org/abs/2408.03119v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Topic Modeling with Fine-tuning LLMs and Bag of Sentences",
    "authors": "Johannes Schneider",
    "abstract": "Large language models (LLM)'s are increasingly used for topic modeling\noutperforming classical topic models such as LDA. Commonly, pre-trained LLM\nencoders such as BERT are used out-of-the-box despite the fact that fine-tuning\nis known to improve LLMs considerably. The challenge lies in obtaining a\nsuitable (labeled) dataset for fine-tuning. In this paper, we use the recent\nidea to use bag of sentences as the elementary unit in computing topics. In\nturn, we derive an approach FT-Topic to perform unsupervised fine-tuning\nrelying primarily on two steps for constructing a training dataset in an\nautomatic fashion. First, a heuristic method to identifies pairs of sentence\ngroups that are either assumed to be of the same or different topics. Second,\nwe remove sentence pairs that are likely labeled incorrectly. The dataset is\nthen used to fine-tune an encoder LLM, which can be leveraged by any topic\nmodeling approach using embeddings. However, in this work, we demonstrate its\neffectiveness by deriving a novel state-of-the-art topic modeling method called\nSenClu, which achieves fast inference through an expectation-maximization\nalgorithm and hard assignments of sentence groups to a single topic, while\ngiving users the possibility to encode prior knowledge on the topic-document\ndistribution. Code is at \\url{https://github.com/JohnTailor/FT-Topic}",
    "arxiv_id": "2408.03099v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03099v1",
    "abstract_url": "http://arxiv.org/abs/2408.03099v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Learning Provably Robust Policies in Uncertain Parametric Environments",
    "authors": "Yannik Schnitzer, Alessandro Abate, David Parker",
    "abstract": "We present a data-driven approach for learning MDP policies that are robust\nacross stochastic environments whose transition probabilities are defined by\nparameters with an unknown distribution. We produce probably approximately\ncorrect (PAC) guarantees for the performance of these learned policies in a\nnew, unseen environment over the unknown distribution. Our approach is based on\nfinite samples of the MDP environments, for each of which we build an\napproximation of the model as an interval MDP, by exploring a set of generated\ntrajectories. We use the built approximations to synthesise a single policy\nthat performs well (meets given requirements) across the sampled environments,\nand furthermore bound its risk (of not meeting the given requirements) when\ndeployed in an unseen environment. Our procedure offers a trade-off between the\nguaranteed performance of the learned policy and the risk of not meeting the\nguarantee in an unseen environment. Our approach exploits knowledge of the\nenvironment's state space and graph structure, and we show how additional\nknowledge of its parametric structure can be leveraged to optimize learning and\nto obtain tighter guarantees from less samples. We evaluate our approach on a\ndiverse range of established benchmarks, demonstrating that we can generate\nhighly performing and robust policies, along with guarantees that tightly\nquantify their performance and the associated risk.",
    "arxiv_id": "2408.03093v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03093v1",
    "abstract_url": "http://arxiv.org/abs/2408.03093v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "QADQN: Quantum Attention Deep Q-Network for Financial Market Prediction",
    "authors": "Siddhant Dutta, Nouhaila Innan, Alberto Marchisio, Sadok Ben Yahia, Muhammad Shafique",
    "abstract": "Financial market prediction and optimal trading strategy development remain\nchallenging due to market complexity and volatility. Our research in quantum\nfinance and reinforcement learning for decision-making demonstrates the\napproach of quantum-classical hybrid algorithms to tackling real-world\nfinancial challenges. In this respect, we corroborate the concept with rigorous\nbacktesting and validate the framework's performance under realistic market\nconditions, by including fixed transaction cost per trade. This paper\nintroduces a Quantum Attention Deep Q-Network (QADQN) approach to address these\nchallenges through quantum-enhanced reinforcement learning. Our QADQN\narchitecture uses a variational quantum circuit inside a traditional deep\nQ-learning framework to take advantage of possible quantum advantages in\ndecision-making. We gauge the QADQN agent's performance on historical data from\nmajor market indices, including the S&P 500. We evaluate the agent's learning\nprocess by examining its reward accumulation and the effectiveness of its\nexperience replay mechanism. Our empirical results demonstrate the QADQN's\nsuperior performance, achieving better risk-adjusted returns with Sortino\nratios of 1.28 and 1.19 for non-overlapping and overlapping test periods\nrespectively, indicating effective downside risk management.",
    "arxiv_id": "2408.03088v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03088v1",
    "abstract_url": "http://arxiv.org/abs/2408.03088v1",
    "primary_category": "quant-ph",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Matrix Multiplication on Quantum Computer",
    "authors": "Jiaqi Yao, Ding Liu",
    "abstract": "This paper introduces an innovative and practical approach to universal\nquantum matrix multiplication. We designed optimized quantum adders and\nmultipliers based on Quantum Fourier Transform (QFT), which significantly\nreduced the number of gates used compared to classical adders and multipliers.\nSubsequently, we construct a basic universal quantum matrix multiplication and\nextend it to the Strassen algorithm. We conduct comparative experiments to\nanalyze the performance of the quantum matrix multiplication and evaluate the\nacceleration provided by the optimized quantum adder and multiplier.\nFurthermore, we investigate the advantages and disadvantages of the quantum\nStrassen algorithm compared to basic quantum matrix multiplication.",
    "arxiv_id": "2408.03085v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03085v1",
    "abstract_url": "http://arxiv.org/abs/2408.03085v1",
    "primary_category": "quant-ph",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Research on Autonomous Driving Decision-making Strategies based Deep Reinforcement Learning",
    "authors": "Zixiang Wang, Hao Yan, Changsong Wei, Junyu Wang, Shi Bo, Minheng Xiao",
    "abstract": "The behavior decision-making subsystem is a key component of the autonomous\ndriving system, which reflects the decision-making ability of the vehicle and\nthe driver, and is an important symbol of the high-level intelligence of the\nvehicle. However, the existing rule-based decision-making schemes are limited\nby the prior knowledge of designers, and it is difficult to cope with complex\nand changeable traffic scenarios. In this work, an advanced deep reinforcement\nlearning model is adopted, which can autonomously learn and optimize driving\nstrategies in a complex and changeable traffic environment by modeling the\ndriving decision-making process as a reinforcement learning problem.\nSpecifically, we used Deep Q-Network (DQN) and Proximal Policy Optimization\n(PPO) for comparative experiments. DQN guides the agent to choose the best\naction by approximating the state-action value function, while PPO improves the\ndecision-making quality by optimizing the policy function. We also introduce\nimprovements in the design of the reward function to promote the robustness and\nadaptability of the model in real-world driving situations. Experimental\nresults show that the decision-making strategy based on deep reinforcement\nlearning has better performance than the traditional rule-based method in a\nvariety of driving tasks.",
    "arxiv_id": "2408.03084v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03084v1",
    "abstract_url": "http://arxiv.org/abs/2408.03084v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Enhancing Complex Causality Extraction via Improved Subtask Interaction and Knowledge Fusion",
    "authors": "Jinglong Gao, Chen Lu, Xiao Ding, Zhongyang Li, Ting Liu, Bing Qin",
    "abstract": "Event Causality Extraction (ECE) aims at extracting causal event pairs from\ntexts. Despite ChatGPT's recent success, fine-tuning small models remains the\nbest approach for the ECE task. However, existing fine-tuning based ECE methods\ncannot address all three key challenges in ECE simultaneously: 1) Complex\nCausality Extraction, where multiple causal-effect pairs occur within a single\nsentence; 2) Subtask~ Interaction, which involves modeling the mutual\ndependence between the two subtasks of ECE, i.e., extracting events and\nidentifying the causal relationship between extracted events; and 3) Knowledge\nFusion, which requires effectively fusing the knowledge in two modalities,\ni.e., the expressive pretrained language models and the structured knowledge\ngraphs. In this paper, we propose a unified ECE framework (UniCE to address all\nthree issues in ECE simultaneously. Specifically, we design a subtask\ninteraction mechanism to enable mutual interaction between the two ECE\nsubtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in\nthe two modalities. Furthermore, we employ separate decoders for each subtask\nto facilitate complex causality extraction. Experiments on three benchmark\ndatasets demonstrate that our method achieves state-of-the-art performance and\noutperforms ChatGPT with a margin of at least 30% F1-score. More importantly,\nour model can also be used to effectively improve the ECE performance of\nChatGPT via in-context learning.",
    "arxiv_id": "2408.03079v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03079v1",
    "abstract_url": "http://arxiv.org/abs/2408.03079v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical Applications",
    "authors": "G. Manni, C. Lauretti, F. Prata, R. Papalia, L. Zollo, P. Soda",
    "abstract": "Endoscopic surgery relies on two-dimensional views, posing challenges for\nsurgeons in depth perception and instrument manipulation. While Simultaneous\nLocalization and Mapping (SLAM) has emerged as a promising solution to address\nthese limitations, its implementation in endoscopic procedures presents\nsignificant challenges due to hardware limitations, such as the use of a\nmonocular camera and the absence of odometry sensors. This study presents a\nrobust deep learning-based SLAM approach that combines state-of-the-art and\nnewly developed models. It consists of three main parts: the Monocular Pose\nEstimation Module that introduces a novel unsupervised method based on the\nCycleGAN architecture, the Monocular Depth Estimation Module that leverages the\nnovel Zoe architecture, and the 3D Reconstruction Module which uses information\nfrom the previous models to create a coherent surgical map. The performance of\nthe procedure was rigorously evaluated using three publicly available datasets\n(Hamlyn, EndoSLAM, and SCARED) and benchmarked against two state-of-the-art\nmethods, EndoSFMLearner and EndoDepth. The integration of Zoe in the MDEM\ndemonstrated superior performance compared to state-of-the-art depth estimation\nalgorithms in endoscopy, whereas the novel approach in the MPEM exhibited\ncompetitive performance and the lowest inference time. The results showcase the\nrobustness of our approach in laparoscopy, gastroscopy, and colonoscopy, three\ndifferent scenarios in endoscopic surgery. The proposed SLAM approach has the\npotential to improve the accuracy and efficiency of endoscopic procedures by\nproviding surgeons with enhanced depth perception and 3D reconstruction\ncapabilities.",
    "arxiv_id": "2408.03078v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03078v1",
    "abstract_url": "http://arxiv.org/abs/2408.03078v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Solving QUBO on the Loihi 2 Neuromorphic Processor",
    "authors": "Alessandro Pierro, Philipp Stratmann, Gabriel Andres Fonseca Guerra, Sumedh Risbud, Timothy Shea, Ashish Rao Mangalore, Andreas Wild",
    "abstract": "In this article, we describe an algorithm for solving Quadratic Unconstrained\nBinary Optimization problems on the Intel Loihi 2 neuromorphic processor. The\nsolver is based on a hardware-aware fine-grained parallel simulated annealing\nalgorithm developed for Intel's neuromorphic research chip Loihi 2. Preliminary\nresults show that our approach can generate feasible solutions in as little as\n1 ms and up to 37x more energy efficient compared to two baseline solvers\nrunning on a CPU. These advantages could be especially relevant for size-,\nweight-, and power-constrained edge computing applications.",
    "arxiv_id": "2408.03076v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03076v1",
    "abstract_url": "http://arxiv.org/abs/2408.03076v1",
    "primary_category": "cs.NE",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents",
    "authors": "Qiang Sun, Yuanyi Luo, Sirui Li, Wenxiao Zhang, Wei Liu",
    "abstract": "Multimodal conversational agents are highly desirable because they offer\nnatural and human-like interaction. However, there is a lack of comprehensive\nend-to-end solutions to support collaborative development and benchmarking.\nWhile proprietary systems like GPT-4o and Gemini demonstrating impressive\nintegration of audio, video, and text with response times of 200-250ms,\nchallenges remain in balancing latency, accuracy, cost, and data privacy. To\nbetter understand and quantify these issues, we developed OpenOmni, an\nopen-source, end-to-end pipeline benchmarking tool that integrates advanced\ntechnologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented\nGeneration, Large Language Models, along with the ability to integrate\ncustomized models. OpenOmni supports local and cloud deployment, ensuring data\nprivacy and supporting latency and accuracy benchmarking. This flexible\nframework allows researchers to customize the pipeline, focusing on real\nbottlenecks and facilitating rapid proof-of-concept development. OpenOmni can\nsignificantly enhance applications like indoor assistance for visually impaired\nindividuals, advancing human-computer interaction. Our demonstration video is\navailable https://www.youtube.com/watch?v=zaSiT3clWqY, demo is available via\nhttps://openomni.ai4wa.com, code is available via\nhttps://github.com/AI4WA/OpenOmniFramework.",
    "arxiv_id": "2408.03047v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03047v1",
    "abstract_url": "http://arxiv.org/abs/2408.03047v1",
    "primary_category": "cs.HC",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning",
    "authors": "Haozhe Ma, Zhengding Luo, Thanh Vinh Vo, Kuankuan Sima, Tze-Yun Leong",
    "abstract": "Reward shaping addresses the challenge of sparse rewards in reinforcement\nlearning by constructing denser and more informative reward signals. To achieve\nself-adaptive and highly efficient reward shaping, we propose a novel method\nthat incorporates success rates derived from historical experiences into shaped\nrewards. Our approach utilizes success rates sampled from Beta distributions,\nwhich dynamically evolve from uncertain to reliable values as more data is\ncollected. Initially, the self-adaptive success rates exhibit more randomness\nto encourage exploration. Over time, they become more certain to enhance\nexploitation, thus achieving a better balance between exploration and\nexploitation. We employ Kernel Density Estimation (KDE) combined with Random\nFourier Features (RFF) to derive the Beta distributions, resulting in a\ncomputationally efficient implementation in high-dimensional continuous state\nspaces. This method provides a non-parametric and learning-free approach. The\nproposed method is evaluated on a wide range of continuous control tasks with\nsparse and delayed rewards, demonstrating significant improvements in sample\nefficiency and convergence stability compared to several baselines.",
    "arxiv_id": "2408.03029v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03029v1",
    "abstract_url": "http://arxiv.org/abs/2408.03029v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Integrating Controllable Motion Skills from Demonstrations",
    "authors": "Honghao Liao, Zhiheng Li, Ziyu Meng, Ran Song, Yibin Li, Wei Zhang",
    "abstract": "The expanding applications of legged robots require their mastery of\nversatile motion skills. Correspondingly, researchers must address the\nchallenge of integrating multiple diverse motion skills into controllers. While\nexisting reinforcement learning (RL)-based approaches have achieved notable\nsuccess in multi-skill integration for legged robots, these methods often\nrequire intricate reward engineering or are restricted to integrating a\npredefined set of motion skills constrained by specific task objectives,\nresulting in limited flexibility. In this work, we introduce a flexible\nmulti-skill integration framework named Controllable Skills Integration (CSI).\nCSI enables the integration of a diverse set of motion skills with varying\nstyles into a single policy without the need for complex reward tuning.\nFurthermore, in a hierarchical control manner, the trained low-level policy can\nbe coupled with a high-level Natural Language Inference (NLI) module to enable\npreliminary language-directed skill control. Our experiments demonstrate that\nCSI can flexibly integrate a diverse array of motion skills more\ncomprehensively and facilitate the transitions between different skills.\nAdditionally, CSI exhibits good scalability as the number of motion skills to\nbe integrated increases significantly.",
    "arxiv_id": "2408.03018v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03018v1",
    "abstract_url": "http://arxiv.org/abs/2408.03018v1",
    "primary_category": "cs.RO",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "NeurDB: On the Design and Implementation of an AI-powered Autonomous Database",
    "authors": "Zhanhao Zhao, Shaofeng Cai, Haotian Gao, Hexiang Pan, Siqi Xiang, Naili Xing, Gang Chen, Beng Chin Ooi, Yanyan Shen, Yuncheng Wu, Meihui Zhang",
    "abstract": "Databases are increasingly embracing AI to provide autonomous system\noptimization and intelligent in-database analytics, aiming to relieve end-user\nburdens across various industry sectors. Nonetheless, most existing approaches\nfail to account for the dynamic nature of databases, which renders them\nineffective for real-world applications characterized by evolving data and\nworkloads. This paper introduces NeurDB, an AI-powered autonomous database that\ndeepens the fusion of AI and databases with adaptability to data and workload\ndrift. NeurDB establishes a new in-database AI ecosystem that seamlessly\nintegrates AI workflows within the database. This integration enables efficient\nand effective in-database AI analytics and fast-adaptive learned system\ncomponents. Empirical evaluations demonstrate that NeurDB substantially\noutperforms existing solutions in managing AI analytics tasks, with the\nproposed learned components more effectively handling environmental dynamism\nthan state-of-the-art approaches.",
    "arxiv_id": "2408.03013v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03013v1",
    "abstract_url": "http://arxiv.org/abs/2408.03013v1",
    "primary_category": "cs.DB",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Cross-cultural analysis of pedestrian group behaviour influence on crossing decisions in interactions with autonomous vehicles",
    "authors": "Sergio Mart\u00edn Serrano, \u00d3scar M\u00e9ndez Blanco, Stewart Worrall, Miguel \u00c1ngel Sotelo, David Fern\u00e1ndez-Llorca",
    "abstract": "Understanding cultural backgrounds is crucial for the seamless integration of\nautonomous driving into daily life as it ensures that systems are attuned to\ndiverse societal norms and behaviours, enhancing acceptance and safety in\nvaried cultural contexts. In this work, we investigate the impact of co-located\npedestrians on crossing behaviour, considering cultural and situational\nfactors. To accomplish this, a full-scale virtual reality (VR) environment was\ncreated in the CARLA simulator, enabling the identical experiment to be\nreplicated in both Spain and Australia. Participants (N=30) attempted to cross\nthe road at an urban crosswalk alongside other pedestrians exhibiting\nconservative to more daring behaviours, while an autonomous vehicle (AV)\napproached with different driving styles. For the analysis of interactions, we\nutilized questionnaires and direct measures of the moment when participants\nentered the lane.\n  Our findings indicate that pedestrians tend to cross the same traffic gap\ntogether, even though reckless behaviour by the group reduces confidence and\nmakes the situation perceived as more complex. Australian participants were\nwilling to take fewer risks than Spanish participants, adopting more cautious\nbehaviour when it was uncertain whether the AV would yield.",
    "arxiv_id": "2408.03003v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03003v1",
    "abstract_url": "http://arxiv.org/abs/2408.03003v1",
    "primary_category": "cs.HC",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
    "authors": "Lekai Chen, Ashutosh Trivedi, Alvaro Velasquez",
    "abstract": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
    "arxiv_id": "2408.02999v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02999v1",
    "abstract_url": "http://arxiv.org/abs/2408.02999v1",
    "primary_category": "cs.FL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application",
    "authors": "Anwesha Mukherjee, Rajkumar Buyya",
    "abstract": "Federated learning has become an emerging technology for data analysis for\nIoT applications. This paper implements centralized and decentralized federated\nlearning frameworks for crop yield prediction based on Long Short-Term Memory\nNetwork. For centralized federated learning, multiple clients and one server is\nconsidered, where the clients exchange their model updates with the server that\nworks as the aggregator to build the global model. For the decentralized\nframework, a collaborative network is formed among the devices either using\nring topology or using mesh topology. In this network, each device receives\nmodel updates from the neighbour devices, and performs aggregation to build the\nupgraded model. The performance of the centralized and decentralized federated\nlearning frameworks are evaluated in terms of prediction accuracy, precision,\nrecall, F1-Score, and training time. The experimental results present that\n$\\geq$97% and $>$97.5% prediction accuracy are achieved using the centralized\nand decentralized federated learning-based frameworks respectively. The results\nalso show that the using centralized federated learning the response time can\nbe reduced by $\\sim$75% than the cloud-only framework. Finally, the future\nresearch directions of the use of federated learning in crop yield prediction\nare explored in this paper.",
    "arxiv_id": "2408.02998v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02998v1",
    "abstract_url": "http://arxiv.org/abs/2408.02998v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "A Differential Smoothness-based Compact-Dynamic Graph Convolutional Network for Spatiotemporal Signal Recovery",
    "authors": "Pengcheng Gao, Zicheng Gao, Ye Yuan",
    "abstract": "High quality spatiotemporal signal is vitally important for real application\nscenarios like energy management, traffic planning and cyber security. Due to\nthe uncontrollable factors like abrupt sensors breakdown or communication\nfault, the spatiotemporal signal collected by sensors is always incomplete. A\ndynamic graph convolutional network (DGCN) is effective for processing\nspatiotemporal signal recovery. However, it adopts a static GCN and a sequence\nneural network to explore the spatial and temporal patterns, separately. Such a\nseparated two-step processing is loose spatiotemporal, thereby failing to\ncapture the complex inner spatiotemporal correlation. To address this issue,\nthis paper proposes a Compact-Dynamic Graph Convolutional Network (CDGCN) for\nspatiotemporal signal recovery with the following two-fold ideas: a) leveraging\nthe tensor M-product to build a unified tensor graph convolution framework,\nwhich considers both spatial and temporal patterns simultaneously; and b)\nconstructing a differential smoothness-based objective function to reduce the\nnoise interference in spatiotemporal signal, thereby further improve the\nrecovery accuracy. Experiments on real-world spatiotemporal datasets\ndemonstrate that the proposed CDGCN significantly outperforms the\nstate-of-the-art models in terms of recovery accuracy.",
    "arxiv_id": "2408.02987v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02987v1",
    "abstract_url": "http://arxiv.org/abs/2408.02987v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "ASR-enhanced Multimodal Representation Learning for Cross-Domain Product Retrieval",
    "authors": "Ruixiang Zhao, Jian Jia, Yan Li, Xuehan Bai, Quan Chen, Han Li, Peng Jiang, Xirong Li",
    "abstract": "E-commerce is increasingly multimedia-enriched, with products exhibited in a\nbroad-domain manner as images, short videos, or live stream promotions. A\nunified and vectorized cross-domain production representation is essential. Due\nto large intra-product variance and high inter-product similarity in the\nbroad-domain scenario, a visual-only representation is inadequate. While\nAutomatic Speech Recognition (ASR) text derived from the short or live-stream\nvideos is readily accessible, how to de-noise the excessively noisy text for\nmultimodal representation learning is mostly untouched. We propose ASR-enhanced\nMultimodal Product Representation Learning (AMPere). In order to extract\nproduct-specific information from the raw ASR text, AMPere uses an\neasy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,\ntogether with visual data, is then fed into a multi-branch network to generate\ncompact multimodal embeddings. Extensive experiments on a large-scale\ntri-domain dataset verify the effectiveness of AMPere in obtaining a unified\nmultimodal product representation that clearly improves cross-domain product\nretrieval.",
    "arxiv_id": "2408.02978v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02978v1",
    "abstract_url": "http://arxiv.org/abs/2408.02978v1",
    "primary_category": "cs.MM",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation",
    "authors": "Hui Ma, Bo Zhang, Bo Xu, Jian Wang, Hongfei Lin, Xiao Sun",
    "abstract": "Empathetic response generation, aiming at understanding the user's situation\nand feelings and respond empathically, is crucial in building human-like\ndialogue systems. Previous methods mainly focus on using maximum likelihood\nestimation as the optimization objective for training response generation\nmodels, without taking into account the empathy level alignment between\ngenerated responses and target responses. To this end, we propose an empathetic\nresponse generation using reinforcement learning (EmpRL) framework. The\nframework designs an effective empathy reward function and generates empathetic\nresponses by maximizing the expected reward through reinforcement learning.\nGiven the powerful text generation capability of pre-trained language models,\nEmpRL utilizes the pre-trained T5 model as the generator and conducts further\ntraining to initialize the policy. To align the empathy level between generated\nresponses and target responses in the context, an empathy reward function\ncontaining three empathy communication mechanisms, i.e., emotional reaction,\ninterpretation, and exploration, is constructed using pre-designed and\npre-trained empathy identifiers. Finally, the proximal policy optimization\nalgorithm is used to further train the policy to produce empathetic responses.\nBoth automatic and manual evaluations demonstrate that the proposed EmpRL\nframework can improve the quality of generated responses, enhance the empathy\nlevel similarity between generated and target responses, and produce empathetic\nresponses covering both affective and cognitive aspects.",
    "arxiv_id": "2408.02976v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02976v1",
    "abstract_url": "http://arxiv.org/abs/2408.02976v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Wave Interpolation Neural Operator: Interpolated Prediction of Electric Fields Across Untrained Wavelengths",
    "authors": "Joonhyuk Seo, Chanik Kang, Dongjin Seo, Haejun Chung",
    "abstract": "Designing photonic structures requires electromagnetic simulations, which\noften require high computational costs. Researchers have developed surrogate\nsolvers for predicting electric fields to alleviate the computational issues.\nHowever, existing surrogate solvers are limited to performing inference at\nfixed simulation conditions and require retraining for different conditions. To\naddress this, we propose Wave Interpolation Neural Operator (WINO), a novel\nsurrogate solver enabling simulation condition interpolation across a\ncontinuous spectrum of broadband wavelengths. WINO introduces the Fourier Group\nConvolution Shuffling operator and a new conditioning method to efficiently\npredict electric fields from both trained and untrained wavelength data,\nachieving significant improvements in parameter efficiency and spectral\ninterpolation performance. Our model demonstrates approximately 100 times\nfaster performance than traditional finite-difference frequency-domain\nsimulations. Moreover, compared to the state-of-the-art model, we achieve a 74%\nreduction in parameters and 80.5% improvements in prediction accuracy for\nuntrained wavelengths, and 13.2% improvements for trained wavelengths.",
    "arxiv_id": "2408.02971v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02971v1",
    "abstract_url": "http://arxiv.org/abs/2408.02971v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Data-Driven Stochastic Closure Modeling via Conditional Diffusion Model and Neural Operator",
    "authors": "Xinghao Dong, Chuanqi Chen, Jin-Long Wu",
    "abstract": "Closure models are widely used in simulating complex multiscale dynamical\nsystems such as turbulence and the earth system, for which direct numerical\nsimulation that resolves all scales is often too expensive. For those systems\nwithout a clear scale separation, deterministic and local closure models often\nlack enough generalization capability, which limits their performance in many\nreal-world applications. In this work, we propose a data-driven modeling\nframework for constructing stochastic and non-local closure models via\nconditional diffusion model and neural operator. Specifically, the Fourier\nneural operator is incorporated into a score-based diffusion model, which\nserves as a data-driven stochastic closure model for complex dynamical systems\ngoverned by partial differential equations (PDEs). We also demonstrate how\naccelerated sampling methods can improve the efficiency of the data-driven\nstochastic closure model. The results show that the proposed methodology\nprovides a systematic approach via generative machine learning techniques to\nconstruct data-driven stochastic closure models for multiscale dynamical\nsystems with continuous spatiotemporal fields.",
    "arxiv_id": "2408.02965v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02965v1",
    "abstract_url": "http://arxiv.org/abs/2408.02965v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Synaptic Modulation using Interspike Intervals Increases Energy Efficiency of Spiking Neural Networks",
    "authors": "Dylan Adams, Magda Zajaczkowska, Ashiq Anjum, Andrea Soltoggio, Shirin Dora",
    "abstract": "Despite basic differences between Spiking Neural Networks (SNN) and\nArtificial Neural Networks (ANN), most research on SNNs involve adapting\nANN-based methods for SNNs. Pruning (dropping connections) and quantization\n(reducing precision) are often used to improve energy efficiency of SNNs. These\nmethods are very effective for ANNs whose energy needs are determined by\nsignals transmitted on synapses. However, the event-driven paradigm in SNNs\nimplies that energy is consumed by spikes. In this paper, we propose a new\nsynapse model whose weights are modulated by Interspike Intervals (ISI) i.e.\ntime difference between two spikes. SNNs composed of this synapse model, termed\nISI Modulated SNNs (IMSNN), can use gradient descent to estimate how the ISI of\na neuron changes after updating its synaptic parameters. A higher ISI implies\nfewer spikes and vice-versa. The learning algorithm for IMSNNs exploits this\ninformation to selectively propagate gradients such that learning is achieved\nby increasing the ISIs resulting in a network that generates fewer spikes. The\nperformance of IMSNNs with dense and convolutional layers have been evaluated\nin terms of classification accuracy and the number of spikes using the MNIST\nand FashionMNIST datasets. The performance comparison with conventional SNNs\nshows that IMSNNs exhibit upto 90% reduction in the number of spikes while\nmaintaining similar classification accuracy.",
    "arxiv_id": "2408.02961v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02961v1",
    "abstract_url": "http://arxiv.org/abs/2408.02961v1",
    "primary_category": "cs.NE",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Anytime Multi-Agent Path Finding with an Adaptive Delay-Based Heuristic",
    "authors": "Thomy Phan, Benran Zhang, Shao-Hung Chan, Sven Koenig",
    "abstract": "Anytime multi-agent path finding (MAPF) is a promising approach to scalable\npath optimization in multi-agent systems. MAPF-LNS, based on Large Neighborhood\nSearch (LNS), is the current state-of-the-art approach where a fast initial\nsolution is iteratively optimized by destroying and repairing selected paths of\nthe solution. Current MAPF-LNS variants commonly use an adaptive selection\nmechanism to choose among multiple destroy heuristics. However, to determine\npromising destroy heuristics, MAPF-LNS requires a considerable amount of\nexploration time. As common destroy heuristics are non-adaptive, any\nperformance bottleneck caused by these heuristics cannot be overcome via\nadaptive heuristic selection alone, thus limiting the overall effectiveness of\nMAPF-LNS in terms of solution cost. In this paper, we propose Adaptive\nDelay-based Destroy-and-Repair Enhanced with Success-based Self-Learning\n(ADDRESS) as a single-destroy-heuristic variant of MAPF-LNS. ADDRESS applies\nrestricted Thompson Sampling to the top-K set of the most delayed agents to\nselect a seed agent for adaptive LNS neighborhood generation. We evaluate\nADDRESS in multiple maps from the MAPF benchmark set and demonstrate cost\nimprovements by at least 50% in large-scale scenarios with up to a thousand\nagents, compared with the original MAPF-LNS and other state-of-the-art methods.",
    "arxiv_id": "2408.02960v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02960v1",
    "abstract_url": "http://arxiv.org/abs/2408.02960v1",
    "primary_category": "cs.AI",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Kolmogorov-Arnold PointNet: Deep learning for prediction of fluid fields on irregular geometries",
    "authors": "Ali Kashefi",
    "abstract": "We present Kolmogorov-Arnold PointNet (KA-PointNet) as a novel supervised\ndeep learning framework for the prediction of incompressible steady-state fluid\nflow fields in irregular domains, where the predicted fields are a function of\nthe geometry of the domains. In KA-PointNet, we implement shared\nKolmogorov-Arnold Networks (KANs) in the segmentation branch of the PointNet\narchitecture. We utilize Jacobi polynomials to construct shared KANs. As a\nbenchmark test case, we consider incompressible laminar steady-state flow over\na cylinder, where the geometry of its cross-section varies over the data set.\nWe investigate the performance of Jacobi polynomials with different degrees as\nwell as special cases of Jacobi polynomials such as Legendre polynomials,\nChebyshev polynomials of the first and second kinds, and Gegenbauer\npolynomials, in terms of the computational cost of training and accuracy of\nprediction of the test set. Additionally, we compare the performance of\nPointNet with shared KANs (i.e., KA-PointNet) and PointNet with shared\nMultilayer Perceptrons (MLPs). It is observed that when the number of trainable\nparameters is approximately equal, PointNet with shared KANs (i.e.,\nKA-PointNet) outperforms PointNet with shared MLPs.",
    "arxiv_id": "2408.02950v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02950v1",
    "abstract_url": "http://arxiv.org/abs/2408.02950v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Few-shot Scooping Under Domain Shift via Simulated Maximal Deployment Gaps",
    "authors": "Yifan Zhu, Pranay Thangeda, Erica L Tevere, Ashish Goel, Erik Kramer, Hari D Nayar, Melkior Ornik, Kris Hauser",
    "abstract": "Autonomous lander missions on extraterrestrial bodies need to sample granular\nmaterials while coping with domain shifts, even when sampling strategies are\nextensively tuned on Earth. To tackle this challenge, this paper studies the\nfew-shot scooping problem and proposes a vision-based adaptive scooping\nstrategy that uses the deep kernel Gaussian process method trained with a novel\nmeta-training strategy to learn online from very limited experience on\nout-of-distribution target terrains. Our Deep Kernel Calibration with Maximal\nDeployment Gaps (kCMD) strategy explicitly trains a deep kernel model to adapt\nto large domain shifts by creating simulated maximal deployment gaps from an\noffline training dataset and training models to overcome these deployment gaps\nduring training. Employed in a Bayesian Optimization sequential decision-making\nframework, the proposed method allows the robot to perform high-quality\nscooping actions on out-of-distribution terrains after a few attempts,\nsignificantly outperforming non-adaptive methods proposed in the excavation\nliterature as well as other state-of-the-art meta-learning methods. The\nproposed method also demonstrates zero-shot transfer capability, successfully\nadapting to the NASA OWLAT platform, which serves as a state-of-the-art\nsimulator for potential future planetary missions. These results demonstrate\nthe potential of training deep models with simulated deployment gaps for more\ngeneralizable meta-learning in high-capacity models. Furthermore, they\nhighlight the promise of our method in autonomous lander sampling missions by\nenabling landers to overcome the deployment gap between Earth and\nextraterrestrial bodies.",
    "arxiv_id": "2408.02949v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02949v1",
    "abstract_url": "http://arxiv.org/abs/2408.02949v1",
    "primary_category": "cs.RO",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Scaling Laws for Data Poisoning in LLMs",
    "authors": "Dillon Bowen, Brendan Murphy, Will Cai, David Khachaturov, Adam Gleave, Kellin Pelrine",
    "abstract": "Recent work shows that LLMs are vulnerable to data poisoning, in which they\nare trained on partially corrupted or harmful data. Poisoned data is hard to\ndetect, breaks guardrails, and leads to undesirable and harmful behavior. Given\nthe intense efforts by leading labs to train and deploy increasingly larger and\nmore capable LLMs, it is critical to ask if the risk of data poisoning will be\nnaturally mitigated by scale, or if it is an increasing threat. We consider\nthree threat models by which data poisoning can occur: malicious fine-tuning,\nimperfect data curation, and intentional data contamination. Our experiments\nevaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72\nbillion parameters on three datasets which speak to each of our threat models.\nWe find that larger LLMs are increasingly vulnerable, learning harmful behavior\n-- including sleeper agent behavior -- significantly more quickly than smaller\nLLMs with even minimal data poisoning. These results underscore the need for\nrobust safeguards against data poisoning in larger LLMs.",
    "arxiv_id": "2408.02946v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02946v1",
    "abstract_url": "http://arxiv.org/abs/2408.02946v1",
    "primary_category": "cs.CR",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "LLM-Empowered Resource Allocation in Wireless Communications Systems",
    "authors": "Woongsup Lee, Jeonghun Park",
    "abstract": "The recent success of large language models (LLMs) has spurred their\napplication in various fields. In particular, there have been efforts to\nintegrate LLMs into various aspects of wireless communication systems. The use\nof LLMs in wireless communication systems has the potential to realize\nartificial general intelligence (AGI)-enabled wireless networks. In this paper,\nwe investigate an LLM-based resource allocation scheme for wireless\ncommunication systems. Specifically, we formulate a simple resource allocation\nproblem involving two transmit pairs and develop an LLM-based resource\nallocation approach that aims to maximize either energy efficiency or spectral\nefficiency. Additionally, we consider the joint use of low-complexity resource\nallocation techniques to compensate for the reliability shortcomings of the\nLLM-based scheme. After confirming the applicability and feasibility of\nLLM-based resource allocation, we address several key technical challenges that\nremain in applying LLMs in practice.",
    "arxiv_id": "2408.02944v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02944v1",
    "abstract_url": "http://arxiv.org/abs/2408.02944v1",
    "primary_category": "eess.SP",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Achieving More with Less: A Tensor-Optimization-Powered Ensemble Method",
    "authors": "Jinghui Yuan, Weijin Jiang, Zhe Cao, Fangyuan Xie, Rong Wang, Feiping Nie, Xuelong Li",
    "abstract": "Ensemble learning is a method that leverages weak learners to produce a\nstrong learner. However, obtaining a large number of base learners requires\nsubstantial time and computational resources. Therefore, it is meaningful to\nstudy how to achieve the performance typically obtained with many base learners\nusing only a few. We argue that to achieve this, it is essential to enhance\nboth classification performance and generalization ability during the ensemble\nprocess. To increase model accuracy, each weak base learner needs to be more\nefficiently integrated. It is observed that different base learners exhibit\nvarying levels of accuracy in predicting different classes. To capitalize on\nthis, we introduce confidence tensors $\\tilde{\\mathbf{\\Theta}}$ and\n$\\tilde{\\mathbf{\\Theta}}_{rst}$ signifies that the $t$-th base classifier\nassigns the sample to class $r$ while it actually belongs to class $s$. To the\nbest of our knowledge, this is the first time an evaluation of the performance\nof base classifiers across different classes has been proposed. The proposed\nconfidence tensor compensates for the strengths and weaknesses of each base\nclassifier in different classes, enabling the method to achieve superior\nresults with a smaller number of base learners. To enhance generalization\nperformance, we design a smooth and convex objective function that leverages\nthe concept of margin, making the strong learner more discriminative.\nFurthermore, it is proved that in gradient matrix of the loss function, the sum\nof each column's elements is zero, allowing us to solve a constrained\noptimization problem using gradient-based methods. We then compare our\nalgorithm with random forests of ten times the size and other classical methods\nacross numerous datasets, demonstrating the superiority of our approach.",
    "arxiv_id": "2408.02936v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02936v1",
    "abstract_url": "http://arxiv.org/abs/2408.02936v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Doubly Stochastic Adaptive Neighbors Clustering via the Marcus Mapping",
    "authors": "Jinghui Yuan, Chusheng Zeng, Fangyuan Xie, Zhe Cao, Rong Wang, Feiping Nie, Xuelong Li",
    "abstract": "Clustering is a fundamental task in machine learning and data science, and\nsimilarity graph-based clustering is an important approach within this domain.\nDoubly stochastic symmetric similarity graphs provide numerous benefits for\nclustering problems and downstream tasks, yet learning such graphs remains a\nsignificant challenge. Marcus theorem states that a strictly positive symmetric\nmatrix can be transformed into a doubly stochastic symmetric matrix by diagonal\nmatrices. However, in clustering, learning sparse matrices is crucial for\ncomputational efficiency. We extend Marcus theorem by proposing the Marcus\nmapping, which indicates that certain sparse matrices can also be transformed\ninto doubly stochastic symmetric matrices via diagonal matrices. Additionally,\nwe introduce rank constraints into the clustering problem and propose the\nDoubly Stochastic Adaptive Neighbors Clustering algorithm based on the Marcus\nMapping (ANCMM). This ensures that the learned graph naturally divides into the\ndesired number of clusters. We validate the effectiveness of our algorithm\nthrough extensive comparisons with state-of-the-art algorithms. Finally, we\nexplore the relationship between the Marcus mapping and optimal transport. We\nprove that the Marcus mapping solves a specific type of optimal transport\nproblem and demonstrate that solving this problem through Marcus mapping is\nmore efficient than directly applying optimal transport methods.",
    "arxiv_id": "2408.02932v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02932v1",
    "abstract_url": "http://arxiv.org/abs/2408.02932v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "The Need for a Big World Simulator: A Scientific Challenge for Continual Learning",
    "authors": "Saurabh Kumar, Hong Jun Jeon, Alex Lewandowski, Benjamin Van Roy",
    "abstract": "The \"small agent, big world\" frame offers a conceptual view that motivates\nthe need for continual learning. The idea is that a small agent operating in a\nmuch bigger world cannot store all information that the world has to offer. To\nperform well, the agent must be carefully designed to ingest, retain, and eject\nthe right information. To enable the development of performant continual\nlearning agents, a number of synthetic environments have been proposed.\nHowever, these benchmarks suffer from limitations, including unnatural\ndistribution shifts and a lack of fidelity to the \"small agent, big world\"\nframing. This paper aims to formalize two desiderata for the design of future\nsimulated environments. These two criteria aim to reflect the objectives and\ncomplexity of continual learning in practical settings while enabling rapid\nprototyping of algorithms on a smaller scale.",
    "arxiv_id": "2408.02930v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02930v1",
    "abstract_url": "http://arxiv.org/abs/2408.02930v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection",
    "authors": "Yuxin Wang, Duanyu Feng, Yongfu Dai, Zhengyu Chen, Jimin Huang, Sophia Ananiadou, Qianqian Xie, Hao Wang",
    "abstract": "Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.",
    "arxiv_id": "2408.02927v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02927v1",
    "abstract_url": "http://arxiv.org/abs/2408.02927v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "A Taxonomy of Architecture Options for Foundation Model-based Agents: Analysis and Decision Model",
    "authors": "Jingwen Zhou, Qinghua Lu, Jieshan Chen, Liming Zhu, Xiwei Xu, Zhenchang Xing, Stefan Harrer",
    "abstract": "The rapid advancement of AI technology has led to widespread applications of\nagent systems across various domains. However, the need for detailed\narchitecture design poses significant challenges in designing and operating\nthese systems. This paper introduces a taxonomy focused on the architectures of\nfoundation-model-based agents, addressing critical aspects such as functional\ncapabilities and non-functional qualities. We also discuss the operations\ninvolved in both design-time and run-time phases, providing a comprehensive\nview of architectural design and operational characteristics. By unifying and\ndetailing these classifications, our taxonomy aims to improve the design of\nfoundation-model-based agents. Additionally, the paper establishes a decision\nmodel that guides critical design and runtime decisions, offering a structured\napproach to enhance the development of foundation-model-based agents. Our\ncontributions include providing a structured architecture design option and\nguiding the development process of foundation-model-based agents, thereby\naddressing current fragmentation in the field.",
    "arxiv_id": "2408.02920v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02920v1",
    "abstract_url": "http://arxiv.org/abs/2408.02920v1",
    "primary_category": "cs.SE",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "KOI: Accelerating Online Imitation Learning via Hybrid Key-state Guidance",
    "authors": "Jingxian Lu, Wenke Xia, Dong Wang, Zhigang Wang, Bin Zhao, Di Hu, Xuelong Li",
    "abstract": "Online Imitation Learning methods struggle with the gap between extensive\nonline exploration space and limited expert trajectories, which hinder\nefficient exploration due to inaccurate task-aware reward estimation. Inspired\nby the findings from cognitive neuroscience that task decomposition could\nfacilitate cognitive processing for efficient learning, we hypothesize that an\nagent could estimate precise task-aware imitation rewards for efficient online\nexploration by decomposing the target task into the objectives of \"what to do\"\nand the mechanisms of \"how to do\". In this work, we introduce the hybrid\nKey-state guided Online Imitation (KOI) learning approach, which leverages the\nintegration of semantic and motion key states as guidance for task-aware reward\nestimation. Initially, we utilize the visual-language models to segment the\nexpert trajectory into semantic key states, indicating the objectives of \"what\nto do\". Within the intervals between semantic key states, optical flow is\nemployed to capture motion key states to understand the process of \"how to do\".\nBy integrating a thorough grasp of both semantic and motion key states, we\nrefine the trajectory-matching reward computation, encouraging task-aware\nexploration for efficient online imitation learning. Our experiment results\nprove that our method is more sample efficient in the Meta-World and LIBERO\nenvironments. We also conduct real-world robotic manipulation experiments to\nvalidate the efficacy of our method, demonstrating the practical applicability\nof our KOI method.",
    "arxiv_id": "2408.02912v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02912v1",
    "abstract_url": "http://arxiv.org/abs/2408.02912v1",
    "primary_category": "cs.RO",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Enabling Intelligent Traffic Systems: A Deep Learning Method for Accurate Arabic License Plate Recognition",
    "authors": "M. A. Sayedelahl",
    "abstract": "This paper introduces a novel two-stage framework for accurate Egyptian\nVehicle License Plate Recognition (EVLPR). The first stage employs image\nprocessing techniques to reliably localize license plates, while the second\nstage utilizes a custom-designed deep learning model for robust Arabic\ncharacter recognition. The proposed system achieves a remarkable 99.3% accuracy\non a diverse dataset, surpassing existing approaches. Its potential\napplications extend to intelligent traffic management, including traffic\nviolation detection and parking optimization. Future research will focus on\nenhancing the system's capabilities through architectural refinements, expanded\ndatasets, and addressing system dependencies.",
    "arxiv_id": "2408.02904v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02904v1",
    "abstract_url": "http://arxiv.org/abs/2408.02904v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "A Metric Driven Approach to Mixed Precision Training",
    "authors": "Mitchelle Rasquinha, Gil Tabak",
    "abstract": "As deep learning methodologies have developed, it has been generally agreed\nthat increasing neural network size improves model quality. However, this is at\nthe expense of memory and compute requirements, which also need to be\nincreased. Various efficiency techniques have been proposed to rein in hardware\ncosts, one being the use of low precision numerics. Recent accelerators have\nintroduced several different 8-bit data types to help accommodate DNNs in terms\nof numerics. In this paper, we identify a metric driven methodology to aid in\nthe choice of numerics. We demonstrate how such a methodology can help scale\ntraining of a language representation model. The technique can be generalized\nto other model architectures.",
    "arxiv_id": "2408.02897v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02897v1",
    "abstract_url": "http://arxiv.org/abs/2408.02897v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "VizECGNet: Visual ECG Image Network for Cardiovascular Diseases Classification with Multi-Modal Training and Knowledge Distillation",
    "authors": "Ju-Hyeon Nam, Seo-Hyung Park, Su Jung Kim, Sang-Chul Lee",
    "abstract": "An electrocardiogram (ECG) captures the heart's electrical signal to assess\nvarious heart conditions. In practice, ECG data is stored as either digitized\nsignals or printed images. Despite the emergence of numerous deep learning\nmodels for digitized signals, many hospitals prefer image storage due to cost\nconsiderations. Recognizing the unavailability of raw ECG signals in many\nclinical settings, we propose VizECGNet, which uses only printed ECG graphics\nto determine the prognosis of multiple cardiovascular diseases. During\ntraining, cross-modal attention modules (CMAM) are used to integrate\ninformation from two modalities - image and signal, while self-modality\nattention modules (SMAM) capture inherent long-range dependencies in ECG data\nof each modality. Additionally, we utilize knowledge distillation to improve\nthe similarity between two distinct predictions from each modality stream. This\ninnovative multi-modal deep learning architecture enables the utilization of\nonly ECG images during inference. VizECGNet with image input achieves higher\nperformance in precision, recall, and F1-Score compared to signal-based ECG\nclassification models, with improvements of 3.50%, 8.21%, and 7.38%,\nrespectively.",
    "arxiv_id": "2408.02888v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02888v1",
    "abstract_url": "http://arxiv.org/abs/2408.02888v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Compromising Embodied Agents with Contextual Backdoor Attacks",
    "authors": "Aishan Liu, Yuguang Zhou, Xianglong Liu, Tianyuan Zhang, Siyuan Liang, Jiakai Wang, Yanjun Pu, Tianlin Li, Junqi Zhang, Wenbo Zhou, Qing Guo, Dacheng Tao",
    "abstract": "Large language models (LLMs) have transformed the development of embodied\nintelligence. By providing a few contextual demonstrations, developers can\nutilize the extensive internal knowledge of LLMs to effortlessly translate\ncomplex tasks described in abstract language into sequences of code snippets,\nwhich will serve as the execution logic for embodied agents. However, this\npaper uncovers a significant backdoor security threat within this process and\nintroduces a novel method called \\method{}. By poisoning just a few contextual\ndemonstrations, attackers can covertly compromise the contextual environment of\na black-box LLM, prompting it to generate programs with context-dependent\ndefects. These programs appear logically sound but contain defects that can\nactivate and induce unintended behaviors when the operational agent encounters\nspecific triggers in its interactive environment. To compromise the LLM's\ncontextual environment, we employ adversarial in-context generation to optimize\npoisoned demonstrations, where an LLM judge evaluates these poisoned prompts,\nreporting to an additional LLM that iteratively optimizes the demonstration in\na two-player adversarial game using chain-of-thought reasoning. To enable\ncontext-dependent behaviors in downstream agents, we implement a dual-modality\nactivation strategy that controls both the generation and execution of program\ndefects through textual and visual triggers. We expand the scope of our attack\nby developing five program defect modes that compromise key aspects of\nconfidentiality, integrity, and availability in embodied agents. To validate\nthe effectiveness of our approach, we conducted extensive experiments across\nvarious tasks, including robot planning, robot manipulation, and compositional\nvisual reasoning. Additionally, we demonstrate the potential impact of our\napproach by successfully attacking real-world autonomous driving systems.",
    "arxiv_id": "2408.02882v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02882v1",
    "abstract_url": "http://arxiv.org/abs/2408.02882v1",
    "primary_category": "cs.AI",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Hide and Seek: Fingerprinting Large Language Models with Evolutionary Learning",
    "authors": "Dmitri Iourovitski, Sanat Sharma, Rakshak Talwar",
    "abstract": "As content generated by Large Language Model (LLM) has grown exponentially,\nthe ability to accurately identify and fingerprint such text has become\nincreasingly crucial. In this work, we introduce a novel black-box approach for\nfingerprinting LLMs, achieving an impressive 72% accuracy in identifying the\ncorrect family of models (Such as Llama, Mistral, Gemma, etc) among a lineup of\nLLMs. We present an evolutionary strategy that leverages the capabilities of\none LLM to discover the most salient features for identifying other LLMs. Our\nmethod employs a unique \"Hide and Seek\" algorithm, where an Auditor LLM\ngenerates discriminative prompts, and a Detective LLM analyzes the responses to\nfingerprint the target models. This approach not only demonstrates the\nfeasibility of LLM-driven model identification but also reveals insights into\nthe semantic manifolds of different LLM families. By iteratively refining\nprompts through in-context learning, our system uncovers subtle distinctions\nbetween model outputs, providing a powerful tool for LLM analysis and\nverification. This research opens new avenues for understanding LLM behavior\nand has significant implications for model attribution, security, and the\nbroader field of AI transparency.",
    "arxiv_id": "2408.02871v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02871v1",
    "abstract_url": "http://arxiv.org/abs/2408.02871v1",
    "primary_category": "cs.CR",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Back-Projection Diffusion: Solving the Wideband Inverse Scattering Problem with Diffusion Models",
    "authors": "Borong Zhang, Mart\u00edn Guerra, Qin Li, Leonardo Zepeda-N\u00fa\u00f1ez",
    "abstract": "We present \\textit{Wideband back-projection diffusion}, an end-to-end\nprobabilistic framework for approximating the posterior distribution induced by\nthe inverse scattering map from wideband scattering data. This framework\nleverages conditional diffusion models coupled with the underlying physics of\nwave-propagation and symmetries in the problem, to produce highly accurate\nreconstructions. The framework introduces a factorization of the score function\ninto a physics-based latent representation inspired by the filtered\nback-propagation formula and a conditional score function conditioned on this\nlatent representation. These two steps are also constrained to obey symmetries\nin the formulation while being amenable to compression by imposing the rank\nstructure found in the filtered back-projection formula. As a result,\nempirically, our framework is able to provide sharp reconstructions\neffortlessly, even recovering sub-Nyquist features in the multiple-scattering\nregime. It has low-sample and computational complexity, its number of\nparameters scales sub-linearly with the target resolution, and it has stable\ntraining dynamics.",
    "arxiv_id": "2408.02866v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02866v1",
    "abstract_url": "http://arxiv.org/abs/2408.02866v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "VisionUnite: A Vision-Language Foundation Model for Ophthalmology Enhanced with Clinical Knowledge",
    "authors": "Zihan Li, Diping Song, Zefeng Yang, Deming Wang, Fei Li, Xiulan Zhang, Paul E. Kinahan, Yu Qiao",
    "abstract": "The need for improved diagnostic methods in ophthalmology is acute,\nespecially in the less developed regions with limited access to specialists and\nadvanced equipment. Therefore, we introduce VisionUnite, a novel\nvision-language foundation model for ophthalmology enhanced with clinical\nknowledge. VisionUnite has been pretrained on an extensive dataset comprising\n1.24 million image-text pairs, and further refined using our proposed MMFundus\ndataset, which includes 296,379 high-quality fundus image-text pairs and\n889,137 simulated doctor-patient dialogue instances. Our experiments indicate\nthat VisionUnite outperforms existing generative foundation models such as\nGPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable\nto junior ophthalmologists. VisionUnite performs well in various clinical\nscenarios including open-ended multi-disease diagnosis, clinical explanation,\nand patient interaction, making it a highly versatile tool for initial\nophthalmic disease screening. VisionUnite can also serve as an educational aid\nfor junior ophthalmologists, accelerating their acquisition of knowledge\nregarding both common and rare ophthalmic conditions. VisionUnite represents a\nsignificant advancement in ophthalmology, with broad implications for\ndiagnostics, medical education, and understanding of disease mechanisms.",
    "arxiv_id": "2408.02865v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02865v1",
    "abstract_url": "http://arxiv.org/abs/2408.02865v1",
    "primary_category": "eess.IV",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "On The Stability of Moral Preferences: A Problem with Computational Elicitation Methods",
    "authors": "Kyle Boerstler, Vijay Keswani, Lok Chan, Jana Schaich Borg, Vincent Conitzer, Hoda Heidari, Walter Sinnott-Armstrong",
    "abstract": "Preference elicitation frameworks feature heavily in the research on\nparticipatory ethical AI tools and provide a viable mechanism to enquire and\nincorporate the moral values of various stakeholders. As part of the\nelicitation process, surveys about moral preferences, opinions, and judgments\nare typically administered only once to each participant. This methodological\npractice is reasonable if participants' responses are stable over time such\nthat, all other relevant factors being held constant, their responses today\nwill be the same as their responses to the same questions at a later time.\nHowever, we do not know how often that is the case. It is possible that\nparticipants' true moral preferences change, are subject to temporary moods or\nwhims, or are influenced by environmental factors we don't track. If\nparticipants' moral responses are unstable in such ways, it would raise\nimportant methodological and theoretical issues for how participants' true\nmoral preferences, opinions, and judgments can be ascertained. We address this\npossibility here by asking the same survey participants the same moral\nquestions about which patient should receive a kidney when only one is\navailable ten times in ten different sessions over two weeks, varying only\npresentation order across sessions. We measured how often participants gave\ndifferent responses to simple (Study One) and more complicated (Study Two)\nrepeated scenarios. On average, the fraction of times participants changed\ntheir responses to controversial scenarios was around 10-18% across studies,\nand this instability is observed to have positive associations with response\ntime and decision-making difficulty. We discuss the implications of these\nresults for the efficacy of moral preference elicitation, highlighting the role\nof response instability in causing value misalignment between stakeholders and\nAI tools trained on their moral judgments.",
    "arxiv_id": "2408.02862v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02862v1",
    "abstract_url": "http://arxiv.org/abs/2408.02862v1",
    "primary_category": "cs.CY",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "A Framework for Fine-Tuning LLMs using Heterogeneous Feedback",
    "authors": "Ryan Aponte, Ryan A. Rossi, Shunan Guo, Franck Dernoncourt, Tong Yu, Xiang Chen, Subrata Mitra, Nedim Lipka",
    "abstract": "Large language models (LLMs) have been applied to a wide range of tasks,\nincluding text summarization, web navigation, and chatbots. They have\nbenefitted from supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) following an unsupervised pretraining. These datasets can\nbe difficult to collect, limited in scope, and vary in sample quality.\nAdditionally, datasets can vary extensively in supervision format, from\nnumerical to binary as well as multi-dimensional with many different values. We\npresent a framework for fine-tuning LLMs using heterogeneous feedback, which\nhas two main components. First, we combine the heterogeneous feedback data into\na single supervision format, compatible with methods like SFT and RLHF. Next,\ngiven this unified feedback dataset, we extract a high-quality and diverse\nsubset to obtain performance increases potentially exceeding the full dataset.\nWe conduct extensive experiments to understand the effectiveness of these\ntechniques for incorporating heterogeneous feedback, and demonstrate\nimprovements from using a high-quality and diverse subset of the data. We find\nthat our framework is able to improve models in multiple areas simultaneously,\nsuch as in instruction following and bias reduction.",
    "arxiv_id": "2408.02861v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02861v1",
    "abstract_url": "http://arxiv.org/abs/2408.02861v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Multistain Pretraining for Slide Representation Learning in Pathology",
    "authors": "Guillaume Jaume, Anurag Vaidya, Andrew Zhang, Andrew H. Song, Richard J. Chen, Sharifa Sahai, Dandan Mo, Emilio Madrigal, Long Phi Le, Faisal Mahmood",
    "abstract": "Developing self-supervised learning (SSL) models that can learn universal and\ntransferable representations of H&E gigapixel whole-slide images (WSIs) is\nbecoming increasingly valuable in computational pathology. These models hold\nthe potential to advance critical tasks such as few-shot classification, slide\nretrieval, and patient stratification. Existing approaches for slide\nrepresentation learning extend the principles of SSL from small images (e.g.,\n224 x 224 patches) to entire slides, usually by aligning two different\naugmentations (or views) of the slide. Yet the resulting representation remains\nconstrained by the limited clinical and biological diversity of the views.\nInstead, we postulate that slides stained with multiple markers, such as\nimmunohistochemistry, can be used as different views to form a rich\ntask-agnostic training signal. To this end, we introduce Madeleine, a\nmultimodal pretraining strategy for slide representation learning. Madeleine is\ntrained with a dual global-local cross-stain alignment objective on large\ncohorts of breast cancer samples (N=4,211 WSIs across five stains) and kidney\ntransplant samples (N=12,070 WSIs across four stains). We demonstrate the\nquality of slide representations learned by Madeleine on various downstream\nevaluations, ranging from morphological and molecular classification to\nprognostic prediction, comprising 21 tasks using 7,299 WSIs from multiple\nmedical centers. Code is available at https://github.com/mahmoodlab/MADELEINE.",
    "arxiv_id": "2408.02859v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02859v1",
    "abstract_url": "http://arxiv.org/abs/2408.02859v1",
    "primary_category": "eess.IV",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Active Learning for WBAN-based Health Monitoring",
    "authors": "Cho-Chun Chiu, Tuan Nguyen, Ting He, Shiqiang Wang, Beom-Su Kim, Ki-Il Kim",
    "abstract": "We consider a novel active learning problem motivated by the need of learning\nmachine learning models for health monitoring in wireless body area network\n(WBAN). Due to the limited resources at body sensors, collecting each unlabeled\nsample in WBAN incurs a nontrivial cost. Moreover, training health monitoring\nmodels typically requires labels indicating the patient's health state that\nneed to be generated by healthcare professionals, which cannot be obtained at\nthe same pace as data collection. These challenges make our problem\nfundamentally different from classical active learning, where unlabeled samples\nare free and labels can be queried in real time. To handle these challenges, we\npropose a two-phased active learning method, consisting of an online phase\nwhere a coreset construction algorithm is proposed to select a subset of\nunlabeled samples based on their noisy predictions, and an offline phase where\nthe selected samples are labeled to train the target model. The samples\nselected by our algorithm are proved to yield a guaranteed error in\napproximating the full dataset in evaluating the loss function. Our evaluation\nbased on real health monitoring data and our own experimentation demonstrates\nthat our solution can drastically save the data curation cost without\nsacrificing the quality of the target model.",
    "arxiv_id": "2408.02849v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02849v1",
    "abstract_url": "http://arxiv.org/abs/2408.02849v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Heterogeneous graph attention network improves cancer multiomics integration",
    "authors": "Sina Tabakhi, Charlotte Vandermeulen, Ian Sudbery, Haiping Lu",
    "abstract": "The increase in high-dimensional multiomics data demands advanced integration\nmodels to capture the complexity of human diseases. Graph-based deep learning\nintegration models, despite their promise, struggle with small patient cohorts\nand high-dimensional features, often applying independent feature selection\nwithout modeling relationships among omics. Furthermore, conventional\ngraph-based omics models focus on homogeneous graphs, lacking multiple types of\nnodes and edges to capture diverse structures. We introduce a Heterogeneous\nGraph ATtention network for omics integration (HeteroGATomics) to improve\ncancer diagnosis. HeteroGATomics performs joint feature selection through a\nmulti-agent system, creating dedicated networks of feature and patient\nsimilarity for each omic modality. These networks are then combined into one\nheterogeneous graph for learning holistic omic-specific representations and\nintegrating predictions across modalities. Experiments on three cancer\nmultiomics datasets demonstrate HeteroGATomics' superior performance in cancer\ndiagnosis. Moreover, HeteroGATomics enhances interpretability by identifying\nimportant biomarkers contributing to the diagnosis outcomes.",
    "arxiv_id": "2408.02845v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02845v1",
    "abstract_url": "http://arxiv.org/abs/2408.02845v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Evaluating Posterior Probabilities: Decision Theory, Proper Scoring Rules, and Calibration",
    "authors": "Luciana Ferrer, Daniel Ramos",
    "abstract": "Most machine learning classifiers are designed to output posterior\nprobabilities for the classes given the input sample. These probabilities may\nbe used to make the categorical decision on the class of the sample; provided\nas input to a downstream system; or provided to a human for interpretation.\nEvaluating the quality of the posteriors generated by these system is an\nessential problem which was addressed decades ago with the invention of proper\nscoring rules (PSRs). Unfortunately, much of the recent machine learning\nliterature uses calibration metrics -- most commonly, the expected calibration\nerror (ECE) -- as a proxy to assess posterior performance. The problem with\nthis approach is that calibration metrics reflect only one aspect of the\nquality of the posteriors, ignoring the discrimination performance. For this\nreason, we argue that calibration metrics should play no role in the assessment\nof posterior quality. Expected PSRs should instead be used for this job,\npreferably normalized for ease of interpretation. In this work, we first give a\nbrief review of PSRs from a practical perspective, motivating their definition\nusing Bayes decision theory. We discuss why expected PSRs provide a principled\nmeasure of the quality of a system's posteriors and why calibration metrics are\nnot the right tool for this job. We argue that calibration metrics, while not\nuseful for performance assessment, may be used as diagnostic tools during\nsystem development. With this purpose in mind, we discuss a simple and\npractical calibration metric, called calibration loss, derived from a\ndecomposition of expected PSRs. We compare this metric with the ECE and with\nthe expected score divergence calibration metric from the PSR literature and\nargue, using theoretical and empirical evidence, that calibration loss is\nsuperior to these two metrics.",
    "arxiv_id": "2408.02841v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02841v1",
    "abstract_url": "http://arxiv.org/abs/2408.02841v1",
    "primary_category": "stat.ML",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Optimizing Cox Models with Stochastic Gradient Descent: Theoretical Foundations and Practical Guidances",
    "authors": "Lang Zeng, Weijing Tang, Zhao Ren, Ying Ding",
    "abstract": "Optimizing Cox regression and its neural network variants poses substantial\ncomputational challenges in large-scale studies. Stochastic gradient descent\n(SGD), known for its scalability in model optimization, has recently been\nadapted to optimize Cox models. Unlike its conventional application, which\ntypically targets a sum of independent individual loss, SGD for Cox models\nupdates parameters based on the partial likelihood of a subset of data. Despite\nits empirical success, the theoretical foundation for optimizing Cox partial\nlikelihood with SGD is largely underexplored. In this work, we demonstrate that\nthe SGD estimator targets an objective function that is batch-size-dependent.\nWe establish that the SGD estimator for the Cox neural network (Cox-NN) is\nconsistent and achieves the optimal minimax convergence rate up to a\npolylogarithmic factor. For Cox regression, we further prove the\n$\\sqrt{n}$-consistency and asymptotic normality of the SGD estimator, with\nvariance depending on the batch size. Furthermore, we quantify the impact of\nbatch size on Cox-NN training and its effect on the SGD estimator's asymptotic\nefficiency in Cox regression. These findings are validated by extensive\nnumerical experiments and provide guidance for selecting batch sizes in SGD\napplications. Finally, we demonstrate the effectiveness of SGD in a real-world\napplication where GD is unfeasible due to the large scale of data.",
    "arxiv_id": "2408.02839v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02839v1",
    "abstract_url": "http://arxiv.org/abs/2408.02839v1",
    "primary_category": "stat.ML",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Interpretation of the Intent Detection Problem as Dynamics in a Low-dimensional Space",
    "authors": "Eduardo Sanchez-Karhunen, Jose F. Quesada-Moreno, Miguel A. Guti\u00e9rrez-Naranjo",
    "abstract": "Intent detection is a text classification task whose aim is to recognize and\nlabel the semantics behind a users query. It plays a critical role in various\nbusiness applications. The output of the intent detection module strongly\nconditions the behavior of the whole system. This sequence analysis task is\nmainly tackled using deep learning techniques. Despite the widespread use of\nthese techniques, the internal mechanisms used by networks to solve the problem\nare poorly understood. Recent lines of work have analyzed the computational\nmechanisms learned by RNNs from a dynamical systems perspective. In this work,\nwe investigate how different RNN architectures solve the SNIPS intent detection\nproblem. Sentences injected into trained networks can be interpreted as\ntrajectories traversing a hidden state space. This space is constrained to a\nlow-dimensional manifold whose dimensionality is related to the embedding and\nhidden layer sizes. To generate predictions, RNN steers the trajectories\ntowards concrete regions, spatially aligned with the output layer matrix rows\ndirections. Underlying the system dynamics, an unexpected fixed point topology\nhas been identified with a limited number of attractors. Our results provide\nnew insights into the inner workings of networks that solve the intent\ndetection task.",
    "arxiv_id": "2408.02838v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02838v1",
    "abstract_url": "http://arxiv.org/abs/2408.02838v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Training a multilayer dynamical spintronic network with standard machine learning tools to perform time series classification",
    "authors": "Erwan Plouet, D\u00e9dalo Sanz-Hern\u00e1ndez, Aymeric Vecchiola, Julie Grollier, Frank Mizrahi",
    "abstract": "The ability to process time-series at low energy cost is critical for many\napplications. Recurrent neural network, which can perform such tasks, are\ncomputationally expensive when implementing in software on conventional\ncomputers. Here we propose to implement a recurrent neural network in hardware\nusing spintronic oscillators as dynamical neurons. Using numerical simulations,\nwe build a multi-layer network and demonstrate that we can use backpropagation\nthrough time (BPTT) and standard machine learning tools to train this network.\nLeveraging the transient dynamics of the spintronic oscillators, we solve the\nsequential digits classification task with $89.83\\pm2.91~\\%$ accuracy, as good\nas the equivalent software network. We devise guidelines on how to choose the\ntime constant of the oscillators as well as hyper-parameters of the network to\nadapt to different input time scales.",
    "arxiv_id": "2408.02835v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02835v1",
    "abstract_url": "http://arxiv.org/abs/2408.02835v1",
    "primary_category": "cond-mat.dis-nn",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "DaCapo: a modular deep learning framework for scalable 3D image segmentation",
    "authors": "William Patton, Jeff L. Rhoades, Marwan Zouinkhi, David G. Ackerman, Caroline Malin-Mayor, Diane Adjavon, Larissa Heinrich, Davis Bennett, Yurii Zubov, CellMap Project Team, Aubrey V. Weigel, Jan Funke",
    "abstract": "DaCapo is a specialized deep learning library tailored to expedite the\ntraining and application of existing machine learning approaches on large,\nnear-isotropic image data. In this correspondence, we introduce DaCapo's unique\nfeatures optimized for this specific domain, highlighting its modular\nstructure, efficient experiment management tools, and scalable deployment\ncapabilities. We discuss its potential to improve access to large-scale,\nisotropic image segmentation and invite the community to explore and contribute\nto this open-source initiative.",
    "arxiv_id": "2408.02834v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02834v1",
    "abstract_url": "http://arxiv.org/abs/2408.02834v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Adaptive Learning for Quantum Linear Regression",
    "authors": "Costantino Carugno, Maurizio Ferrari Dacrema, Paolo Cremonesi",
    "abstract": "The recent availability of quantum annealers as cloud-based services has\nenabled new ways to handle machine learning problems, and several relevant\nalgorithms have been adapted to run on these devices. In a recent work, linear\nregression was formulated as a quadratic binary optimization problem that can\nbe solved via quantum annealing. Although this approach promises a\ncomputational time advantage for large datasets, the quality of the solution is\nlimited by the necessary use of a precision vector, used to approximate the\nreal-numbered regression coefficients in the quantum formulation. In this work,\nwe focus on the practical challenge of improving the precision vector encoding:\ninstead of setting an array of generic values equal for all coefficients, we\nallow each one to be expressed by its specific precision, which is tuned with a\nsimple adaptive algorithm. This approach is evaluated on synthetic datasets of\nincreasing size, and linear regression is solved using the D-Wave Advantage\nquantum annealer, as well as classical solvers. To the best of our knowledge,\nthis is the largest dataset ever evaluated for linear regression on a quantum\nannealer. The results show that our formulation is able to deliver improved\nsolution quality in all instances, and could better exploit the potential of\ncurrent quantum devices.",
    "arxiv_id": "2408.02833v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02833v1",
    "abstract_url": "http://arxiv.org/abs/2408.02833v1",
    "primary_category": "quant-ph",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Setting the duration of online A/B experiments",
    "authors": "Harrison H. Li, Chaoyu Yu",
    "abstract": "In designing an online A/B experiment, it is crucial to select a sample size\nand duration that ensure the resulting confidence interval (CI) for the\ntreatment effect is the right width to detect an effect of meaningful magnitude\nwith sufficient statistical power without wasting resources. While the\nrelationship between sample size and CI width is well understood, the effect of\nexperiment duration on CI width remains less clear. This paper provides an\nanalytical formula for the width of a CI based on a ratio treatment effect\nestimator as a function of both sample size (N) and duration (T). The formula\nis derived from a mixed effects model with two variance components. One\ncomponent, referred to as the temporal variance, persists over time for\nexperiments where the same users are kept in the same experiment arm across\ndifferent days. The remaining error variance component, by contrast, decays to\nzero as T gets large. The formula we derive introduces a key parameter that we\ncall the user-specific temporal correlation (UTC), which quantifies the\nrelative sizes of the two variance components and can be estimated from\nhistorical experiments. Higher UTC indicates a slower decay in CI width over\ntime. On the other hand, when the UTC is 0 -- as for experiments where users\nshuffle in and out of the experiment across days -- the CI width decays at the\nstandard parametric 1/T rate. We also study how access to pre-period data for\nthe users in the experiment affects the CI width decay. We show our formula\nclosely explains CI widths on real A/B experiments at YouTube.",
    "arxiv_id": "2408.02830v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02830v1",
    "abstract_url": "http://arxiv.org/abs/2408.02830v1",
    "primary_category": "stat.ME",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Wave-RVFL: A Randomized Neural Network Based on Wave Loss Function",
    "authors": "M. Sajid, A. Quadir, M. Tanveer",
    "abstract": "The random vector functional link (RVFL) network is well-regarded for its\nstrong generalization capabilities in the field of machine learning. However,\nits inherent dependencies on the square loss function make it susceptible to\nnoise and outliers. Furthermore, the calculation of RVFL's unknown parameters\nnecessitates matrix inversion of the entire training sample, which constrains\nits scalability. To address these challenges, we propose the Wave-RVFL, an RVFL\nmodel incorporating the wave loss function. We formulate and solve the proposed\noptimization problem of the Wave-RVFL using the adaptive moment estimation\n(Adam) algorithm in a way that successfully eliminates the requirement for\nmatrix inversion and significantly enhances scalability. The Wave-RVFL exhibits\nrobustness against noise and outliers by preventing over-penalization of\ndeviations, thereby maintaining a balanced approach to managing noise and\noutliers. The proposed Wave-RVFL model is evaluated on multiple UCI datasets,\nboth with and without the addition of noise and outliers, across various\ndomains and sizes. Empirical results affirm the superior performance and\nrobustness of the Wave-RVFL compared to baseline models, establishing it as a\nhighly effective and scalable classification solution.",
    "arxiv_id": "2408.02824v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02824v1",
    "abstract_url": "http://arxiv.org/abs/2408.02824v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Continuous Monitoring via Repeated Significance",
    "authors": "Eric Bax, Arundhyoti Sarkar, Alex Shtoff",
    "abstract": "Requiring statistical significance at multiple interim analyses to declare a\nstatistically significant result for an AB test allows less stringent\nrequirements for significance at each interim analysis. Repeated repeated\nsignificance competes well with methods built on assumptions about the test --\nassumptions that may be impossible to evaluate a priori and may require extra\ndata to evaluate empirically.\n  Instead, requiring repeated significance allows the data itself to prove\ndirectly that the required results are not due to chance alone. We explain how\nto apply tests with repeated significance to continuously monitor unbounded\ntests -- tests that do not have an a priori bound on running time or number of\nobservations. We show that it is impossible to maintain a constant requirement\nfor significance for unbounded tests, but that we can come arbitrarily close to\nthat goal.",
    "arxiv_id": "2408.02821v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02821v1",
    "abstract_url": "http://arxiv.org/abs/2408.02821v1",
    "primary_category": "stat.ME",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Pre-trained Encoder Inference: Revealing Upstream Encoders In Downstream Machine Learning Services",
    "authors": "Shaopeng Fu, Xuexue Sun, Ke Qing, Tianhang Zheng, Di Wang",
    "abstract": "Though pre-trained encoders can be easily accessed online to build downstream\nmachine learning (ML) services quickly, various attacks have been designed to\ncompromise the security and privacy of these encoders. While most attacks\ntarget encoders on the upstream side, it remains unknown how an encoder could\nbe threatened when deployed in a downstream ML service. This paper unveils a\nnew vulnerability: the Pre-trained Encoder Inference (PEI) attack, which posts\nprivacy threats toward encoders hidden behind downstream ML services. By only\nproviding API accesses to a targeted downstream service and a set of candidate\nencoders, the PEI attack can infer which encoder is secretly used by the\ntargeted service based on candidate ones. We evaluate the attack performance of\nPEI against real-world encoders on three downstream tasks: image\nclassification, text classification, and text-to-image generation. Experiments\nshow that the PEI attack succeeds in revealing the hidden encoder in most cases\nand seldom makes mistakes even when the hidden encoder is not in the candidate\nset. We also conducted a case study on one of the most recent vision-language\nmodels, LLaVA, to illustrate that the PEI attack is useful in assisting other\nML attacks such as adversarial attacks. The code is available at\nhttps://github.com/fshp971/encoder-inference.",
    "arxiv_id": "2408.02814v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02814v1",
    "abstract_url": "http://arxiv.org/abs/2408.02814v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Mitigating Malicious Attacks in Federated Learning via Confidence-aware Defense",
    "authors": "Qilei Li, Ahmed M. Abdelmoniem",
    "abstract": "Federated Learning (FL) is an emerging distributed machine learning paradigm\nthat allows multiple clients to collaboratively train a global model without\nsharing private local data. However, FL systems are vulnerable to attacks from\nmalicious clients, who can degrade the global model performance through data\npoisoning and model poisoning. Existing defense methods typically focus on a\nsingle type of attack, such as Byzantine attacks or backdoor attacks, and are\noften ineffective against potential data poisoning attacks like label flipping\nand label shuffling. Additionally, these methods often lack accuracy and\nrobustness in detecting and handling malicious updates. To address these\nissues, we propose a novel method based on model confidence scores, which\nevaluates the uncertainty of client model updates to detect and defend against\nmalicious clients. Our approach is comprehensively effective for both model\npoisoning and data poisoning attacks and is capable of accurately identifying\nand mitigating potential malicious updates from being aggregated. Experimental\nresults demonstrate that our method significantly improves the robustness of FL\nsystems against various types of attacks, also achieving higher model accuracy\nand stability across various scenarios.",
    "arxiv_id": "2408.02813v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02813v1",
    "abstract_url": "http://arxiv.org/abs/2408.02813v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Development of REGAI: Rubric Enabled Generative Artificial Intelligence",
    "authors": "Zach Johnson, Jeremy Straub",
    "abstract": "This paper presents and evaluates a new retrieval augmented generation (RAG)\nand large language model (LLM)-based artificial intelligence (AI) technique:\nrubric enabled generative artificial intelligence (REGAI). REGAI uses rubrics,\nwhich can be created manually or automatically by the system, to enhance the\nperformance of LLMs for evaluation purposes. REGAI improves on the performance\nof both classical LLMs and RAG-based LLM techniques. This paper describes\nREGAI, presents data regarding its performance and discusses several possible\napplication areas for the technology.",
    "arxiv_id": "2408.02811v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02811v1",
    "abstract_url": "http://arxiv.org/abs/2408.02811v1",
    "primary_category": "cs.AI",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Deciphering Air Travel Disruptions: A Machine Learning Approach",
    "authors": "Aravinda Jatavallabha, Jacob Gerlach, Aadithya Naresh",
    "abstract": "This research investigates flight delay trends by examining factors such as\ndeparture time, airline, and airport. It employs regression machine learning\nmethods to predict the contributions of various sources to delays. Time-series\nmodels, including LSTM, Hybrid LSTM, and Bi-LSTM, are compared with baseline\nregression models such as Multiple Regression, Decision Tree Regression, Random\nForest Regression, and Neural Network. Despite considerable errors in the\nbaseline models, the study aims to identify influential features in delay\nprediction, potentially informing flight planning strategies. Unlike previous\nwork, this research focuses on regression tasks and explores the use of\ntime-series models for predicting flight delays. It offers insights into\naviation operations by independently analyzing each delay component (e.g.,\nsecurity, weather).",
    "arxiv_id": "2408.02802v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02802v1",
    "abstract_url": "http://arxiv.org/abs/2408.02802v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Sparse Deep Learning Models with the $\\ell_1$ Regularization",
    "authors": "Lixin Shen, Rui Wang, Yuesheng Xu, Mingsong Yan",
    "abstract": "Sparse neural networks are highly desirable in deep learning in reducing its\ncomplexity. The goal of this paper is to study how choices of regularization\nparameters influence the sparsity level of learned neural networks. We first\nderive the $\\ell_1$-norm sparsity-promoting deep learning models including\nsingle and multiple regularization parameters models, from a statistical\nviewpoint. We then characterize the sparsity level of a regularized neural\nnetwork in terms of the choice of the regularization parameters. Based on the\ncharacterizations, we develop iterative algorithms for selecting regularization\nparameters so that the weight parameters of the resulting deep neural network\nenjoy prescribed sparsity levels. Numerical experiments are presented to\ndemonstrate the effectiveness of the proposed algorithms in choosing desirable\nregularization parameters and obtaining corresponding neural networks having\nboth of predetermined sparsity levels and satisfactory approximation accuracy.",
    "arxiv_id": "2408.02801v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02801v1",
    "abstract_url": "http://arxiv.org/abs/2408.02801v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Examining Gender and Power on Wikipedia Through Face and Politeness",
    "authors": "Adil Soubki, Shyne Choi, Owen Rambow",
    "abstract": "We propose a framework for analyzing discourse by combining two\ninterdependent concepts from sociolinguistic theory: face acts and politeness.\nWhile politeness has robust existing tools and data, face acts are less\nresourced. We introduce a new corpus created by annotating Wikipedia talk pages\nwith face acts and we use this to train a face act tagger. We then employ our\nframework to study how face and politeness interact with gender and power in\ndiscussions between Wikipedia editors. Among other findings, we observe that\nfemale Wikipedians are not only more polite, which is consistent with prior\nstudies, but that this difference corresponds with significantly more language\ndirected at humbling aspects of their own face. Interestingly, the distinction\nnearly vanishes once limiting to editors with administrative power.",
    "arxiv_id": "2408.02798v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02798v1",
    "abstract_url": "http://arxiv.org/abs/2408.02798v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Algorithm-Informed Graph Neural Networks for Leakage Detection and Localization in Water Distribution Networks",
    "authors": "Zepeng Zhang, Olga Fink",
    "abstract": "Detecting and localizing leakages is a significant challenge for the\nefficient and sustainable management of water distribution networks (WDN).\nLeveraging the inherent graph structure of WDNs, recent approaches have used\ngraph-based data-driven methods. However, these methods often learn shortcuts\nthat work well with in-distribution data but fail to generalize to\nout-of-distribution data. To address this limitation and inspired by the\nperfect generalization ability of classical algorithms, we propose an\nalgorithm-informed graph neural network (AIGNN). Recognizing that WDNs function\nas flow networks, incorporating max-flow information can be beneficial for\ninferring pressures. In the proposed framework, we first train AIGNN to emulate\nthe Ford-Fulkerson algorithm for solving max-flow problems. This algorithmic\nknowledge is then transferred to address the pressure estimation problem in\nWDNs. Two AIGNNs are deployed, one to reconstruct pressure based on the current\nmeasurements, and another to predict pressure based on previous measurements.\nLeakages are detected and localized by comparing the outputs of the\nreconstructor and the predictor. By pretraining AIGNNs to reason like\nalgorithms, they are expected to extract more task-relevant and generalizable\nfeatures. Experimental results demonstrate that the proposed algorithm-informed\napproach achieves superior results with better generalization ability compared\nto GNNs that do not incorporate algorithmic knowledge.",
    "arxiv_id": "2408.02797v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02797v1",
    "abstract_url": "http://arxiv.org/abs/2408.02797v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "4D-Var using Hessian approximation and backpropagation applied to automatically-differentiable numerical and machine learning models",
    "authors": "Kylen Solvik, Stephen G. Penny, Stephan Hoyer",
    "abstract": "Constraining a numerical weather prediction (NWP) model with observations via\n4D variational (4D-Var) data assimilation is often difficult to implement in\npractice due to the need to develop and maintain a software-based tangent\nlinear model and adjoint model. One of the most common 4D-Var algorithms uses\nan incremental update procedure, which has been shown to be an approximation of\nthe Gauss-Newton method. Here we demonstrate that when using a forecast model\nthat supports automatic differentiation, an efficient and in some cases more\naccurate alternative approximation of the Gauss-Newton method can be applied by\ncombining backpropagation of errors with Hessian approximation. This approach\ncan be used with either a conventional numerical model implemented within a\nsoftware framework that supports automatic differentiation, or a machine\nlearning (ML) based surrogate model. We test the new approach on a variety of\nLorenz-96 and quasi-geostrophic models. The results indicate potential for a\ndeeper integration of modeling, data assimilation, and new technologies in a\nnext-generation of operational forecast systems that leverage weather models\ndesigned to support automatic differentiation.",
    "arxiv_id": "2408.02767v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02767v1",
    "abstract_url": "http://arxiv.org/abs/2408.02767v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "ConDL: Detector-Free Dense Image Matching",
    "authors": "Monika Kwiatkowski, Simon Matern, Olaf Hellwich",
    "abstract": "In this work, we introduce a deep-learning framework designed for estimating\ndense image correspondences. Our fully convolutional model generates dense\nfeature maps for images, where each pixel is associated with a descriptor that\ncan be matched across multiple images. Unlike previous methods, our model is\ntrained on synthetic data that includes significant distortions, such as\nperspective changes, illumination variations, shadows, and specular highlights.\nUtilizing contrastive learning, our feature maps achieve greater invariance to\nthese distortions, enabling robust matching. Notably, our method eliminates the\nneed for a keypoint detector, setting it apart from many existing\nimage-matching techniques.",
    "arxiv_id": "2408.02766v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02766v1",
    "abstract_url": "http://arxiv.org/abs/2408.02766v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Dimensionality Reduction and Nearest Neighbors for Improving Out-of-Distribution Detection in Medical Image Segmentation",
    "authors": "McKell Woodland, Nihil Patel, Austin Castelo, Mais Al Taie, Mohamed Eltaher, Joshua P. Yung, Tucker J. Netherton, Tiffany L. Calderone, Jessica I. Sanchez, Darrel W. Cleere, Ahmed Elsaiey, Nakul Gupta, David Victor, Laura Beretta, Ankit B. Patel Kristy K. Brock",
    "abstract": "Clinically deployed deep learning-based segmentation models are known to fail\non data outside of their training distributions. While clinicians review the\nsegmentations, these models tend to perform well in most instances, which could\nexacerbate automation bias. Therefore, detecting out-of-distribution images at\ninference is critical to warn the clinicians that the model likely failed. This\nwork applied the Mahalanobis distance (MD) post hoc to the bottleneck features\nof four Swin UNETR and nnU-net models that segmented the liver on T1-weighted\nmagnetic resonance imaging and computed tomography. By reducing the dimensions\nof the bottleneck features with either principal component analysis or uniform\nmanifold approximation and projection, images the models failed on were\ndetected with high performance and minimal computational load. In addition,\nthis work explored a non-parametric alternative to the MD, a k-th nearest\nneighbors distance (KNN). KNN drastically improved scalability and performance\nover MD when both were applied to raw and average-pooled bottleneck features.",
    "arxiv_id": "2408.02761v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02761v1",
    "abstract_url": "http://arxiv.org/abs/2408.02761v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Classification of Raw MEG/EEG Data with Detach-Rocket Ensemble: An Improved ROCKET Algorithm for Multivariate Time Series Analysis",
    "authors": "Adri\u00e0 Solana, Erik Frans\u00e9n, Gonzalo Uribarri",
    "abstract": "Multivariate Time Series Classification (MTSC) is a ubiquitous problem in\nscience and engineering, particularly in neuroscience, where most data\nacquisition modalities involve the simultaneous time-dependent recording of\nbrain activity in multiple brain regions. In recent years, Random Convolutional\nKernel models such as ROCKET and MiniRocket have emerged as highly effective\ntime series classification algorithms, capable of achieving state-of-the-art\naccuracy results with low computational load. Despite their success, these\ntypes of models face two major challenges when employed in neuroscience: 1)\nthey struggle to deal with high-dimensional data such as EEG and MEG, and 2)\nthey are difficult to interpret. In this work, we present a novel ROCKET-based\nalgorithm, named Detach-Rocket Ensemble, that is specifically designed to\naddress these two problems in MTSC. Our algorithm leverages pruning to provide\nan integrated estimation of channel importance, and ensembles to achieve better\naccuracy and provide a label probability. Using a synthetic multivariate time\nseries classification dataset in which we control the amount of information\ncarried by each of the channels, we first show that our algorithm is able to\ncorrectly recover the channel importance for classification. Then, using two\nreal-world datasets, a MEG dataset and an EEG dataset, we show that\nDetach-Rocket Ensemble is able to provide both interpretable channel relevance\nand competitive classification accuracy, even when applied directly to the raw\nbrain data, without the need for feature engineering.",
    "arxiv_id": "2408.02760v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02760v1",
    "abstract_url": "http://arxiv.org/abs/2408.02760v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "A Novel Hybrid Approach for Tornado Prediction in the United States: Kalman-Convolutional BiLSTM with Multi-Head Attention",
    "authors": "Jiawei Zhou",
    "abstract": "Tornadoes are among the most intense atmospheric vortex phenomena and pose\nsignificant challenges for detection and forecasting. Conventional methods,\nwhich heavily depend on ground-based observations and radar data, are limited\nby issues such as decreased accuracy over greater distances and a high rate of\nfalse positives. To address these challenges, this study utilizes the Seamless\nHybrid Scan Reflectivity (SHSR) dataset from the Multi-Radar Multi-Sensor\n(MRMS) system, which integrates data from multiple radar sources to enhance\naccuracy. A novel hybrid model, the Kalman-Convolutional BiLSTM with Multi-Head\nAttention, is introduced to improve dynamic state estimation and capture both\nspatial and temporal dependencies within the data. This model demonstrates\nsuperior performance in precision, recall, F1-Score, and accuracy compared to\nmethods such as K-Nearest Neighbors (KNN) and LightGBM. The results highlight\nthe considerable potential of advanced machine learning techniques to improve\ntornado prediction and reduce false alarm rates. Future research will focus on\nexpanding datasets, exploring innovative model architectures, and incorporating\nlarge language models (LLMs) to provide deeper insights. This research\nintroduces a novel model for tornado prediction, offering a robust framework\nfor enhancing forecasting accuracy and public safety.",
    "arxiv_id": "2408.02751v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02751v1",
    "abstract_url": "http://arxiv.org/abs/2408.02751v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "KAN we improve on HEP classification tasks? Kolmogorov-Arnold Networks applied to an LHC physics example",
    "authors": "Johannes Erdmann, Florian Mausolf, Jan Lukas Sp\u00e4h",
    "abstract": "Recently, Kolmogorov-Arnold Networks (KANs) have been proposed as an\nalternative to multilayer perceptrons, suggesting advantages in performance and\ninterpretability. We study a typical binary event classification task in\nhigh-energy physics including high-level features and comment on the\nperformance and interpretability of KANs in this context. We find that the\nlearned activation functions of a one-layer KAN resemble the log-likelihood\nratio of the input features. In deeper KANs, the activations in the first KAN\nlayer differ from those in the one-layer KAN, which indicates that the deeper\nKANs learn more complex representations of the data. We study KANs with\ndifferent depths and widths and we compare them to multilayer perceptrons in\nterms of performance and number of trainable parameters. For the chosen\nclassification task, we do not find that KANs are more parameter efficient.\nHowever, small KANs may offer advantages in terms of interpretability that come\nat the cost of only a moderate loss in performance.",
    "arxiv_id": "2408.02743v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02743v1",
    "abstract_url": "http://arxiv.org/abs/2408.02743v1",
    "primary_category": "hep-ph",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  }
]