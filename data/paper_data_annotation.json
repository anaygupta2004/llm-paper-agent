[
  {
    "title": "LLaVA-OneVision: Easy Visual Task Transfer",
    "authors": "Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li",
    "abstract": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.",
    "arxiv_id": "2408.03326v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03326v1",
    "abstract_url": "http://arxiv.org/abs/2408.03326v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "ClassiFIM: An Unsupervised Method To Detect Phase Transitions",
    "authors": "Victor Kasatkin, Evgeny Mozgunov, Nicholas Ezzell, Utkarsh Mishra, Itay Hen, Daniel Lidar",
    "abstract": "Estimation of the Fisher Information Metric (FIM-estimation) is an important\ntask that arises in unsupervised learning of phase transitions, a problem\nproposed by physicists. This work completes the definition of the task by\ndefining rigorous evaluation metrics distMSE, distMSEPS, and distRE and\nintroduces ClassiFIM, a novel machine learning method designed to solve the\nFIM-estimation task. Unlike existing methods for unsupervised learning of phase\ntransitions, ClassiFIM directly estimates a well-defined quantity (the FIM),\nallowing it to be rigorously compared to any present and future other methods\nthat estimate the same. ClassiFIM transforms a dataset for the FIM-estimation\ntask into a dataset for an auxiliary binary classification task and involves\nselecting and training a model for the latter. We prove that the output of\nClassiFIM approaches the exact FIM in the limit of infinite dataset size and\nunder certain regularity conditions. We implement ClassiFIM on multiple\ndatasets, including datasets describing classical and quantum phase\ntransitions, and find that it achieves a good ground truth approximation with\nmodest computational resources. Furthermore, we independently implement two\nalternative state-of-the-art methods for unsupervised estimation of phase\ntransition locations on the same datasets and find that ClassiFIM predicts such\nlocations at least as well as these other methods. To emphasize the generality\nof our method, we also propose and generate the MNIST-CNN dataset, which\nconsists of the output of CNNs trained on MNIST for different hyperparameter\nchoices. Using ClassiFIM on this dataset suggests there is a phase transition\nin the distribution of image-prediction pairs for CNNs trained on MNIST,\ndemonstrating the broad scope of FIM-estimation beyond physics.",
    "arxiv_id": "2408.03323v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03323v1",
    "abstract_url": "http://arxiv.org/abs/2408.03323v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Hedge Fund Portfolio Construction Using PolyModel Theory and iTransformer",
    "authors": "Siqiao Zhao, Zhikang Dong, Zeyu Cao, Raphael Douady",
    "abstract": "When constructing portfolios, a key problem is that a lot of financial time\nseries data are sparse, making it challenging to apply machine learning\nmethods. Polymodel theory can solve this issue and demonstrate superiority in\nportfolio construction from various aspects. To implement the PolyModel theory\nfor constructing a hedge fund portfolio, we begin by identifying an asset pool,\nutilizing over 10,000 hedge funds for the past 29 years' data. PolyModel theory\nalso involves choosing a wide-ranging set of risk factors, which includes\nvarious financial indices, currencies, and commodity prices. This comprehensive\nselection mirrors the complexities of the real-world environment. Leveraging on\nthe PolyModel theory, we create quantitative measures such as Long-term Alpha,\nLong-term Ratio, and SVaR. We also use more classical measures like the Sharpe\nratio or Morningstar's MRAR. To enhance the performance of the constructed\nportfolio, we also employ the latest deep learning techniques (iTransformer) to\ncapture the upward trend, while efficiently controlling the downside, using all\nthe features. The iTransformer model is specifically designed to address the\nchallenges in high-dimensional time series forecasting and could largely\nimprove our strategies. More precisely, our strategies achieve better Sharpe\nratio and annualized return. The above process enables us to create multiple\nportfolio strategies aiming for high returns and low risks when compared to\nvarious benchmarks.",
    "arxiv_id": "2408.03320v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03320v1",
    "abstract_url": "http://arxiv.org/abs/2408.03320v1",
    "primary_category": "q-fin.PM",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Training LLMs to Recognize Hedges in Spontaneous Narratives",
    "authors": "Amie J. Paige, Adil Soubki, John Murzaku, Owen Rambow, Susan E. Brennan",
    "abstract": "Hedges allow speakers to mark utterances as provisional, whether to signal\nnon-prototypicality or \"fuzziness\", to indicate a lack of commitment to an\nutterance, to attribute responsibility for a statement to someone else, to\ninvite input from a partner, or to soften critical feedback in the service of\nface-management needs. Here we focus on hedges in an experimentally\nparameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced\nfrom memory by 21 speakers for co-present addressees, transcribed to text\n(Galati and Brennan, 2010). We created a gold standard of hedges annotated by\nhuman coders (the Roadrunner-Hedge corpus) and compared three LLM-based\napproaches for hedge detection: fine-tuning BERT, and zero and few-shot\nprompting with GPT-4o and LLaMA-3. The best-performing approach was a\nfine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on\nthe top performing approaches, we used an LLM-in-the-Loop approach to improve\nthe gold standard coding, as well as to highlight cases in which hedges are\nambiguous in linguistically interesting ways that will guide future research.\nThis is the first step in our research program to train LLMs to interpret and\ngenerate collateral signals appropriately and meaningfully in conversation.",
    "arxiv_id": "2408.03319v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03319v1",
    "abstract_url": "http://arxiv.org/abs/2408.03319v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
    "authors": "Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar",
    "abstract": "Enabling LLMs to improve their outputs by using more test-time computation is\na critical step towards building generally self-improving agents that can\noperate on open-ended natural language. In this paper, we study the scaling of\ninference-time computation in LLMs, with a focus on answering the question: if\nan LLM is allowed to use a fixed but non-trivial amount of inference-time\ncompute, how much can it improve its performance on a challenging prompt?\nAnswering this question has implications not only on the achievable performance\nof LLMs, but also on the future of LLM pretraining and how one should tradeoff\ninference-time and pre-training compute. Despite its importance, little\nresearch attempted to understand the scaling behaviors of various test-time\ninference methods. Moreover, current work largely provides negative results for\na number of these strategies. In this work, we analyze two primary mechanisms\nto scale test-time computation: (1) searching against dense, process-based\nverifier reward models; and (2) updating the model's distribution over a\nresponse adaptively, given the prompt at test time. We find that in both cases,\nthe effectiveness of different approaches to scaling test-time compute\ncritically varies depending on the difficulty of the prompt. This observation\nmotivates applying a \"compute-optimal\" scaling strategy, which acts to most\neffectively allocate test-time compute adaptively per prompt. Using this\ncompute-optimal strategy, we can improve the efficiency of test-time compute\nscaling by more than 4x compared to a best-of-N baseline. Additionally, in a\nFLOPs-matched evaluation, we find that on problems where a smaller base model\nattains somewhat non-trivial success rates, test-time compute can be used to\noutperform a 14x larger model.",
    "arxiv_id": "2408.03314v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03314v1",
    "abstract_url": "http://arxiv.org/abs/2408.03314v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Pre-training and in-context learning IS Bayesian inference a la De Finetti",
    "authors": "Naimeng Ye, Hanming Yang, Andrew Siah, Hongseok Namkoong",
    "abstract": "Accurately gauging uncertainty on the underlying environment is a\nlongstanding goal of intelligent systems. We characterize which latent concepts\npre-trained sequence models are naturally able to reason with. We go back to De\nFinetti's predictive view of Bayesian reasoning: instead of modeling latent\nparameters through priors and likelihoods like topic models do, De Finetti has\nlong advocated for modeling exchangeable (permutation invariant) sequences of\nobservables. According to this view, pre-training autoregressive models\nformulates informed beliefs based on prior observations (\"empirical Bayes\"),\nand forward generation is a simulated instantiation of an environment\n(\"posterior inference\"). This connection allows extending in-context learning\n(ICL) beyond predictive settings, highlighting sequence models' ability to\nperform explicit statistical inference. In particular, we show the sequence\nprediction loss over exchangeable documents controls performance on downstream\ntasks where uncertainty quantification is key. Empirically, we propose and\ndemonstrate several approaches for encoding exchangeability in sequence model\narchitectures: data augmentation, regularization, and causal masking.",
    "arxiv_id": "2408.03307v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03307v1",
    "abstract_url": "http://arxiv.org/abs/2408.03307v1",
    "primary_category": "stat.ML",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks",
    "authors": "Rafael Sterzinger, Christian Stippel, Robert Sablatnig",
    "abstract": "Etruscan mirrors constitute a significant category in Etruscan art,\ncharacterized by elaborate figurative illustrations featured on their backside.\nA laborious and costly aspect of their analysis and documentation is the task\nof manually tracing these illustrations. In previous work, a methodology has\nbeen proposed to automate this process, involving photometric-stereo scanning\nin combination with deep neural networks. While achieving quantitative\nperformance akin to an expert annotator, some results still lack qualitative\nprecision and, thus, require annotators for inspection and potential\ncorrection, maintaining resource intensity. In response, we propose a deep\nneural network trained to interactively refine existing annotations based on\nhuman guidance. Our human-in-the-loop approach streamlines annotation,\nachieving equal quality with up to 75% less manual input required. Moreover,\nduring the refinement process, the relative improvement of our methodology over\npure manual labeling reaches peak values of up to 26%, attaining drastically\nbetter quality quicker. By being tailored to the complex task of segmenting\nintricate lines, specifically distinguishing it from previous methods, our\napproach offers drastic improvements in efficacy, transferable to a broad\nspectrum of applications beyond Etruscan mirrors.",
    "arxiv_id": "2408.03304v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03304v1",
    "abstract_url": "http://arxiv.org/abs/2408.03304v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Understanding How Blind Users Handle Object Recognition Errors: Strategies and Challenges",
    "authors": "Jonggi Hong, Hernisa Kacorri",
    "abstract": "Object recognition technologies hold the potential to support blind and\nlow-vision people in navigating the world around them. However, the gap between\nbenchmark performances and practical usability remains a significant challenge.\nThis paper presents a study aimed at understanding blind users' interaction\nwith object recognition systems for identifying and avoiding errors. Leveraging\na pre-existing object recognition system, URCam, fine-tuned for our experiment,\nwe conducted a user study involving 12 blind and low-vision participants.\nThrough in-depth interviews and hands-on error identification tasks, we gained\ninsights into users' experiences, challenges, and strategies for identifying\nerrors in camera-based assistive technologies and object recognition systems.\nDuring interviews, many participants preferred independent error review, while\nexpressing apprehension toward misrecognitions. In the error identification\ntask, participants varied viewpoints, backgrounds, and object sizes in their\nimages to avoid and overcome errors. Even after repeating the task,\nparticipants identified only half of the errors, and the proportion of errors\nidentified did not significantly differ from their first attempts. Based on\nthese insights, we offer implications for designing accessible interfaces\ntailored to the needs of blind and low-vision users in identifying object\nrecognition errors.",
    "arxiv_id": "2408.03303v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03303v1",
    "abstract_url": "http://arxiv.org/abs/2408.03303v1",
    "primary_category": "cs.HC",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models",
    "authors": "Ruizhe Zhang, Yongxin Xu, Yuzhen Xiao, Runchuan Zhu, Xinke Jiang, Xu Chu, Junfeng Zhao, Yasha Wang",
    "abstract": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has\nbecome an effective strategy for mitigating the hallucination problems that\nlarge language models (LLMs) encounter when dealing with knowledge-intensive\ntasks. However, in the process of integrating external non-parametric\nsupporting evidence with internal parametric knowledge, inevitable knowledge\nconflicts may arise, leading to confusion in the model's responses. To enhance\nthe knowledge selection of LLMs in various contexts, some research has focused\non refining their behavior patterns through instruction-tuning. Nonetheless,\ndue to the absence of explicit negative signals and comparative objectives,\nmodels fine-tuned in this manner may still exhibit undesirable behaviors in the\nintricate and realistic retrieval scenarios. To this end, we propose a\nKnowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving\ncontrollable knowledge selection in real retrieval scenarios. Concretely, we\nexplore and simulate error types across diverse context combinations and learn\nhow to avoid these negative signals through preference optimization methods.\nSimultaneously, by adjusting the balance between response length and the\nproportion of preference data representing different behavior patterns, we\nenhance the adherence capabilities and noise robustness of LLMs in a balanced\nmanner. Experimental results show that KaPO outperforms previous methods for\nhandling knowledge conflicts by over 37%, while also exhibiting robust\ngeneralization across various out-of-distribution datasets.",
    "arxiv_id": "2408.03297v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03297v1",
    "abstract_url": "http://arxiv.org/abs/2408.03297v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Static IR Drop Prediction with Attention U-Net and Saliency-Based Explainability",
    "authors": "Lizi Zhang, Azadeh Davoodi",
    "abstract": "There has been significant recent progress to reduce the computational effort\nof static IR drop analysis using neural networks, and modeling as an\nimage-to-image translation task. A crucial issue is the lack of sufficient data\nfrom real industry designs to train these networks. Additionally, there is no\nmethodology to explain a high-drop pixel in a predicted IR drop image to its\nspecific root-causes. In this work, we first propose a U-Net neural network\nmodel with attention gates which is specifically tailored to achieve fast and\naccurate image-based static IR drop prediction. Attention gates allow selective\nemphasis on relevant parts of the input data without supervision which is\ndesired because of the often sparse nature of the IR drop map. We propose a\ntwo-phase training process which utilizes a mix of artificially-generated data\nand a limited number of points from real designs. The results are, on-average,\n18% (53%) better in MAE and 14% (113%) in F1 score compared to the winner of\nthe ICCAD 2023 contest (and U-Net only) when tested on real designs. Second, we\npropose a fast method using saliency maps which can explain a predicted IR drop\nin terms of specific input pixels contributing the most to a drop. In our\nexperiments, we show the number of high IR drop pixels can be reduced\non-average by 18% by mimicking upsize of a tiny portion of PDN's resistive\nedges.",
    "arxiv_id": "2408.03292v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03292v1",
    "abstract_url": "http://arxiv.org/abs/2408.03292v1",
    "primary_category": "cs.AR",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "SARA: Singular-Value Based Adaptive Low-Rank Adaption",
    "authors": "Jihao Gu, Shuai Chen, Zelin Wang, Yibo Zhang, Ping Gong",
    "abstract": "With the increasing number of parameters in large pre-trained models, LoRA as\na parameter-efficient fine-tuning(PEFT) method is widely used for not adding\ninference overhead. The LoRA method assumes that weight changes during\nfine-tuning can be approximated by low-rank matrices. However, the rank values\nneed to be manually verified to match different downstream tasks, and they\ncannot accommodate the varying importance of different layers in the model. In\nthis work, we first analyze the relationship between the performance of\ndifferent layers and their ranks using SVD. Based on this, we design the\nSingular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds\nthe rank during initialization by performing SVD on the pre-trained weights.\nAdditionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly\nreduces the number of parameters by fine-tuning only multiple parallel sets of\nsingular values controlled by a router. Extensive experiments on various\ncomplex tasks demonstrate the simplicity and parameter efficiency of our\nmethods. They can effectively and adaptively find the most suitable rank for\neach layer of each model.",
    "arxiv_id": "2408.03290v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03290v1",
    "abstract_url": "http://arxiv.org/abs/2408.03290v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Malicious Internet Entity Detection Using Local Graph Inference",
    "authors": "Simon Mandlik, Tomas Pevny, Vaclav Smidl, Lukas Bajer",
    "abstract": "Detection of malicious behavior in a large network is a challenging problem\nfor machine learning in computer security, since it requires a model with high\nexpressive power and scalable inference. Existing solutions struggle to achieve\nthis feat -- current cybersec-tailored approaches are still limited in\nexpressivity, and methods successful in other domains do not scale well for\nlarge volumes of data, rendering frequent retraining impossible. This work\nproposes a new perspective for learning from graph data that is modeling\nnetwork entity interactions as a large heterogeneous graph. High expressivity\nof the method is achieved with neural network architecture HMILnet that\nnaturally models this type of data and provides theoretical guarantees. The\nscalability is achieved by pursuing local graph inference, i.e., classifying\nindividual vertices and their neighborhood as independent samples. Our\nexperiments exhibit improvement over the state-of-the-art Probabilistic Threat\nPropagation (PTP) algorithm, show a further threefold accuracy improvement when\nadditional data is used, which is not possible with the PTP algorithm, and\ndemonstrate the generalization capabilities of the method to new, previously\nunseen entities.",
    "arxiv_id": "2408.03287v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03287v1",
    "abstract_url": "http://arxiv.org/abs/2408.03287v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation",
    "authors": "Boxi Cao, Mengjie Ren, Hongyu Lin, Xianpei Han, Feng Zhang, Junfeng Zhan, Le Sun",
    "abstract": "Evaluation is the baton for the development of large language models. Current\nevaluations typically employ a single-item assessment paradigm for each atomic\ntest objective, which struggles to discern whether a model genuinely possesses\nthe required capabilities or merely memorizes/guesses the answers to specific\nquestions. To this end, we propose a novel evaluation framework referred to as\nStructEval. Starting from an atomic test objective, StructEval deepens and\nbroadens the evaluation by conducting a structured assessment across multiple\ncognitive levels and critical concepts, and therefore offers a comprehensive,\nrobust and consistent evaluation for LLMs. Experiments on three widely-used\nbenchmarks demonstrate that StructEval serves as a reliable tool for resisting\nthe risk of data contamination and reducing the interference of potential\nbiases, thereby providing more reliable and consistent conclusions regarding\nmodel capabilities. Our framework also sheds light on the design of future\nprincipled and trustworthy LLM evaluation protocols.",
    "arxiv_id": "2408.03281v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03281v1",
    "abstract_url": "http://arxiv.org/abs/2408.03281v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments",
    "authors": "Angie Boggust, Venkatesh Sivaraman, Yannick Assogba, Donghao Ren, Dominik Moritz, Fred Hohman",
    "abstract": "To deploy machine learning models on-device, practitioners use compression\nalgorithms to shrink and speed up models while maintaining their high-quality\noutput. A critical aspect of compression in practice is model comparison,\nincluding tracking many compression experiments, identifying subtle changes in\nmodel behavior, and negotiating complex accuracy-efficiency trade-offs.\nHowever, existing compression tools poorly support comparison, leading to\ntedious and, sometimes, incomplete analyses spread across disjoint tools. To\nsupport real-world comparative workflows, we develop an interactive visual\nsystem called Compress and Compare. Within a single interface, Compress and\nCompare surfaces promising compression strategies by visualizing provenance\nrelationships between compressed models and reveals compression-induced\nbehavior changes by comparing models' predictions, weights, and activations. We\ndemonstrate how Compress and Compare supports common compression analysis tasks\nthrough two case studies, debugging failed compression on generative language\nmodels and identifying compression artifacts in image classification models. We\nfurther evaluate Compress and Compare in a user study with eight compression\nexperts, illustrating its potential to provide structure to compression\nworkflows, help practitioners build intuition about compression, and encourage\nthorough analysis of compression's effect on model behavior. Through these\nevaluations, we identify compression-specific challenges that future visual\nanalytics tools should consider and Compress and Compare visualizations that\nmay generalize to broader model comparison tasks.",
    "arxiv_id": "2408.03274v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03274v1",
    "abstract_url": "http://arxiv.org/abs/2408.03274v1",
    "primary_category": "cs.HC",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons",
    "authors": "Yifei Wang, Yuheng Chen, Wanting Wen, Yu Sheng, Linjing Li, Daniel Dajun Zeng",
    "abstract": "In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon.",
    "arxiv_id": "2408.03247v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03247v1",
    "abstract_url": "http://arxiv.org/abs/2408.03247v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Analysis of Partially-Calibrated Sparse Subarrays for Direction Finding with Extended Degrees of Freedom",
    "authors": "W. S. Leite, R. C. de Lamare",
    "abstract": "This paper investigates the problem of direction-of-arrival (DOA) estimation\nusing multiple partially-calibrated sparse subarrays. In particular, we present\nthe Generalized Coarray Multiple Signal Classification (GCA-MUSIC) DOA\nestimation algorithm to scenarios with partially-calibrated sparse subarrays.\nThe proposed GCA-MUSIC algorithm exploits the difference coarray for each\nsubarray, followed by a specific pseudo-spectrum merging rule that is based on\nthe intersection of the signal subspaces associated to each subarray. This rule\nassumes that there is no a priori knowledge about the cross-covariance between\nsubarrays. In that way, only the second-order statistics of each subarray are\nused to estimate the directions with increased degrees of freedom, i.e., the\nestimation procedure preserves the coarray Multiple Signal Classification and\nsparse arrays properties to estimate more sources than the number of physical\nsensors in each subarray. Numerical simulations show that the proposed\nGCA-MUSIC has better performance than other similar strategies.",
    "arxiv_id": "2408.03236v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03236v1",
    "abstract_url": "http://arxiv.org/abs/2408.03236v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Don't Think It Twice: Exploit Shift Invariance for Efficient Online Streaming Inference of CNNs",
    "authors": "Christodoulos Kechris, Jonathan Dan, Jose Miranda, David Atienza",
    "abstract": "Deep learning time-series processing often relies on convolutional neural\nnetworks with overlapping windows. This overlap allows the network to produce\nan output faster than the window length. However, it introduces additional\ncomputations. This work explores the potential to optimize computational\nefficiency during inference by exploiting convolution's shift-invariance\nproperties to skip the calculation of layer activations between successive\noverlapping windows. Although convolutions are shift-invariant, zero-padding\nand pooling operations, widely used in such networks, are not efficient and\ncomplicate efficient streaming inference. We introduce StreamiNNC, a strategy\nto deploy Convolutional Neural Networks for online streaming inference. We\nexplore the adverse effects of zero padding and pooling on the accuracy of\nstreaming inference, deriving theoretical error upper bounds for pooling during\nstreaming. We address these limitations by proposing signal padding and pooling\nalignment and provide guidelines for designing and deploying models for\nStreamiNNC. We validate our method in simulated data and on three real-world\nbiomedical signal processing applications. StreamiNNC achieves a low deviation\nbetween streaming output and normal inference for all three networks (2.03 -\n3.55% NRMSE). This work demonstrates that it is possible to linearly speed up\nthe inference of streaming CNNs processing overlapping windows, negating the\nadditional computation typically incurred by overlapping windows.",
    "arxiv_id": "2408.03223v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03223v1",
    "abstract_url": "http://arxiv.org/abs/2408.03223v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Masked Random Noise for Communication Efficient Federaetd Learning",
    "authors": "Shiwei Li, Yingyi Cheng, Haozhao Wang, Xing Tang, Shijie Xu, Weihong Luo, Yuhua Li, Dugang Liu, Xiuqiang He, and Ruixuan Li",
    "abstract": "Federated learning is a promising distributed training paradigm that\neffectively safeguards data privacy. However, it may involve significant\ncommunication costs, which hinders training efficiency. In this paper, we aim\nto enhance communication efficiency from a new perspective. Specifically, we\nrequest the distributed clients to find optimal model updates relative to\nglobal model parameters within predefined random noise. For this purpose, we\npropose Federated Masked Random Noise (FedMRN), a novel framework that enables\nclients to learn a 1-bit mask for each model parameter and apply masked random\nnoise (i.e., the Hadamard product of random noise and masks) to represent model\nupdates. To make FedMRN feasible, we propose an advanced mask training\nstrategy, called progressive stochastic masking (PSM). After local training,\neach client only need to transmit local masks and a random seed to the server.\nAdditionally, we provide theoretical guarantees for the convergence of FedMRN\nunder both strongly convex and non-convex assumptions. Extensive experiments\nare conducted on four popular datasets. The results show that FedMRN exhibits\nsuperior convergence speed and test accuracy compared to relevant baselines,\nwhile attaining a similar level of accuracy as FedAvg.",
    "arxiv_id": "2408.03220v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03220v1",
    "abstract_url": "http://arxiv.org/abs/2408.03220v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Learning to Learn without Forgetting using Attention",
    "authors": "Anna Vettoruzzo, Joaquin Vanschoren, Mohamed-Rafik Bouguelia, Thorsteinn R\u00f6gnvaldsson",
    "abstract": "Continual learning (CL) refers to the ability to continually learn over time\nby accommodating new knowledge while retaining previously learned experience.\nWhile this concept is inherent in human learning, current machine learning\nmethods are highly prone to overwrite previously learned patterns and thus\nforget past experience. Instead, model parameters should be updated selectively\nand carefully, avoiding unnecessary forgetting while optimally leveraging\npreviously learned patterns to accelerate future learning. Since hand-crafting\neffective update mechanisms is difficult, we propose meta-learning a\ntransformer-based optimizer to enhance CL. This meta-learned optimizer uses\nattention to learn the complex relationships between model parameters across a\nstream of tasks, and is designed to generate effective weight updates for the\ncurrent task while preventing catastrophic forgetting on previously encountered\ntasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and\nSplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both\nforward and backward transfer, even on small sets of labeled data, highlighting\nthe advantages of integrating a meta-learned optimizer within the continual\nlearning framework.",
    "arxiv_id": "2408.03219v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03219v1",
    "abstract_url": "http://arxiv.org/abs/2408.03219v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "FedBAT: Communication-Efficient Federated Learning via Learnable Binarization",
    "authors": "Shiwei Li, Wenchao Xu, Haozhao Wang, Xing Tang, Yining Qi, Shijie Xu, Weihong Luo, Yuhua Li, Xiuqiang He, Ruixuan Li",
    "abstract": "Federated learning is a promising distributed machine learning paradigm that\ncan effectively exploit large-scale data without exposing users' privacy.\nHowever, it may incur significant communication overhead, thereby potentially\nimpairing the training efficiency. To address this challenge, numerous studies\nsuggest binarizing the model updates. Nonetheless, traditional methods usually\nbinarize model updates in a post-training manner, resulting in significant\napproximation errors and consequent degradation in model accuracy. To this end,\nwe propose Federated Binarization-Aware Training (FedBAT), a novel framework\nthat directly learns binary model updates during the local training process,\nthus inherently reducing the approximation errors. FedBAT incorporates an\ninnovative binarization operator, along with meticulously designed derivatives\nto facilitate efficient learning. In addition, we establish theoretical\nguarantees regarding the convergence of FedBAT. Extensive experiments are\nconducted on four popular datasets. The results show that FedBAT significantly\naccelerates the convergence and exceeds the accuracy of baselines by up to 9\\%,\neven surpassing that of FedAvg in some cases.",
    "arxiv_id": "2408.03215v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03215v1",
    "abstract_url": "http://arxiv.org/abs/2408.03215v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Personalizing Federated Instrument Segmentation with Visual Trait Priors in Robotic Surgery",
    "authors": "Jialang Xu, Jiacheng Wang, Lequan Yu, Danail Stoyanov, Yueming Jin, Evangelos B. Mazomenos",
    "abstract": "Personalized federated learning (PFL) for surgical instrument segmentation\n(SIS) is a promising approach. It enables multiple clinical sites to\ncollaboratively train a series of models in privacy, with each model tailored\nto the individual distribution of each site. Existing PFL methods rarely\nconsider the personalization of multi-headed self-attention, and do not account\nfor appearance diversity and instrument shape similarity, both inherent in\nsurgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait\npriors for SIS, incorporating global-personalized disentanglement (GPD),\nappearance-regulation personalized enhancement (APE), and shape-similarity\nglobal enhancement (SGE), to boost SIS performance in each site. GPD represents\nthe first attempt at head-wise assignment for multi-headed self-attention\npersonalization. To preserve the unique appearance representation of each site\nand gradually leverage the inter-site difference, APE introduces appearance\nregulation and provides customized layer-wise aggregation solutions via\nhypernetworks for each site's personalized parameters. The mutual shape\ninformation of instruments is maintained and shared via SGE, which enhances the\ncross-style shape consistency on the image level and computes the\nshape-similarity contribution of each site on the prediction level for updating\nthe global parameters. PFedSIS outperforms state-of-the-art methods with +1.51%\nDice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding\ncode and models will be released at https://github.com/wzjialang/PFedSIS.",
    "arxiv_id": "2408.03208v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03208v1",
    "abstract_url": "http://arxiv.org/abs/2408.03208v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Adversarial Safety-Critical Scenario Generation using Naturalistic Human Driving Priors",
    "authors": "Kunkun Hao, Yonggang Luo, Wen Cui, Yuqiao Bai, Jucheng Yang, Songyang Yan, Yuxi Pan, Zijiang Yang",
    "abstract": "Evaluating the decision-making system is indispensable in developing\nautonomous vehicles, while realistic and challenging safety-critical test\nscenarios play a crucial role. Obtaining these scenarios is non-trivial, thanks\nto the long-tailed distribution, sparsity, and rarity in real-world data sets.\nTo tackle this problem, in this paper, we introduce a natural adversarial\nscenario generation solution using naturalistic human driving priors and\nreinforcement learning techniques. By doing this, we can obtain large-scale\ntest scenarios that are both diverse and realistic. Specifically, we build a\nsimulation environment that mimics natural traffic interaction scenarios.\nInformed by this environment, we implement a two-stage procedure. The first\nstage incorporates conventional rule-based models, e.g., IDM~(Intelligent\nDriver Model) and MOBIL~(Minimizing Overall Braking Induced by Lane changes)\nmodel, to coarsely and discretely capture and calibrate key control parameters\nfrom the real-world dataset. Next, we leverage GAIL~(Generative Adversarial\nImitation Learning) to represent driver behaviors continuously. The derived\nGAIL can be further used to design a PPO~(Proximal Policy Optimization)-based\nactor-critic network framework to fine-tune the reward function, and then\noptimizes our natural adversarial scenario generation solution. Extensive\nexperiments have been conducted in the NGSIM dataset including the trajectory\nof 3,000 vehicles. Essential traffic parameters were measured in comparison\nwith the baseline model, e.g., the collision rate, accelerations, steering, and\nthe number of lane changes. Our findings demonstrate that the proposed model\ncan generate realistic safety-critical test scenarios covering both naturalness\nand adversariality, which can be a cornerstone for the development of\nautonomous vehicles.",
    "arxiv_id": "2408.03200v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03200v1",
    "abstract_url": "http://arxiv.org/abs/2408.03200v1",
    "primary_category": "cs.RO",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Convergence Conditions for Stochastic Line Search Based Optimization of Over-parametrized Models",
    "authors": "Matteo Lapucci, Davide Pucci",
    "abstract": "In this paper, we deal with algorithms to solve the finite-sum problems\nrelated to fitting over-parametrized models, that typically satisfy the\ninterpolation condition. In particular, we focus on approaches based on\nstochastic line searches and employing general search directions. We define\nconditions on the sequence of search directions that guarantee finite\ntermination and bounds for the backtracking procedure. Moreover, we shed light\non the additional property of directions needed to prove fast (linear)\nconvergence of the general class of algorithms when applied to PL functions in\nthe interpolation regime. From the point of view of algorithms design, the\nproposed analysis identifies safeguarding conditions that could be employed in\nrelevant algorithmic framework. In particular, it could be of interest to\nintegrate stochastic line searches within momentum, conjugate gradient or\nadaptive preconditioning methods.",
    "arxiv_id": "2408.03199v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03199v1",
    "abstract_url": "http://arxiv.org/abs/2408.03199v1",
    "primary_category": "math.OC",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "RELIEF: Reinforcement Learning Empowered Graph Feature Prompt Tuning",
    "authors": "Jiapeng Zhu, Zichen Ding, Jianxiang Yu, Jiaqi Tan, Xiang Li, Weining Qian",
    "abstract": "The advent of the \"pre-train, prompt\" paradigm has recently extended its\ngeneralization ability and data efficiency to graph representation learning,\nfollowing its achievements in Natural Language Processing (NLP). Initial graph\nprompt tuning approaches tailored specialized prompting functions for Graph\nNeural Network (GNN) models pre-trained with specific strategies, such as edge\nprediction, thus limiting their applicability. In contrast, another pioneering\nline of research has explored universal prompting via adding prompts to the\ninput graph's feature space, thereby removing the reliance on specific\npre-training strategies. However, the necessity to add feature prompts to all\nnodes remains an open question. Motivated by findings from prompt tuning\nresearch in the NLP domain, which suggest that highly capable pre-trained\nmodels need less conditioning signal to achieve desired behaviors, we advocate\nfor strategically incorporating necessary and lightweight feature prompts to\ncertain graph nodes to enhance downstream task performance. This introduces a\ncombinatorial optimization problem, requiring a policy to decide 1) which nodes\nto prompt and 2) what specific feature prompts to attach. We then address the\nproblem by framing the prompt incorporation process as a sequential\ndecision-making problem and propose our method, RELIEF, which employs\nReinforcement Learning (RL) to optimize it. At each step, the RL agent selects\na node (discrete action) and determines the prompt content (continuous action),\naiming to maximize cumulative performance gain. Extensive experiments on graph\nand node-level tasks with various pre-training strategies in few-shot scenarios\ndemonstrate that our RELIEF outperforms fine-tuning and other prompt-based\napproaches in classification performance and data efficiency.",
    "arxiv_id": "2408.03195v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03195v1",
    "abstract_url": "http://arxiv.org/abs/2408.03195v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "An Object is Worth 64x64 Pixels: Generating 3D Object via Image Diffusion",
    "authors": "Xingguang Yan, Han-Hung Lee, Ziyu Wan, Angel X. Chang",
    "abstract": "We introduce a new approach for generating realistic 3D models with UV maps\nthrough a representation termed \"Object Images.\" This approach encapsulates\nsurface geometry, appearance, and patch structures within a 64x64 pixel image,\neffectively converting complex 3D shapes into a more manageable 2D format. By\ndoing so, we address the challenges of both geometric and semantic irregularity\ninherent in polygonal meshes. This method allows us to use image generation\nmodels, such as Diffusion Transformers, directly for 3D shape generation.\nEvaluated on the ABO dataset, our generated shapes with patch structures\nachieve point cloud FID comparable to recent 3D generative models, while\nnaturally supporting PBR material generation.",
    "arxiv_id": "2408.03178v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03178v1",
    "abstract_url": "http://arxiv.org/abs/2408.03178v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Leveraging Parameter Efficient Training Methods for Low Resource Text Classification: A Case Study in Marathi",
    "authors": "Pranita Deshmukh, Nikita Kulkarni, Sanhita Kulkarni, Kareena Manghani, Raviraj Joshi",
    "abstract": "With the surge in digital content in low-resource languages, there is an\nescalating demand for advanced Natural Language Processing (NLP) techniques\ntailored to these languages. BERT (Bidirectional Encoder Representations from\nTransformers), serving as the foundational framework for numerous NLP\narchitectures and language models, is increasingly employed for the development\nof low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a method\nfor fine-tuning Large Language Models (LLMs) and reducing the training\nparameters to some extent to decrease the computational costs needed for\ntraining the model and achieve results comparable to a fully fine-tuned model.\nIn this work, we present a study of PEFT methods for the Indic low-resource\nlanguage Marathi. We conduct a comprehensive analysis of PEFT methods applied\nto various monolingual and multilingual Marathi BERT models. These approaches\nare evaluated on prominent text classification datasets like MahaSent,\nMahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated to\nsignificantly expedite the training speed of the models, addressing a critical\naspect of model development and deployment. In this study, we explore Low-Rank\nAdaptation of Large Language Models (LoRA) and adapter methods for low-resource\ntext classification. We show that these methods are competitive with full\nfine-tuning and can be used without loss in accuracy. This study contributes\nvaluable insights into the effectiveness of Marathi BERT models, offering a\nfoundation for the continued advancement of NLP capabilities in Marathi and\nsimilar Indic languages.",
    "arxiv_id": "2408.03172v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03172v1",
    "abstract_url": "http://arxiv.org/abs/2408.03172v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Training on the Fly: On-device Self-supervised Learning aboard Nano-drones within 20 mW",
    "authors": "Elia Cereda, Alessandro Giusti, Daniele Palossi",
    "abstract": "Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning\n(TinyML), such as nano-drones, are becoming an increasingly attractive\ntechnology. Their small form factor (i.e., ~10cm diameter) ensures vast\napplicability, ranging from the exploration of narrow disaster scenarios to\nsafe human-robot interaction. Simple electronics make these CPSes inexpensive,\nbut strongly limit the computational, memory, and sensing resources available\non board. In real-world applications, these limitations are further exacerbated\nby domain shift. This fundamental machine learning problem implies that model\nperception performance drops when moving from the training domain to a\ndifferent deployment one. To cope with and mitigate this general problem, we\npresent a novel on-device fine-tuning approach that relies only on the limited\nultra-low power resources available aboard nano-drones. Then, to overcome the\nlack of ground-truth training labels aboard our CPS, we also employ a\nself-supervised method based on ego-motion consistency. Albeit our work builds\non top of a specific real-world vision-based human pose estimation task, it is\nwidely applicable for many embedded TinyML use cases. Our 512-image on-device\ntraining procedure is fully deployed aboard an ultra-low power GWT GAP9\nSystem-on-Chip and requires only 1MB of memory while consuming as low as 19mW\nor running in just 510ms (at 38mW). Finally, we demonstrate the benefits of our\non-device learning approach by field-testing our closed-loop CPS, showing a\nreduction in horizontal position error of up to 26% vs. a non-fine-tuned\nstate-of-the-art baseline. In the most challenging never-seen-before\nenvironment, our on-device learning procedure makes the difference between\nsucceeding or failing the mission.",
    "arxiv_id": "2408.03168v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03168v1",
    "abstract_url": "http://arxiv.org/abs/2408.03168v1",
    "primary_category": "cs.RO",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Dilated Convolution with Learnable Spacings makes visual models more aligned with humans: a Grad-CAM study",
    "authors": "Rabih Chamas, Ismail Khalfaoui-Hassani, Timothee Masquelier",
    "abstract": "Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced\nconvolution method that allows enlarging the receptive fields (RF) without\nincreasing the number of parameters, like the dilated convolution, yet without\nimposing a regular grid. DCLS has been shown to outperform the standard and\ndilated convolutions on several computer vision benchmarks. Here, we show that,\nin addition, DCLS increases the models' interpretability, defined as the\nalignment with human visual strategies. To quantify it, we use the Spearman\ncorrelation between the models' GradCAM heatmaps and the ClickMe dataset\nheatmaps, which reflect human visual attention. We took eight reference models\n- ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and\n36) - and drop-in replaced the standard convolution layers with DCLS ones. This\nimproved the interpretability score in seven of them. Moreover, we observed\nthat Grad-CAM generated random heatmaps for two models in our study: CAFormer\nand ConvFormer models, leading to low interpretability scores. We addressed\nthis issue by introducing Threshold-Grad-CAM, a modification built on top of\nGrad-CAM that enhanced interpretability across nearly all models. The code and\ncheckpoints to reproduce this study are available at:\nhttps://github.com/rabihchamas/DCLS-GradCAM-Eval.",
    "arxiv_id": "2408.03164v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03164v1",
    "abstract_url": "http://arxiv.org/abs/2408.03164v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Iterative CT Reconstruction via Latent Variable Optimization of Shallow Diffusion Models",
    "authors": "Sho Ozaki, Shizuo Kaji, Toshikazu Imae, Kanabu Nawa, Hideomi Yamashita, Keiichi Nakagawa",
    "abstract": "Image generative AI has garnered significant attention in recent years. In\nparticular, the diffusion model, a core component of recent generative AI,\nproduces high-quality images with rich diversity. In this study, we propose a\nnovel CT reconstruction method by combining the denoising diffusion\nprobabilistic model with iterative CT reconstruction. In sharp contrast to\nprevious studies, we optimize the fidelity loss of CT reconstruction with\nrespect to the latent variable of the diffusion model, instead of the image and\nmodel parameters. To suppress anatomical structure changes produced by the\ndiffusion model, we shallow the diffusion and reverse processes, and fix a set\nof added noises in the reverse process to make it deterministic during\ninference. We demonstrate the effectiveness of the proposed method through\nsparse view CT reconstruction of 1/10 view projection data. Despite the\nsimplicity of the implementation, the proposed method shows the capability of\nreconstructing high-quality images while preserving the patient's anatomical\nstructure, and outperforms existing methods including iterative reconstruction,\niterative reconstruction with total variation, and the diffusion model alone in\nterms of quantitative indices such as SSIM and PSNR. We also explore further\nsparse view CT using 1/20 view projection data with the same trained diffusion\nmodel. As the number of iterations increases, image quality improvement\ncomparable to that of 1/10 sparse view CT reconstruction is achieved. In\nprinciple, the proposed method can be widely applied not only to CT but also to\nother imaging modalities such as MRI, PET, and SPECT.",
    "arxiv_id": "2408.03156v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03156v1",
    "abstract_url": "http://arxiv.org/abs/2408.03156v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "TSC: A Simple Two-Sided Constraint against Over-Smoothing",
    "authors": "Furong Peng, Kang Liu, Xuan Lu, Yuhua Qian, Hongren Yan, Chao Ma",
    "abstract": "Graph Convolutional Neural Network (GCN), a widely adopted method for\nanalyzing relational data, enhances node discriminability through the\naggregation of neighboring information. Usually, stacking multiple layers can\nimprove the performance of GCN by leveraging information from high-order\nneighbors. However, the increase of the network depth will induce the\nover-smoothing problem, which can be attributed to the quality and quantity of\nneighbors changing: (a) neighbor quality, node's neighbors become overlapping\nin high order, leading to aggregated information becoming indistinguishable,\n(b) neighbor quantity, the exponentially growing aggregated neighbors submerges\nthe node's initial feature by recursively aggregating operations. Current\nsolutions mainly focus on one of the above causes and seldom consider both at\nonce.\n  Aiming at tackling both causes of over-smoothing in one shot, we introduce a\nsimple Two-Sided Constraint (TSC) for GCNs, comprising two straightforward yet\npotent techniques: random masking and contrastive constraint. The random\nmasking acts on the representation matrix's columns to regulate the degree of\ninformation aggregation from neighbors, thus preventing the convergence of node\nrepresentations. Meanwhile, the contrastive constraint, applied to the\nrepresentation matrix's rows, enhances the discriminability of the nodes.\nDesigned as a plug-in module, TSC can be easily coupled with GCN or SGC\narchitectures. Experimental analyses on diverse real-world graph datasets\nverify that our approach markedly reduces the convergence of node's\nrepresentation and the performance degradation in deeper GCN.",
    "arxiv_id": "2408.03152v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03152v1",
    "abstract_url": "http://arxiv.org/abs/2408.03152v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Conditioning LLMs with Emotion in Neural Machine Translation",
    "authors": "Charles Brazier, Jean-Luc Rouas",
    "abstract": "Large Language Models (LLMs) have shown remarkable performance in Natural\nLanguage Processing tasks, including Machine Translation (MT). In this work, we\npropose a novel MT pipeline that integrates emotion information extracted from\na Speech Emotion Recognition (SER) model into LLMs to enhance translation\nquality. We first fine-tune five existing LLMs on the Libri-trans dataset and\nselect the most performant model. Subsequently, we augment LLM prompts with\ndifferent dimensional emotions and train the selected LLM under these different\nconfigurations. Our experiments reveal that integrating emotion information,\nespecially arousal, into LLM prompts leads to notable improvements in\ntranslation quality.",
    "arxiv_id": "2408.03150v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03150v1",
    "abstract_url": "http://arxiv.org/abs/2408.03150v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Active Learning for Level Set Estimation Using Randomized Straddle Algorithms",
    "authors": "Yu Inatsu, Shion Takeno, Kentaro Kutsukake, Ichiro Takeuchi",
    "abstract": "Level set estimation (LSE), the problem of identifying the set of input\npoints where a function takes value above (or below) a given threshold, is\nimportant in practical applications. When the function is expensive-to-evaluate\nand black-box, the \\textit{straddle} algorithm, which is a representative\nheuristic for LSE based on Gaussian process models, and its extensions having\ntheoretical guarantees have been developed. However, many of existing methods\ninclude a confidence parameter $\\beta^{1/2}_t$ that must be specified by the\nuser, and methods that choose $\\beta^{1/2}_t$ heuristically do not provide\ntheoretical guarantees. In contrast, theoretically guaranteed values of\n$\\beta^{1/2}_t$ need to be increased depending on the number of iterations and\ncandidate points, and are conservative and not good for practical performance.\nIn this study, we propose a novel method, the \\textit{randomized straddle}\nalgorithm, in which $\\beta_t$ in the straddle algorithm is replaced by a random\nsample from the chi-squared distribution with two degrees of freedom. The\nconfidence parameter in the proposed method has the advantages of not needing\nadjustment, not depending on the number of iterations and candidate points, and\nnot being conservative. Furthermore, we show that the proposed method has\ntheoretical guarantees that depend on the sample complexity and the number of\niterations. Finally, we confirm the usefulness of the proposed method through\nnumerical experiments using synthetic and real data.",
    "arxiv_id": "2408.03144v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03144v1",
    "abstract_url": "http://arxiv.org/abs/2408.03144v1",
    "primary_category": "stat.ML",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "COMMENTATOR: A Code-mixed Multilingual Text Annotation Framework",
    "authors": "Rajvee Sheth, Shubh Nisar, Heenaben Prajapati, Himanshu Beniwal, Mayank Singh",
    "abstract": "As the NLP community increasingly addresses challenges associated with\nmultilingualism, robust annotation tools are essential to handle multilingual\ndatasets efficiently. In this paper, we introduce a code-mixed multilingual\ntext annotation framework, COMMENTATOR, specifically designed for annotating\ncode-mixed text. The tool demonstrates its effectiveness in token-level and\nsentence-level language annotation tasks for Hinglish text. We perform robust\nqualitative human-based evaluations to showcase COMMENTATOR led to 5x faster\nannotations than the best baseline. Our code is publicly available at\n\\url{https://github.com/lingo-iitgn/commentator}. The demonstration video is\navailable at \\url{https://bit.ly/commentator_video}.",
    "arxiv_id": "2408.03125v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03125v1",
    "abstract_url": "http://arxiv.org/abs/2408.03125v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Evaluating the Translation Performance of Large Language Models Based on Euas-20",
    "authors": "Yan Huang, Wei Liu",
    "abstract": "In recent years, with the rapid development of deep learning technology,\nlarge language models (LLMs) such as BERT and GPT have achieved breakthrough\nresults in natural language processing tasks. Machine translation (MT), as one\nof the core tasks of natural language processing, has also benefited from the\ndevelopment of large language models and achieved a qualitative leap. Despite\nthe significant progress in translation performance achieved by large language\nmodels, machine translation still faces many challenges. Therefore, in this\npaper, we construct the dataset Euas-20 to evaluate the performance of large\nlanguage models on translation tasks, the translation ability on different\nlanguages, and the effect of pre-training data on the translation ability of\nLLMs for researchers and developers.",
    "arxiv_id": "2408.03119v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03119v1",
    "abstract_url": "http://arxiv.org/abs/2408.03119v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Topic Modeling with Fine-tuning LLMs and Bag of Sentences",
    "authors": "Johannes Schneider",
    "abstract": "Large language models (LLM)'s are increasingly used for topic modeling\noutperforming classical topic models such as LDA. Commonly, pre-trained LLM\nencoders such as BERT are used out-of-the-box despite the fact that fine-tuning\nis known to improve LLMs considerably. The challenge lies in obtaining a\nsuitable (labeled) dataset for fine-tuning. In this paper, we use the recent\nidea to use bag of sentences as the elementary unit in computing topics. In\nturn, we derive an approach FT-Topic to perform unsupervised fine-tuning\nrelying primarily on two steps for constructing a training dataset in an\nautomatic fashion. First, a heuristic method to identifies pairs of sentence\ngroups that are either assumed to be of the same or different topics. Second,\nwe remove sentence pairs that are likely labeled incorrectly. The dataset is\nthen used to fine-tune an encoder LLM, which can be leveraged by any topic\nmodeling approach using embeddings. However, in this work, we demonstrate its\neffectiveness by deriving a novel state-of-the-art topic modeling method called\nSenClu, which achieves fast inference through an expectation-maximization\nalgorithm and hard assignments of sentence groups to a single topic, while\ngiving users the possibility to encode prior knowledge on the topic-document\ndistribution. Code is at \\url{https://github.com/JohnTailor/FT-Topic}",
    "arxiv_id": "2408.03099v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03099v1",
    "abstract_url": "http://arxiv.org/abs/2408.03099v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Learning Provably Robust Policies in Uncertain Parametric Environments",
    "authors": "Yannik Schnitzer, Alessandro Abate, David Parker",
    "abstract": "We present a data-driven approach for learning MDP policies that are robust\nacross stochastic environments whose transition probabilities are defined by\nparameters with an unknown distribution. We produce probably approximately\ncorrect (PAC) guarantees for the performance of these learned policies in a\nnew, unseen environment over the unknown distribution. Our approach is based on\nfinite samples of the MDP environments, for each of which we build an\napproximation of the model as an interval MDP, by exploring a set of generated\ntrajectories. We use the built approximations to synthesise a single policy\nthat performs well (meets given requirements) across the sampled environments,\nand furthermore bound its risk (of not meeting the given requirements) when\ndeployed in an unseen environment. Our procedure offers a trade-off between the\nguaranteed performance of the learned policy and the risk of not meeting the\nguarantee in an unseen environment. Our approach exploits knowledge of the\nenvironment's state space and graph structure, and we show how additional\nknowledge of its parametric structure can be leveraged to optimize learning and\nto obtain tighter guarantees from less samples. We evaluate our approach on a\ndiverse range of established benchmarks, demonstrating that we can generate\nhighly performing and robust policies, along with guarantees that tightly\nquantify their performance and the associated risk.",
    "arxiv_id": "2408.03093v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03093v1",
    "abstract_url": "http://arxiv.org/abs/2408.03093v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "QADQN: Quantum Attention Deep Q-Network for Financial Market Prediction",
    "authors": "Siddhant Dutta, Nouhaila Innan, Alberto Marchisio, Sadok Ben Yahia, Muhammad Shafique",
    "abstract": "Financial market prediction and optimal trading strategy development remain\nchallenging due to market complexity and volatility. Our research in quantum\nfinance and reinforcement learning for decision-making demonstrates the\napproach of quantum-classical hybrid algorithms to tackling real-world\nfinancial challenges. In this respect, we corroborate the concept with rigorous\nbacktesting and validate the framework's performance under realistic market\nconditions, by including fixed transaction cost per trade. This paper\nintroduces a Quantum Attention Deep Q-Network (QADQN) approach to address these\nchallenges through quantum-enhanced reinforcement learning. Our QADQN\narchitecture uses a variational quantum circuit inside a traditional deep\nQ-learning framework to take advantage of possible quantum advantages in\ndecision-making. We gauge the QADQN agent's performance on historical data from\nmajor market indices, including the S&P 500. We evaluate the agent's learning\nprocess by examining its reward accumulation and the effectiveness of its\nexperience replay mechanism. Our empirical results demonstrate the QADQN's\nsuperior performance, achieving better risk-adjusted returns with Sortino\nratios of 1.28 and 1.19 for non-overlapping and overlapping test periods\nrespectively, indicating effective downside risk management.",
    "arxiv_id": "2408.03088v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03088v1",
    "abstract_url": "http://arxiv.org/abs/2408.03088v1",
    "primary_category": "quant-ph",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Matrix Multiplication on Quantum Computer",
    "authors": "Jiaqi Yao, Ding Liu",
    "abstract": "This paper introduces an innovative and practical approach to universal\nquantum matrix multiplication. We designed optimized quantum adders and\nmultipliers based on Quantum Fourier Transform (QFT), which significantly\nreduced the number of gates used compared to classical adders and multipliers.\nSubsequently, we construct a basic universal quantum matrix multiplication and\nextend it to the Strassen algorithm. We conduct comparative experiments to\nanalyze the performance of the quantum matrix multiplication and evaluate the\nacceleration provided by the optimized quantum adder and multiplier.\nFurthermore, we investigate the advantages and disadvantages of the quantum\nStrassen algorithm compared to basic quantum matrix multiplication.",
    "arxiv_id": "2408.03085v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03085v1",
    "abstract_url": "http://arxiv.org/abs/2408.03085v1",
    "primary_category": "quant-ph",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Research on Autonomous Driving Decision-making Strategies based Deep Reinforcement Learning",
    "authors": "Zixiang Wang, Hao Yan, Changsong Wei, Junyu Wang, Shi Bo, Minheng Xiao",
    "abstract": "The behavior decision-making subsystem is a key component of the autonomous\ndriving system, which reflects the decision-making ability of the vehicle and\nthe driver, and is an important symbol of the high-level intelligence of the\nvehicle. However, the existing rule-based decision-making schemes are limited\nby the prior knowledge of designers, and it is difficult to cope with complex\nand changeable traffic scenarios. In this work, an advanced deep reinforcement\nlearning model is adopted, which can autonomously learn and optimize driving\nstrategies in a complex and changeable traffic environment by modeling the\ndriving decision-making process as a reinforcement learning problem.\nSpecifically, we used Deep Q-Network (DQN) and Proximal Policy Optimization\n(PPO) for comparative experiments. DQN guides the agent to choose the best\naction by approximating the state-action value function, while PPO improves the\ndecision-making quality by optimizing the policy function. We also introduce\nimprovements in the design of the reward function to promote the robustness and\nadaptability of the model in real-world driving situations. Experimental\nresults show that the decision-making strategy based on deep reinforcement\nlearning has better performance than the traditional rule-based method in a\nvariety of driving tasks.",
    "arxiv_id": "2408.03084v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03084v1",
    "abstract_url": "http://arxiv.org/abs/2408.03084v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Enhancing Complex Causality Extraction via Improved Subtask Interaction and Knowledge Fusion",
    "authors": "Jinglong Gao, Chen Lu, Xiao Ding, Zhongyang Li, Ting Liu, Bing Qin",
    "abstract": "Event Causality Extraction (ECE) aims at extracting causal event pairs from\ntexts. Despite ChatGPT's recent success, fine-tuning small models remains the\nbest approach for the ECE task. However, existing fine-tuning based ECE methods\ncannot address all three key challenges in ECE simultaneously: 1) Complex\nCausality Extraction, where multiple causal-effect pairs occur within a single\nsentence; 2) Subtask~ Interaction, which involves modeling the mutual\ndependence between the two subtasks of ECE, i.e., extracting events and\nidentifying the causal relationship between extracted events; and 3) Knowledge\nFusion, which requires effectively fusing the knowledge in two modalities,\ni.e., the expressive pretrained language models and the structured knowledge\ngraphs. In this paper, we propose a unified ECE framework (UniCE to address all\nthree issues in ECE simultaneously. Specifically, we design a subtask\ninteraction mechanism to enable mutual interaction between the two ECE\nsubtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in\nthe two modalities. Furthermore, we employ separate decoders for each subtask\nto facilitate complex causality extraction. Experiments on three benchmark\ndatasets demonstrate that our method achieves state-of-the-art performance and\noutperforms ChatGPT with a margin of at least 30% F1-score. More importantly,\nour model can also be used to effectively improve the ECE performance of\nChatGPT via in-context learning.",
    "arxiv_id": "2408.03079v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03079v1",
    "abstract_url": "http://arxiv.org/abs/2408.03079v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical Applications",
    "authors": "G. Manni, C. Lauretti, F. Prata, R. Papalia, L. Zollo, P. Soda",
    "abstract": "Endoscopic surgery relies on two-dimensional views, posing challenges for\nsurgeons in depth perception and instrument manipulation. While Simultaneous\nLocalization and Mapping (SLAM) has emerged as a promising solution to address\nthese limitations, its implementation in endoscopic procedures presents\nsignificant challenges due to hardware limitations, such as the use of a\nmonocular camera and the absence of odometry sensors. This study presents a\nrobust deep learning-based SLAM approach that combines state-of-the-art and\nnewly developed models. It consists of three main parts: the Monocular Pose\nEstimation Module that introduces a novel unsupervised method based on the\nCycleGAN architecture, the Monocular Depth Estimation Module that leverages the\nnovel Zoe architecture, and the 3D Reconstruction Module which uses information\nfrom the previous models to create a coherent surgical map. The performance of\nthe procedure was rigorously evaluated using three publicly available datasets\n(Hamlyn, EndoSLAM, and SCARED) and benchmarked against two state-of-the-art\nmethods, EndoSFMLearner and EndoDepth. The integration of Zoe in the MDEM\ndemonstrated superior performance compared to state-of-the-art depth estimation\nalgorithms in endoscopy, whereas the novel approach in the MPEM exhibited\ncompetitive performance and the lowest inference time. The results showcase the\nrobustness of our approach in laparoscopy, gastroscopy, and colonoscopy, three\ndifferent scenarios in endoscopic surgery. The proposed SLAM approach has the\npotential to improve the accuracy and efficiency of endoscopic procedures by\nproviding surgeons with enhanced depth perception and 3D reconstruction\ncapabilities.",
    "arxiv_id": "2408.03078v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03078v1",
    "abstract_url": "http://arxiv.org/abs/2408.03078v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Solving QUBO on the Loihi 2 Neuromorphic Processor",
    "authors": "Alessandro Pierro, Philipp Stratmann, Gabriel Andres Fonseca Guerra, Sumedh Risbud, Timothy Shea, Ashish Rao Mangalore, Andreas Wild",
    "abstract": "In this article, we describe an algorithm for solving Quadratic Unconstrained\nBinary Optimization problems on the Intel Loihi 2 neuromorphic processor. The\nsolver is based on a hardware-aware fine-grained parallel simulated annealing\nalgorithm developed for Intel's neuromorphic research chip Loihi 2. Preliminary\nresults show that our approach can generate feasible solutions in as little as\n1 ms and up to 37x more energy efficient compared to two baseline solvers\nrunning on a CPU. These advantages could be especially relevant for size-,\nweight-, and power-constrained edge computing applications.",
    "arxiv_id": "2408.03076v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03076v1",
    "abstract_url": "http://arxiv.org/abs/2408.03076v1",
    "primary_category": "cs.NE",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents",
    "authors": "Qiang Sun, Yuanyi Luo, Sirui Li, Wenxiao Zhang, Wei Liu",
    "abstract": "Multimodal conversational agents are highly desirable because they offer\nnatural and human-like interaction. However, there is a lack of comprehensive\nend-to-end solutions to support collaborative development and benchmarking.\nWhile proprietary systems like GPT-4o and Gemini demonstrating impressive\nintegration of audio, video, and text with response times of 200-250ms,\nchallenges remain in balancing latency, accuracy, cost, and data privacy. To\nbetter understand and quantify these issues, we developed OpenOmni, an\nopen-source, end-to-end pipeline benchmarking tool that integrates advanced\ntechnologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented\nGeneration, Large Language Models, along with the ability to integrate\ncustomized models. OpenOmni supports local and cloud deployment, ensuring data\nprivacy and supporting latency and accuracy benchmarking. This flexible\nframework allows researchers to customize the pipeline, focusing on real\nbottlenecks and facilitating rapid proof-of-concept development. OpenOmni can\nsignificantly enhance applications like indoor assistance for visually impaired\nindividuals, advancing human-computer interaction. Our demonstration video is\navailable https://www.youtube.com/watch?v=zaSiT3clWqY, demo is available via\nhttps://openomni.ai4wa.com, code is available via\nhttps://github.com/AI4WA/OpenOmniFramework.",
    "arxiv_id": "2408.03047v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03047v1",
    "abstract_url": "http://arxiv.org/abs/2408.03047v1",
    "primary_category": "cs.HC",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning",
    "authors": "Haozhe Ma, Zhengding Luo, Thanh Vinh Vo, Kuankuan Sima, Tze-Yun Leong",
    "abstract": "Reward shaping addresses the challenge of sparse rewards in reinforcement\nlearning by constructing denser and more informative reward signals. To achieve\nself-adaptive and highly efficient reward shaping, we propose a novel method\nthat incorporates success rates derived from historical experiences into shaped\nrewards. Our approach utilizes success rates sampled from Beta distributions,\nwhich dynamically evolve from uncertain to reliable values as more data is\ncollected. Initially, the self-adaptive success rates exhibit more randomness\nto encourage exploration. Over time, they become more certain to enhance\nexploitation, thus achieving a better balance between exploration and\nexploitation. We employ Kernel Density Estimation (KDE) combined with Random\nFourier Features (RFF) to derive the Beta distributions, resulting in a\ncomputationally efficient implementation in high-dimensional continuous state\nspaces. This method provides a non-parametric and learning-free approach. The\nproposed method is evaluated on a wide range of continuous control tasks with\nsparse and delayed rewards, demonstrating significant improvements in sample\nefficiency and convergence stability compared to several baselines.",
    "arxiv_id": "2408.03029v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03029v1",
    "abstract_url": "http://arxiv.org/abs/2408.03029v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Integrating Controllable Motion Skills from Demonstrations",
    "authors": "Honghao Liao, Zhiheng Li, Ziyu Meng, Ran Song, Yibin Li, Wei Zhang",
    "abstract": "The expanding applications of legged robots require their mastery of\nversatile motion skills. Correspondingly, researchers must address the\nchallenge of integrating multiple diverse motion skills into controllers. While\nexisting reinforcement learning (RL)-based approaches have achieved notable\nsuccess in multi-skill integration for legged robots, these methods often\nrequire intricate reward engineering or are restricted to integrating a\npredefined set of motion skills constrained by specific task objectives,\nresulting in limited flexibility. In this work, we introduce a flexible\nmulti-skill integration framework named Controllable Skills Integration (CSI).\nCSI enables the integration of a diverse set of motion skills with varying\nstyles into a single policy without the need for complex reward tuning.\nFurthermore, in a hierarchical control manner, the trained low-level policy can\nbe coupled with a high-level Natural Language Inference (NLI) module to enable\npreliminary language-directed skill control. Our experiments demonstrate that\nCSI can flexibly integrate a diverse array of motion skills more\ncomprehensively and facilitate the transitions between different skills.\nAdditionally, CSI exhibits good scalability as the number of motion skills to\nbe integrated increases significantly.",
    "arxiv_id": "2408.03018v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03018v1",
    "abstract_url": "http://arxiv.org/abs/2408.03018v1",
    "primary_category": "cs.RO",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "NeurDB: On the Design and Implementation of an AI-powered Autonomous Database",
    "authors": "Zhanhao Zhao, Shaofeng Cai, Haotian Gao, Hexiang Pan, Siqi Xiang, Naili Xing, Gang Chen, Beng Chin Ooi, Yanyan Shen, Yuncheng Wu, Meihui Zhang",
    "abstract": "Databases are increasingly embracing AI to provide autonomous system\noptimization and intelligent in-database analytics, aiming to relieve end-user\nburdens across various industry sectors. Nonetheless, most existing approaches\nfail to account for the dynamic nature of databases, which renders them\nineffective for real-world applications characterized by evolving data and\nworkloads. This paper introduces NeurDB, an AI-powered autonomous database that\ndeepens the fusion of AI and databases with adaptability to data and workload\ndrift. NeurDB establishes a new in-database AI ecosystem that seamlessly\nintegrates AI workflows within the database. This integration enables efficient\nand effective in-database AI analytics and fast-adaptive learned system\ncomponents. Empirical evaluations demonstrate that NeurDB substantially\noutperforms existing solutions in managing AI analytics tasks, with the\nproposed learned components more effectively handling environmental dynamism\nthan state-of-the-art approaches.",
    "arxiv_id": "2408.03013v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03013v1",
    "abstract_url": "http://arxiv.org/abs/2408.03013v1",
    "primary_category": "cs.DB",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Cross-cultural analysis of pedestrian group behaviour influence on crossing decisions in interactions with autonomous vehicles",
    "authors": "Sergio Mart\u00edn Serrano, \u00d3scar M\u00e9ndez Blanco, Stewart Worrall, Miguel \u00c1ngel Sotelo, David Fern\u00e1ndez-Llorca",
    "abstract": "Understanding cultural backgrounds is crucial for the seamless integration of\nautonomous driving into daily life as it ensures that systems are attuned to\ndiverse societal norms and behaviours, enhancing acceptance and safety in\nvaried cultural contexts. In this work, we investigate the impact of co-located\npedestrians on crossing behaviour, considering cultural and situational\nfactors. To accomplish this, a full-scale virtual reality (VR) environment was\ncreated in the CARLA simulator, enabling the identical experiment to be\nreplicated in both Spain and Australia. Participants (N=30) attempted to cross\nthe road at an urban crosswalk alongside other pedestrians exhibiting\nconservative to more daring behaviours, while an autonomous vehicle (AV)\napproached with different driving styles. For the analysis of interactions, we\nutilized questionnaires and direct measures of the moment when participants\nentered the lane.\n  Our findings indicate that pedestrians tend to cross the same traffic gap\ntogether, even though reckless behaviour by the group reduces confidence and\nmakes the situation perceived as more complex. Australian participants were\nwilling to take fewer risks than Spanish participants, adopting more cautious\nbehaviour when it was uncertain whether the AV would yield.",
    "arxiv_id": "2408.03003v1",
    "pdf_url": "http://arxiv.org/pdf/2408.03003v1",
    "abstract_url": "http://arxiv.org/abs/2408.03003v1",
    "primary_category": "cs.HC",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
    "authors": "Lekai Chen, Ashutosh Trivedi, Alvaro Velasquez",
    "abstract": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
    "arxiv_id": "2408.02999v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02999v1",
    "abstract_url": "http://arxiv.org/abs/2408.02999v1",
    "primary_category": "cs.FL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application",
    "authors": "Anwesha Mukherjee, Rajkumar Buyya",
    "abstract": "Federated learning has become an emerging technology for data analysis for\nIoT applications. This paper implements centralized and decentralized federated\nlearning frameworks for crop yield prediction based on Long Short-Term Memory\nNetwork. For centralized federated learning, multiple clients and one server is\nconsidered, where the clients exchange their model updates with the server that\nworks as the aggregator to build the global model. For the decentralized\nframework, a collaborative network is formed among the devices either using\nring topology or using mesh topology. In this network, each device receives\nmodel updates from the neighbour devices, and performs aggregation to build the\nupgraded model. The performance of the centralized and decentralized federated\nlearning frameworks are evaluated in terms of prediction accuracy, precision,\nrecall, F1-Score, and training time. The experimental results present that\n$\\geq$97% and $>$97.5% prediction accuracy are achieved using the centralized\nand decentralized federated learning-based frameworks respectively. The results\nalso show that the using centralized federated learning the response time can\nbe reduced by $\\sim$75% than the cloud-only framework. Finally, the future\nresearch directions of the use of federated learning in crop yield prediction\nare explored in this paper.",
    "arxiv_id": "2408.02998v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02998v1",
    "abstract_url": "http://arxiv.org/abs/2408.02998v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "A Differential Smoothness-based Compact-Dynamic Graph Convolutional Network for Spatiotemporal Signal Recovery",
    "authors": "Pengcheng Gao, Zicheng Gao, Ye Yuan",
    "abstract": "High quality spatiotemporal signal is vitally important for real application\nscenarios like energy management, traffic planning and cyber security. Due to\nthe uncontrollable factors like abrupt sensors breakdown or communication\nfault, the spatiotemporal signal collected by sensors is always incomplete. A\ndynamic graph convolutional network (DGCN) is effective for processing\nspatiotemporal signal recovery. However, it adopts a static GCN and a sequence\nneural network to explore the spatial and temporal patterns, separately. Such a\nseparated two-step processing is loose spatiotemporal, thereby failing to\ncapture the complex inner spatiotemporal correlation. To address this issue,\nthis paper proposes a Compact-Dynamic Graph Convolutional Network (CDGCN) for\nspatiotemporal signal recovery with the following two-fold ideas: a) leveraging\nthe tensor M-product to build a unified tensor graph convolution framework,\nwhich considers both spatial and temporal patterns simultaneously; and b)\nconstructing a differential smoothness-based objective function to reduce the\nnoise interference in spatiotemporal signal, thereby further improve the\nrecovery accuracy. Experiments on real-world spatiotemporal datasets\ndemonstrate that the proposed CDGCN significantly outperforms the\nstate-of-the-art models in terms of recovery accuracy.",
    "arxiv_id": "2408.02987v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02987v1",
    "abstract_url": "http://arxiv.org/abs/2408.02987v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "ASR-enhanced Multimodal Representation Learning for Cross-Domain Product Retrieval",
    "authors": "Ruixiang Zhao, Jian Jia, Yan Li, Xuehan Bai, Quan Chen, Han Li, Peng Jiang, Xirong Li",
    "abstract": "E-commerce is increasingly multimedia-enriched, with products exhibited in a\nbroad-domain manner as images, short videos, or live stream promotions. A\nunified and vectorized cross-domain production representation is essential. Due\nto large intra-product variance and high inter-product similarity in the\nbroad-domain scenario, a visual-only representation is inadequate. While\nAutomatic Speech Recognition (ASR) text derived from the short or live-stream\nvideos is readily accessible, how to de-noise the excessively noisy text for\nmultimodal representation learning is mostly untouched. We propose ASR-enhanced\nMultimodal Product Representation Learning (AMPere). In order to extract\nproduct-specific information from the raw ASR text, AMPere uses an\neasy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,\ntogether with visual data, is then fed into a multi-branch network to generate\ncompact multimodal embeddings. Extensive experiments on a large-scale\ntri-domain dataset verify the effectiveness of AMPere in obtaining a unified\nmultimodal product representation that clearly improves cross-domain product\nretrieval.",
    "arxiv_id": "2408.02978v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02978v1",
    "abstract_url": "http://arxiv.org/abs/2408.02978v1",
    "primary_category": "cs.MM",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation",
    "authors": "Hui Ma, Bo Zhang, Bo Xu, Jian Wang, Hongfei Lin, Xiao Sun",
    "abstract": "Empathetic response generation, aiming at understanding the user's situation\nand feelings and respond empathically, is crucial in building human-like\ndialogue systems. Previous methods mainly focus on using maximum likelihood\nestimation as the optimization objective for training response generation\nmodels, without taking into account the empathy level alignment between\ngenerated responses and target responses. To this end, we propose an empathetic\nresponse generation using reinforcement learning (EmpRL) framework. The\nframework designs an effective empathy reward function and generates empathetic\nresponses by maximizing the expected reward through reinforcement learning.\nGiven the powerful text generation capability of pre-trained language models,\nEmpRL utilizes the pre-trained T5 model as the generator and conducts further\ntraining to initialize the policy. To align the empathy level between generated\nresponses and target responses in the context, an empathy reward function\ncontaining three empathy communication mechanisms, i.e., emotional reaction,\ninterpretation, and exploration, is constructed using pre-designed and\npre-trained empathy identifiers. Finally, the proximal policy optimization\nalgorithm is used to further train the policy to produce empathetic responses.\nBoth automatic and manual evaluations demonstrate that the proposed EmpRL\nframework can improve the quality of generated responses, enhance the empathy\nlevel similarity between generated and target responses, and produce empathetic\nresponses covering both affective and cognitive aspects.",
    "arxiv_id": "2408.02976v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02976v1",
    "abstract_url": "http://arxiv.org/abs/2408.02976v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Wave Interpolation Neural Operator: Interpolated Prediction of Electric Fields Across Untrained Wavelengths",
    "authors": "Joonhyuk Seo, Chanik Kang, Dongjin Seo, Haejun Chung",
    "abstract": "Designing photonic structures requires electromagnetic simulations, which\noften require high computational costs. Researchers have developed surrogate\nsolvers for predicting electric fields to alleviate the computational issues.\nHowever, existing surrogate solvers are limited to performing inference at\nfixed simulation conditions and require retraining for different conditions. To\naddress this, we propose Wave Interpolation Neural Operator (WINO), a novel\nsurrogate solver enabling simulation condition interpolation across a\ncontinuous spectrum of broadband wavelengths. WINO introduces the Fourier Group\nConvolution Shuffling operator and a new conditioning method to efficiently\npredict electric fields from both trained and untrained wavelength data,\nachieving significant improvements in parameter efficiency and spectral\ninterpolation performance. Our model demonstrates approximately 100 times\nfaster performance than traditional finite-difference frequency-domain\nsimulations. Moreover, compared to the state-of-the-art model, we achieve a 74%\nreduction in parameters and 80.5% improvements in prediction accuracy for\nuntrained wavelengths, and 13.2% improvements for trained wavelengths.",
    "arxiv_id": "2408.02971v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02971v1",
    "abstract_url": "http://arxiv.org/abs/2408.02971v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Data-Driven Stochastic Closure Modeling via Conditional Diffusion Model and Neural Operator",
    "authors": "Xinghao Dong, Chuanqi Chen, Jin-Long Wu",
    "abstract": "Closure models are widely used in simulating complex multiscale dynamical\nsystems such as turbulence and the earth system, for which direct numerical\nsimulation that resolves all scales is often too expensive. For those systems\nwithout a clear scale separation, deterministic and local closure models often\nlack enough generalization capability, which limits their performance in many\nreal-world applications. In this work, we propose a data-driven modeling\nframework for constructing stochastic and non-local closure models via\nconditional diffusion model and neural operator. Specifically, the Fourier\nneural operator is incorporated into a score-based diffusion model, which\nserves as a data-driven stochastic closure model for complex dynamical systems\ngoverned by partial differential equations (PDEs). We also demonstrate how\naccelerated sampling methods can improve the efficiency of the data-driven\nstochastic closure model. The results show that the proposed methodology\nprovides a systematic approach via generative machine learning techniques to\nconstruct data-driven stochastic closure models for multiscale dynamical\nsystems with continuous spatiotemporal fields.",
    "arxiv_id": "2408.02965v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02965v1",
    "abstract_url": "http://arxiv.org/abs/2408.02965v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Synaptic Modulation using Interspike Intervals Increases Energy Efficiency of Spiking Neural Networks",
    "authors": "Dylan Adams, Magda Zajaczkowska, Ashiq Anjum, Andrea Soltoggio, Shirin Dora",
    "abstract": "Despite basic differences between Spiking Neural Networks (SNN) and\nArtificial Neural Networks (ANN), most research on SNNs involve adapting\nANN-based methods for SNNs. Pruning (dropping connections) and quantization\n(reducing precision) are often used to improve energy efficiency of SNNs. These\nmethods are very effective for ANNs whose energy needs are determined by\nsignals transmitted on synapses. However, the event-driven paradigm in SNNs\nimplies that energy is consumed by spikes. In this paper, we propose a new\nsynapse model whose weights are modulated by Interspike Intervals (ISI) i.e.\ntime difference between two spikes. SNNs composed of this synapse model, termed\nISI Modulated SNNs (IMSNN), can use gradient descent to estimate how the ISI of\na neuron changes after updating its synaptic parameters. A higher ISI implies\nfewer spikes and vice-versa. The learning algorithm for IMSNNs exploits this\ninformation to selectively propagate gradients such that learning is achieved\nby increasing the ISIs resulting in a network that generates fewer spikes. The\nperformance of IMSNNs with dense and convolutional layers have been evaluated\nin terms of classification accuracy and the number of spikes using the MNIST\nand FashionMNIST datasets. The performance comparison with conventional SNNs\nshows that IMSNNs exhibit upto 90% reduction in the number of spikes while\nmaintaining similar classification accuracy.",
    "arxiv_id": "2408.02961v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02961v1",
    "abstract_url": "http://arxiv.org/abs/2408.02961v1",
    "primary_category": "cs.NE",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Anytime Multi-Agent Path Finding with an Adaptive Delay-Based Heuristic",
    "authors": "Thomy Phan, Benran Zhang, Shao-Hung Chan, Sven Koenig",
    "abstract": "Anytime multi-agent path finding (MAPF) is a promising approach to scalable\npath optimization in multi-agent systems. MAPF-LNS, based on Large Neighborhood\nSearch (LNS), is the current state-of-the-art approach where a fast initial\nsolution is iteratively optimized by destroying and repairing selected paths of\nthe solution. Current MAPF-LNS variants commonly use an adaptive selection\nmechanism to choose among multiple destroy heuristics. However, to determine\npromising destroy heuristics, MAPF-LNS requires a considerable amount of\nexploration time. As common destroy heuristics are non-adaptive, any\nperformance bottleneck caused by these heuristics cannot be overcome via\nadaptive heuristic selection alone, thus limiting the overall effectiveness of\nMAPF-LNS in terms of solution cost. In this paper, we propose Adaptive\nDelay-based Destroy-and-Repair Enhanced with Success-based Self-Learning\n(ADDRESS) as a single-destroy-heuristic variant of MAPF-LNS. ADDRESS applies\nrestricted Thompson Sampling to the top-K set of the most delayed agents to\nselect a seed agent for adaptive LNS neighborhood generation. We evaluate\nADDRESS in multiple maps from the MAPF benchmark set and demonstrate cost\nimprovements by at least 50% in large-scale scenarios with up to a thousand\nagents, compared with the original MAPF-LNS and other state-of-the-art methods.",
    "arxiv_id": "2408.02960v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02960v1",
    "abstract_url": "http://arxiv.org/abs/2408.02960v1",
    "primary_category": "cs.AI",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Kolmogorov-Arnold PointNet: Deep learning for prediction of fluid fields on irregular geometries",
    "authors": "Ali Kashefi",
    "abstract": "We present Kolmogorov-Arnold PointNet (KA-PointNet) as a novel supervised\ndeep learning framework for the prediction of incompressible steady-state fluid\nflow fields in irregular domains, where the predicted fields are a function of\nthe geometry of the domains. In KA-PointNet, we implement shared\nKolmogorov-Arnold Networks (KANs) in the segmentation branch of the PointNet\narchitecture. We utilize Jacobi polynomials to construct shared KANs. As a\nbenchmark test case, we consider incompressible laminar steady-state flow over\na cylinder, where the geometry of its cross-section varies over the data set.\nWe investigate the performance of Jacobi polynomials with different degrees as\nwell as special cases of Jacobi polynomials such as Legendre polynomials,\nChebyshev polynomials of the first and second kinds, and Gegenbauer\npolynomials, in terms of the computational cost of training and accuracy of\nprediction of the test set. Additionally, we compare the performance of\nPointNet with shared KANs (i.e., KA-PointNet) and PointNet with shared\nMultilayer Perceptrons (MLPs). It is observed that when the number of trainable\nparameters is approximately equal, PointNet with shared KANs (i.e.,\nKA-PointNet) outperforms PointNet with shared MLPs.",
    "arxiv_id": "2408.02950v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02950v1",
    "abstract_url": "http://arxiv.org/abs/2408.02950v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Few-shot Scooping Under Domain Shift via Simulated Maximal Deployment Gaps",
    "authors": "Yifan Zhu, Pranay Thangeda, Erica L Tevere, Ashish Goel, Erik Kramer, Hari D Nayar, Melkior Ornik, Kris Hauser",
    "abstract": "Autonomous lander missions on extraterrestrial bodies need to sample granular\nmaterials while coping with domain shifts, even when sampling strategies are\nextensively tuned on Earth. To tackle this challenge, this paper studies the\nfew-shot scooping problem and proposes a vision-based adaptive scooping\nstrategy that uses the deep kernel Gaussian process method trained with a novel\nmeta-training strategy to learn online from very limited experience on\nout-of-distribution target terrains. Our Deep Kernel Calibration with Maximal\nDeployment Gaps (kCMD) strategy explicitly trains a deep kernel model to adapt\nto large domain shifts by creating simulated maximal deployment gaps from an\noffline training dataset and training models to overcome these deployment gaps\nduring training. Employed in a Bayesian Optimization sequential decision-making\nframework, the proposed method allows the robot to perform high-quality\nscooping actions on out-of-distribution terrains after a few attempts,\nsignificantly outperforming non-adaptive methods proposed in the excavation\nliterature as well as other state-of-the-art meta-learning methods. The\nproposed method also demonstrates zero-shot transfer capability, successfully\nadapting to the NASA OWLAT platform, which serves as a state-of-the-art\nsimulator for potential future planetary missions. These results demonstrate\nthe potential of training deep models with simulated deployment gaps for more\ngeneralizable meta-learning in high-capacity models. Furthermore, they\nhighlight the promise of our method in autonomous lander sampling missions by\nenabling landers to overcome the deployment gap between Earth and\nextraterrestrial bodies.",
    "arxiv_id": "2408.02949v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02949v1",
    "abstract_url": "http://arxiv.org/abs/2408.02949v1",
    "primary_category": "cs.RO",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Scaling Laws for Data Poisoning in LLMs",
    "authors": "Dillon Bowen, Brendan Murphy, Will Cai, David Khachaturov, Adam Gleave, Kellin Pelrine",
    "abstract": "Recent work shows that LLMs are vulnerable to data poisoning, in which they\nare trained on partially corrupted or harmful data. Poisoned data is hard to\ndetect, breaks guardrails, and leads to undesirable and harmful behavior. Given\nthe intense efforts by leading labs to train and deploy increasingly larger and\nmore capable LLMs, it is critical to ask if the risk of data poisoning will be\nnaturally mitigated by scale, or if it is an increasing threat. We consider\nthree threat models by which data poisoning can occur: malicious fine-tuning,\nimperfect data curation, and intentional data contamination. Our experiments\nevaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72\nbillion parameters on three datasets which speak to each of our threat models.\nWe find that larger LLMs are increasingly vulnerable, learning harmful behavior\n-- including sleeper agent behavior -- significantly more quickly than smaller\nLLMs with even minimal data poisoning. These results underscore the need for\nrobust safeguards against data poisoning in larger LLMs.",
    "arxiv_id": "2408.02946v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02946v1",
    "abstract_url": "http://arxiv.org/abs/2408.02946v1",
    "primary_category": "cs.CR",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "LLM-Empowered Resource Allocation in Wireless Communications Systems",
    "authors": "Woongsup Lee, Jeonghun Park",
    "abstract": "The recent success of large language models (LLMs) has spurred their\napplication in various fields. In particular, there have been efforts to\nintegrate LLMs into various aspects of wireless communication systems. The use\nof LLMs in wireless communication systems has the potential to realize\nartificial general intelligence (AGI)-enabled wireless networks. In this paper,\nwe investigate an LLM-based resource allocation scheme for wireless\ncommunication systems. Specifically, we formulate a simple resource allocation\nproblem involving two transmit pairs and develop an LLM-based resource\nallocation approach that aims to maximize either energy efficiency or spectral\nefficiency. Additionally, we consider the joint use of low-complexity resource\nallocation techniques to compensate for the reliability shortcomings of the\nLLM-based scheme. After confirming the applicability and feasibility of\nLLM-based resource allocation, we address several key technical challenges that\nremain in applying LLMs in practice.",
    "arxiv_id": "2408.02944v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02944v1",
    "abstract_url": "http://arxiv.org/abs/2408.02944v1",
    "primary_category": "eess.SP",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Achieving More with Less: A Tensor-Optimization-Powered Ensemble Method",
    "authors": "Jinghui Yuan, Weijin Jiang, Zhe Cao, Fangyuan Xie, Rong Wang, Feiping Nie, Xuelong Li",
    "abstract": "Ensemble learning is a method that leverages weak learners to produce a\nstrong learner. However, obtaining a large number of base learners requires\nsubstantial time and computational resources. Therefore, it is meaningful to\nstudy how to achieve the performance typically obtained with many base learners\nusing only a few. We argue that to achieve this, it is essential to enhance\nboth classification performance and generalization ability during the ensemble\nprocess. To increase model accuracy, each weak base learner needs to be more\nefficiently integrated. It is observed that different base learners exhibit\nvarying levels of accuracy in predicting different classes. To capitalize on\nthis, we introduce confidence tensors $\\tilde{\\mathbf{\\Theta}}$ and\n$\\tilde{\\mathbf{\\Theta}}_{rst}$ signifies that the $t$-th base classifier\nassigns the sample to class $r$ while it actually belongs to class $s$. To the\nbest of our knowledge, this is the first time an evaluation of the performance\nof base classifiers across different classes has been proposed. The proposed\nconfidence tensor compensates for the strengths and weaknesses of each base\nclassifier in different classes, enabling the method to achieve superior\nresults with a smaller number of base learners. To enhance generalization\nperformance, we design a smooth and convex objective function that leverages\nthe concept of margin, making the strong learner more discriminative.\nFurthermore, it is proved that in gradient matrix of the loss function, the sum\nof each column's elements is zero, allowing us to solve a constrained\noptimization problem using gradient-based methods. We then compare our\nalgorithm with random forests of ten times the size and other classical methods\nacross numerous datasets, demonstrating the superiority of our approach.",
    "arxiv_id": "2408.02936v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02936v1",
    "abstract_url": "http://arxiv.org/abs/2408.02936v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Doubly Stochastic Adaptive Neighbors Clustering via the Marcus Mapping",
    "authors": "Jinghui Yuan, Chusheng Zeng, Fangyuan Xie, Zhe Cao, Rong Wang, Feiping Nie, Xuelong Li",
    "abstract": "Clustering is a fundamental task in machine learning and data science, and\nsimilarity graph-based clustering is an important approach within this domain.\nDoubly stochastic symmetric similarity graphs provide numerous benefits for\nclustering problems and downstream tasks, yet learning such graphs remains a\nsignificant challenge. Marcus theorem states that a strictly positive symmetric\nmatrix can be transformed into a doubly stochastic symmetric matrix by diagonal\nmatrices. However, in clustering, learning sparse matrices is crucial for\ncomputational efficiency. We extend Marcus theorem by proposing the Marcus\nmapping, which indicates that certain sparse matrices can also be transformed\ninto doubly stochastic symmetric matrices via diagonal matrices. Additionally,\nwe introduce rank constraints into the clustering problem and propose the\nDoubly Stochastic Adaptive Neighbors Clustering algorithm based on the Marcus\nMapping (ANCMM). This ensures that the learned graph naturally divides into the\ndesired number of clusters. We validate the effectiveness of our algorithm\nthrough extensive comparisons with state-of-the-art algorithms. Finally, we\nexplore the relationship between the Marcus mapping and optimal transport. We\nprove that the Marcus mapping solves a specific type of optimal transport\nproblem and demonstrate that solving this problem through Marcus mapping is\nmore efficient than directly applying optimal transport methods.",
    "arxiv_id": "2408.02932v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02932v1",
    "abstract_url": "http://arxiv.org/abs/2408.02932v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "The Need for a Big World Simulator: A Scientific Challenge for Continual Learning",
    "authors": "Saurabh Kumar, Hong Jun Jeon, Alex Lewandowski, Benjamin Van Roy",
    "abstract": "The \"small agent, big world\" frame offers a conceptual view that motivates\nthe need for continual learning. The idea is that a small agent operating in a\nmuch bigger world cannot store all information that the world has to offer. To\nperform well, the agent must be carefully designed to ingest, retain, and eject\nthe right information. To enable the development of performant continual\nlearning agents, a number of synthetic environments have been proposed.\nHowever, these benchmarks suffer from limitations, including unnatural\ndistribution shifts and a lack of fidelity to the \"small agent, big world\"\nframing. This paper aims to formalize two desiderata for the design of future\nsimulated environments. These two criteria aim to reflect the objectives and\ncomplexity of continual learning in practical settings while enabling rapid\nprototyping of algorithms on a smaller scale.",
    "arxiv_id": "2408.02930v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02930v1",
    "abstract_url": "http://arxiv.org/abs/2408.02930v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection",
    "authors": "Yuxin Wang, Duanyu Feng, Yongfu Dai, Zhengyu Chen, Jimin Huang, Sophia Ananiadou, Qianqian Xie, Hao Wang",
    "abstract": "Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.",
    "arxiv_id": "2408.02927v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02927v1",
    "abstract_url": "http://arxiv.org/abs/2408.02927v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "A Taxonomy of Architecture Options for Foundation Model-based Agents: Analysis and Decision Model",
    "authors": "Jingwen Zhou, Qinghua Lu, Jieshan Chen, Liming Zhu, Xiwei Xu, Zhenchang Xing, Stefan Harrer",
    "abstract": "The rapid advancement of AI technology has led to widespread applications of\nagent systems across various domains. However, the need for detailed\narchitecture design poses significant challenges in designing and operating\nthese systems. This paper introduces a taxonomy focused on the architectures of\nfoundation-model-based agents, addressing critical aspects such as functional\ncapabilities and non-functional qualities. We also discuss the operations\ninvolved in both design-time and run-time phases, providing a comprehensive\nview of architectural design and operational characteristics. By unifying and\ndetailing these classifications, our taxonomy aims to improve the design of\nfoundation-model-based agents. Additionally, the paper establishes a decision\nmodel that guides critical design and runtime decisions, offering a structured\napproach to enhance the development of foundation-model-based agents. Our\ncontributions include providing a structured architecture design option and\nguiding the development process of foundation-model-based agents, thereby\naddressing current fragmentation in the field.",
    "arxiv_id": "2408.02920v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02920v1",
    "abstract_url": "http://arxiv.org/abs/2408.02920v1",
    "primary_category": "cs.SE",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "KOI: Accelerating Online Imitation Learning via Hybrid Key-state Guidance",
    "authors": "Jingxian Lu, Wenke Xia, Dong Wang, Zhigang Wang, Bin Zhao, Di Hu, Xuelong Li",
    "abstract": "Online Imitation Learning methods struggle with the gap between extensive\nonline exploration space and limited expert trajectories, which hinder\nefficient exploration due to inaccurate task-aware reward estimation. Inspired\nby the findings from cognitive neuroscience that task decomposition could\nfacilitate cognitive processing for efficient learning, we hypothesize that an\nagent could estimate precise task-aware imitation rewards for efficient online\nexploration by decomposing the target task into the objectives of \"what to do\"\nand the mechanisms of \"how to do\". In this work, we introduce the hybrid\nKey-state guided Online Imitation (KOI) learning approach, which leverages the\nintegration of semantic and motion key states as guidance for task-aware reward\nestimation. Initially, we utilize the visual-language models to segment the\nexpert trajectory into semantic key states, indicating the objectives of \"what\nto do\". Within the intervals between semantic key states, optical flow is\nemployed to capture motion key states to understand the process of \"how to do\".\nBy integrating a thorough grasp of both semantic and motion key states, we\nrefine the trajectory-matching reward computation, encouraging task-aware\nexploration for efficient online imitation learning. Our experiment results\nprove that our method is more sample efficient in the Meta-World and LIBERO\nenvironments. We also conduct real-world robotic manipulation experiments to\nvalidate the efficacy of our method, demonstrating the practical applicability\nof our KOI method.",
    "arxiv_id": "2408.02912v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02912v1",
    "abstract_url": "http://arxiv.org/abs/2408.02912v1",
    "primary_category": "cs.RO",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Enabling Intelligent Traffic Systems: A Deep Learning Method for Accurate Arabic License Plate Recognition",
    "authors": "M. A. Sayedelahl",
    "abstract": "This paper introduces a novel two-stage framework for accurate Egyptian\nVehicle License Plate Recognition (EVLPR). The first stage employs image\nprocessing techniques to reliably localize license plates, while the second\nstage utilizes a custom-designed deep learning model for robust Arabic\ncharacter recognition. The proposed system achieves a remarkable 99.3% accuracy\non a diverse dataset, surpassing existing approaches. Its potential\napplications extend to intelligent traffic management, including traffic\nviolation detection and parking optimization. Future research will focus on\nenhancing the system's capabilities through architectural refinements, expanded\ndatasets, and addressing system dependencies.",
    "arxiv_id": "2408.02904v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02904v1",
    "abstract_url": "http://arxiv.org/abs/2408.02904v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "A Metric Driven Approach to Mixed Precision Training",
    "authors": "Mitchelle Rasquinha, Gil Tabak",
    "abstract": "As deep learning methodologies have developed, it has been generally agreed\nthat increasing neural network size improves model quality. However, this is at\nthe expense of memory and compute requirements, which also need to be\nincreased. Various efficiency techniques have been proposed to rein in hardware\ncosts, one being the use of low precision numerics. Recent accelerators have\nintroduced several different 8-bit data types to help accommodate DNNs in terms\nof numerics. In this paper, we identify a metric driven methodology to aid in\nthe choice of numerics. We demonstrate how such a methodology can help scale\ntraining of a language representation model. The technique can be generalized\nto other model architectures.",
    "arxiv_id": "2408.02897v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02897v1",
    "abstract_url": "http://arxiv.org/abs/2408.02897v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "VizECGNet: Visual ECG Image Network for Cardiovascular Diseases Classification with Multi-Modal Training and Knowledge Distillation",
    "authors": "Ju-Hyeon Nam, Seo-Hyung Park, Su Jung Kim, Sang-Chul Lee",
    "abstract": "An electrocardiogram (ECG) captures the heart's electrical signal to assess\nvarious heart conditions. In practice, ECG data is stored as either digitized\nsignals or printed images. Despite the emergence of numerous deep learning\nmodels for digitized signals, many hospitals prefer image storage due to cost\nconsiderations. Recognizing the unavailability of raw ECG signals in many\nclinical settings, we propose VizECGNet, which uses only printed ECG graphics\nto determine the prognosis of multiple cardiovascular diseases. During\ntraining, cross-modal attention modules (CMAM) are used to integrate\ninformation from two modalities - image and signal, while self-modality\nattention modules (SMAM) capture inherent long-range dependencies in ECG data\nof each modality. Additionally, we utilize knowledge distillation to improve\nthe similarity between two distinct predictions from each modality stream. This\ninnovative multi-modal deep learning architecture enables the utilization of\nonly ECG images during inference. VizECGNet with image input achieves higher\nperformance in precision, recall, and F1-Score compared to signal-based ECG\nclassification models, with improvements of 3.50%, 8.21%, and 7.38%,\nrespectively.",
    "arxiv_id": "2408.02888v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02888v1",
    "abstract_url": "http://arxiv.org/abs/2408.02888v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Compromising Embodied Agents with Contextual Backdoor Attacks",
    "authors": "Aishan Liu, Yuguang Zhou, Xianglong Liu, Tianyuan Zhang, Siyuan Liang, Jiakai Wang, Yanjun Pu, Tianlin Li, Junqi Zhang, Wenbo Zhou, Qing Guo, Dacheng Tao",
    "abstract": "Large language models (LLMs) have transformed the development of embodied\nintelligence. By providing a few contextual demonstrations, developers can\nutilize the extensive internal knowledge of LLMs to effortlessly translate\ncomplex tasks described in abstract language into sequences of code snippets,\nwhich will serve as the execution logic for embodied agents. However, this\npaper uncovers a significant backdoor security threat within this process and\nintroduces a novel method called \\method{}. By poisoning just a few contextual\ndemonstrations, attackers can covertly compromise the contextual environment of\na black-box LLM, prompting it to generate programs with context-dependent\ndefects. These programs appear logically sound but contain defects that can\nactivate and induce unintended behaviors when the operational agent encounters\nspecific triggers in its interactive environment. To compromise the LLM's\ncontextual environment, we employ adversarial in-context generation to optimize\npoisoned demonstrations, where an LLM judge evaluates these poisoned prompts,\nreporting to an additional LLM that iteratively optimizes the demonstration in\na two-player adversarial game using chain-of-thought reasoning. To enable\ncontext-dependent behaviors in downstream agents, we implement a dual-modality\nactivation strategy that controls both the generation and execution of program\ndefects through textual and visual triggers. We expand the scope of our attack\nby developing five program defect modes that compromise key aspects of\nconfidentiality, integrity, and availability in embodied agents. To validate\nthe effectiveness of our approach, we conducted extensive experiments across\nvarious tasks, including robot planning, robot manipulation, and compositional\nvisual reasoning. Additionally, we demonstrate the potential impact of our\napproach by successfully attacking real-world autonomous driving systems.",
    "arxiv_id": "2408.02882v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02882v1",
    "abstract_url": "http://arxiv.org/abs/2408.02882v1",
    "primary_category": "cs.AI",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Hide and Seek: Fingerprinting Large Language Models with Evolutionary Learning",
    "authors": "Dmitri Iourovitski, Sanat Sharma, Rakshak Talwar",
    "abstract": "As content generated by Large Language Model (LLM) has grown exponentially,\nthe ability to accurately identify and fingerprint such text has become\nincreasingly crucial. In this work, we introduce a novel black-box approach for\nfingerprinting LLMs, achieving an impressive 72% accuracy in identifying the\ncorrect family of models (Such as Llama, Mistral, Gemma, etc) among a lineup of\nLLMs. We present an evolutionary strategy that leverages the capabilities of\none LLM to discover the most salient features for identifying other LLMs. Our\nmethod employs a unique \"Hide and Seek\" algorithm, where an Auditor LLM\ngenerates discriminative prompts, and a Detective LLM analyzes the responses to\nfingerprint the target models. This approach not only demonstrates the\nfeasibility of LLM-driven model identification but also reveals insights into\nthe semantic manifolds of different LLM families. By iteratively refining\nprompts through in-context learning, our system uncovers subtle distinctions\nbetween model outputs, providing a powerful tool for LLM analysis and\nverification. This research opens new avenues for understanding LLM behavior\nand has significant implications for model attribution, security, and the\nbroader field of AI transparency.",
    "arxiv_id": "2408.02871v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02871v1",
    "abstract_url": "http://arxiv.org/abs/2408.02871v1",
    "primary_category": "cs.CR",
    "published_date": "2024-08-06",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Back-Projection Diffusion: Solving the Wideband Inverse Scattering Problem with Diffusion Models",
    "authors": "Borong Zhang, Mart\u00edn Guerra, Qin Li, Leonardo Zepeda-N\u00fa\u00f1ez",
    "abstract": "We present \\textit{Wideband back-projection diffusion}, an end-to-end\nprobabilistic framework for approximating the posterior distribution induced by\nthe inverse scattering map from wideband scattering data. This framework\nleverages conditional diffusion models coupled with the underlying physics of\nwave-propagation and symmetries in the problem, to produce highly accurate\nreconstructions. The framework introduces a factorization of the score function\ninto a physics-based latent representation inspired by the filtered\nback-propagation formula and a conditional score function conditioned on this\nlatent representation. These two steps are also constrained to obey symmetries\nin the formulation while being amenable to compression by imposing the rank\nstructure found in the filtered back-projection formula. As a result,\nempirically, our framework is able to provide sharp reconstructions\neffortlessly, even recovering sub-Nyquist features in the multiple-scattering\nregime. It has low-sample and computational complexity, its number of\nparameters scales sub-linearly with the target resolution, and it has stable\ntraining dynamics.",
    "arxiv_id": "2408.02866v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02866v1",
    "abstract_url": "http://arxiv.org/abs/2408.02866v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "VisionUnite: A Vision-Language Foundation Model for Ophthalmology Enhanced with Clinical Knowledge",
    "authors": "Zihan Li, Diping Song, Zefeng Yang, Deming Wang, Fei Li, Xiulan Zhang, Paul E. Kinahan, Yu Qiao",
    "abstract": "The need for improved diagnostic methods in ophthalmology is acute,\nespecially in the less developed regions with limited access to specialists and\nadvanced equipment. Therefore, we introduce VisionUnite, a novel\nvision-language foundation model for ophthalmology enhanced with clinical\nknowledge. VisionUnite has been pretrained on an extensive dataset comprising\n1.24 million image-text pairs, and further refined using our proposed MMFundus\ndataset, which includes 296,379 high-quality fundus image-text pairs and\n889,137 simulated doctor-patient dialogue instances. Our experiments indicate\nthat VisionUnite outperforms existing generative foundation models such as\nGPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable\nto junior ophthalmologists. VisionUnite performs well in various clinical\nscenarios including open-ended multi-disease diagnosis, clinical explanation,\nand patient interaction, making it a highly versatile tool for initial\nophthalmic disease screening. VisionUnite can also serve as an educational aid\nfor junior ophthalmologists, accelerating their acquisition of knowledge\nregarding both common and rare ophthalmic conditions. VisionUnite represents a\nsignificant advancement in ophthalmology, with broad implications for\ndiagnostics, medical education, and understanding of disease mechanisms.",
    "arxiv_id": "2408.02865v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02865v1",
    "abstract_url": "http://arxiv.org/abs/2408.02865v1",
    "primary_category": "eess.IV",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "On The Stability of Moral Preferences: A Problem with Computational Elicitation Methods",
    "authors": "Kyle Boerstler, Vijay Keswani, Lok Chan, Jana Schaich Borg, Vincent Conitzer, Hoda Heidari, Walter Sinnott-Armstrong",
    "abstract": "Preference elicitation frameworks feature heavily in the research on\nparticipatory ethical AI tools and provide a viable mechanism to enquire and\nincorporate the moral values of various stakeholders. As part of the\nelicitation process, surveys about moral preferences, opinions, and judgments\nare typically administered only once to each participant. This methodological\npractice is reasonable if participants' responses are stable over time such\nthat, all other relevant factors being held constant, their responses today\nwill be the same as their responses to the same questions at a later time.\nHowever, we do not know how often that is the case. It is possible that\nparticipants' true moral preferences change, are subject to temporary moods or\nwhims, or are influenced by environmental factors we don't track. If\nparticipants' moral responses are unstable in such ways, it would raise\nimportant methodological and theoretical issues for how participants' true\nmoral preferences, opinions, and judgments can be ascertained. We address this\npossibility here by asking the same survey participants the same moral\nquestions about which patient should receive a kidney when only one is\navailable ten times in ten different sessions over two weeks, varying only\npresentation order across sessions. We measured how often participants gave\ndifferent responses to simple (Study One) and more complicated (Study Two)\nrepeated scenarios. On average, the fraction of times participants changed\ntheir responses to controversial scenarios was around 10-18% across studies,\nand this instability is observed to have positive associations with response\ntime and decision-making difficulty. We discuss the implications of these\nresults for the efficacy of moral preference elicitation, highlighting the role\nof response instability in causing value misalignment between stakeholders and\nAI tools trained on their moral judgments.",
    "arxiv_id": "2408.02862v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02862v1",
    "abstract_url": "http://arxiv.org/abs/2408.02862v1",
    "primary_category": "cs.CY",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "A Framework for Fine-Tuning LLMs using Heterogeneous Feedback",
    "authors": "Ryan Aponte, Ryan A. Rossi, Shunan Guo, Franck Dernoncourt, Tong Yu, Xiang Chen, Subrata Mitra, Nedim Lipka",
    "abstract": "Large language models (LLMs) have been applied to a wide range of tasks,\nincluding text summarization, web navigation, and chatbots. They have\nbenefitted from supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) following an unsupervised pretraining. These datasets can\nbe difficult to collect, limited in scope, and vary in sample quality.\nAdditionally, datasets can vary extensively in supervision format, from\nnumerical to binary as well as multi-dimensional with many different values. We\npresent a framework for fine-tuning LLMs using heterogeneous feedback, which\nhas two main components. First, we combine the heterogeneous feedback data into\na single supervision format, compatible with methods like SFT and RLHF. Next,\ngiven this unified feedback dataset, we extract a high-quality and diverse\nsubset to obtain performance increases potentially exceeding the full dataset.\nWe conduct extensive experiments to understand the effectiveness of these\ntechniques for incorporating heterogeneous feedback, and demonstrate\nimprovements from using a high-quality and diverse subset of the data. We find\nthat our framework is able to improve models in multiple areas simultaneously,\nsuch as in instruction following and bias reduction.",
    "arxiv_id": "2408.02861v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02861v1",
    "abstract_url": "http://arxiv.org/abs/2408.02861v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Multistain Pretraining for Slide Representation Learning in Pathology",
    "authors": "Guillaume Jaume, Anurag Vaidya, Andrew Zhang, Andrew H. Song, Richard J. Chen, Sharifa Sahai, Dandan Mo, Emilio Madrigal, Long Phi Le, Faisal Mahmood",
    "abstract": "Developing self-supervised learning (SSL) models that can learn universal and\ntransferable representations of H&E gigapixel whole-slide images (WSIs) is\nbecoming increasingly valuable in computational pathology. These models hold\nthe potential to advance critical tasks such as few-shot classification, slide\nretrieval, and patient stratification. Existing approaches for slide\nrepresentation learning extend the principles of SSL from small images (e.g.,\n224 x 224 patches) to entire slides, usually by aligning two different\naugmentations (or views) of the slide. Yet the resulting representation remains\nconstrained by the limited clinical and biological diversity of the views.\nInstead, we postulate that slides stained with multiple markers, such as\nimmunohistochemistry, can be used as different views to form a rich\ntask-agnostic training signal. To this end, we introduce Madeleine, a\nmultimodal pretraining strategy for slide representation learning. Madeleine is\ntrained with a dual global-local cross-stain alignment objective on large\ncohorts of breast cancer samples (N=4,211 WSIs across five stains) and kidney\ntransplant samples (N=12,070 WSIs across four stains). We demonstrate the\nquality of slide representations learned by Madeleine on various downstream\nevaluations, ranging from morphological and molecular classification to\nprognostic prediction, comprising 21 tasks using 7,299 WSIs from multiple\nmedical centers. Code is available at https://github.com/mahmoodlab/MADELEINE.",
    "arxiv_id": "2408.02859v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02859v1",
    "abstract_url": "http://arxiv.org/abs/2408.02859v1",
    "primary_category": "eess.IV",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Active Learning for WBAN-based Health Monitoring",
    "authors": "Cho-Chun Chiu, Tuan Nguyen, Ting He, Shiqiang Wang, Beom-Su Kim, Ki-Il Kim",
    "abstract": "We consider a novel active learning problem motivated by the need of learning\nmachine learning models for health monitoring in wireless body area network\n(WBAN). Due to the limited resources at body sensors, collecting each unlabeled\nsample in WBAN incurs a nontrivial cost. Moreover, training health monitoring\nmodels typically requires labels indicating the patient's health state that\nneed to be generated by healthcare professionals, which cannot be obtained at\nthe same pace as data collection. These challenges make our problem\nfundamentally different from classical active learning, where unlabeled samples\nare free and labels can be queried in real time. To handle these challenges, we\npropose a two-phased active learning method, consisting of an online phase\nwhere a coreset construction algorithm is proposed to select a subset of\nunlabeled samples based on their noisy predictions, and an offline phase where\nthe selected samples are labeled to train the target model. The samples\nselected by our algorithm are proved to yield a guaranteed error in\napproximating the full dataset in evaluating the loss function. Our evaluation\nbased on real health monitoring data and our own experimentation demonstrates\nthat our solution can drastically save the data curation cost without\nsacrificing the quality of the target model.",
    "arxiv_id": "2408.02849v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02849v1",
    "abstract_url": "http://arxiv.org/abs/2408.02849v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Heterogeneous graph attention network improves cancer multiomics integration",
    "authors": "Sina Tabakhi, Charlotte Vandermeulen, Ian Sudbery, Haiping Lu",
    "abstract": "The increase in high-dimensional multiomics data demands advanced integration\nmodels to capture the complexity of human diseases. Graph-based deep learning\nintegration models, despite their promise, struggle with small patient cohorts\nand high-dimensional features, often applying independent feature selection\nwithout modeling relationships among omics. Furthermore, conventional\ngraph-based omics models focus on homogeneous graphs, lacking multiple types of\nnodes and edges to capture diverse structures. We introduce a Heterogeneous\nGraph ATtention network for omics integration (HeteroGATomics) to improve\ncancer diagnosis. HeteroGATomics performs joint feature selection through a\nmulti-agent system, creating dedicated networks of feature and patient\nsimilarity for each omic modality. These networks are then combined into one\nheterogeneous graph for learning holistic omic-specific representations and\nintegrating predictions across modalities. Experiments on three cancer\nmultiomics datasets demonstrate HeteroGATomics' superior performance in cancer\ndiagnosis. Moreover, HeteroGATomics enhances interpretability by identifying\nimportant biomarkers contributing to the diagnosis outcomes.",
    "arxiv_id": "2408.02845v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02845v1",
    "abstract_url": "http://arxiv.org/abs/2408.02845v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Evaluating Posterior Probabilities: Decision Theory, Proper Scoring Rules, and Calibration",
    "authors": "Luciana Ferrer, Daniel Ramos",
    "abstract": "Most machine learning classifiers are designed to output posterior\nprobabilities for the classes given the input sample. These probabilities may\nbe used to make the categorical decision on the class of the sample; provided\nas input to a downstream system; or provided to a human for interpretation.\nEvaluating the quality of the posteriors generated by these system is an\nessential problem which was addressed decades ago with the invention of proper\nscoring rules (PSRs). Unfortunately, much of the recent machine learning\nliterature uses calibration metrics -- most commonly, the expected calibration\nerror (ECE) -- as a proxy to assess posterior performance. The problem with\nthis approach is that calibration metrics reflect only one aspect of the\nquality of the posteriors, ignoring the discrimination performance. For this\nreason, we argue that calibration metrics should play no role in the assessment\nof posterior quality. Expected PSRs should instead be used for this job,\npreferably normalized for ease of interpretation. In this work, we first give a\nbrief review of PSRs from a practical perspective, motivating their definition\nusing Bayes decision theory. We discuss why expected PSRs provide a principled\nmeasure of the quality of a system's posteriors and why calibration metrics are\nnot the right tool for this job. We argue that calibration metrics, while not\nuseful for performance assessment, may be used as diagnostic tools during\nsystem development. With this purpose in mind, we discuss a simple and\npractical calibration metric, called calibration loss, derived from a\ndecomposition of expected PSRs. We compare this metric with the ECE and with\nthe expected score divergence calibration metric from the PSR literature and\nargue, using theoretical and empirical evidence, that calibration loss is\nsuperior to these two metrics.",
    "arxiv_id": "2408.02841v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02841v1",
    "abstract_url": "http://arxiv.org/abs/2408.02841v1",
    "primary_category": "stat.ML",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Optimizing Cox Models with Stochastic Gradient Descent: Theoretical Foundations and Practical Guidances",
    "authors": "Lang Zeng, Weijing Tang, Zhao Ren, Ying Ding",
    "abstract": "Optimizing Cox regression and its neural network variants poses substantial\ncomputational challenges in large-scale studies. Stochastic gradient descent\n(SGD), known for its scalability in model optimization, has recently been\nadapted to optimize Cox models. Unlike its conventional application, which\ntypically targets a sum of independent individual loss, SGD for Cox models\nupdates parameters based on the partial likelihood of a subset of data. Despite\nits empirical success, the theoretical foundation for optimizing Cox partial\nlikelihood with SGD is largely underexplored. In this work, we demonstrate that\nthe SGD estimator targets an objective function that is batch-size-dependent.\nWe establish that the SGD estimator for the Cox neural network (Cox-NN) is\nconsistent and achieves the optimal minimax convergence rate up to a\npolylogarithmic factor. For Cox regression, we further prove the\n$\\sqrt{n}$-consistency and asymptotic normality of the SGD estimator, with\nvariance depending on the batch size. Furthermore, we quantify the impact of\nbatch size on Cox-NN training and its effect on the SGD estimator's asymptotic\nefficiency in Cox regression. These findings are validated by extensive\nnumerical experiments and provide guidance for selecting batch sizes in SGD\napplications. Finally, we demonstrate the effectiveness of SGD in a real-world\napplication where GD is unfeasible due to the large scale of data.",
    "arxiv_id": "2408.02839v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02839v1",
    "abstract_url": "http://arxiv.org/abs/2408.02839v1",
    "primary_category": "stat.ML",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Interpretation of the Intent Detection Problem as Dynamics in a Low-dimensional Space",
    "authors": "Eduardo Sanchez-Karhunen, Jose F. Quesada-Moreno, Miguel A. Guti\u00e9rrez-Naranjo",
    "abstract": "Intent detection is a text classification task whose aim is to recognize and\nlabel the semantics behind a users query. It plays a critical role in various\nbusiness applications. The output of the intent detection module strongly\nconditions the behavior of the whole system. This sequence analysis task is\nmainly tackled using deep learning techniques. Despite the widespread use of\nthese techniques, the internal mechanisms used by networks to solve the problem\nare poorly understood. Recent lines of work have analyzed the computational\nmechanisms learned by RNNs from a dynamical systems perspective. In this work,\nwe investigate how different RNN architectures solve the SNIPS intent detection\nproblem. Sentences injected into trained networks can be interpreted as\ntrajectories traversing a hidden state space. This space is constrained to a\nlow-dimensional manifold whose dimensionality is related to the embedding and\nhidden layer sizes. To generate predictions, RNN steers the trajectories\ntowards concrete regions, spatially aligned with the output layer matrix rows\ndirections. Underlying the system dynamics, an unexpected fixed point topology\nhas been identified with a limited number of attractors. Our results provide\nnew insights into the inner workings of networks that solve the intent\ndetection task.",
    "arxiv_id": "2408.02838v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02838v1",
    "abstract_url": "http://arxiv.org/abs/2408.02838v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Training a multilayer dynamical spintronic network with standard machine learning tools to perform time series classification",
    "authors": "Erwan Plouet, D\u00e9dalo Sanz-Hern\u00e1ndez, Aymeric Vecchiola, Julie Grollier, Frank Mizrahi",
    "abstract": "The ability to process time-series at low energy cost is critical for many\napplications. Recurrent neural network, which can perform such tasks, are\ncomputationally expensive when implementing in software on conventional\ncomputers. Here we propose to implement a recurrent neural network in hardware\nusing spintronic oscillators as dynamical neurons. Using numerical simulations,\nwe build a multi-layer network and demonstrate that we can use backpropagation\nthrough time (BPTT) and standard machine learning tools to train this network.\nLeveraging the transient dynamics of the spintronic oscillators, we solve the\nsequential digits classification task with $89.83\\pm2.91~\\%$ accuracy, as good\nas the equivalent software network. We devise guidelines on how to choose the\ntime constant of the oscillators as well as hyper-parameters of the network to\nadapt to different input time scales.",
    "arxiv_id": "2408.02835v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02835v1",
    "abstract_url": "http://arxiv.org/abs/2408.02835v1",
    "primary_category": "cond-mat.dis-nn",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "DaCapo: a modular deep learning framework for scalable 3D image segmentation",
    "authors": "William Patton, Jeff L. Rhoades, Marwan Zouinkhi, David G. Ackerman, Caroline Malin-Mayor, Diane Adjavon, Larissa Heinrich, Davis Bennett, Yurii Zubov, CellMap Project Team, Aubrey V. Weigel, Jan Funke",
    "abstract": "DaCapo is a specialized deep learning library tailored to expedite the\ntraining and application of existing machine learning approaches on large,\nnear-isotropic image data. In this correspondence, we introduce DaCapo's unique\nfeatures optimized for this specific domain, highlighting its modular\nstructure, efficient experiment management tools, and scalable deployment\ncapabilities. We discuss its potential to improve access to large-scale,\nisotropic image segmentation and invite the community to explore and contribute\nto this open-source initiative.",
    "arxiv_id": "2408.02834v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02834v1",
    "abstract_url": "http://arxiv.org/abs/2408.02834v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Adaptive Learning for Quantum Linear Regression",
    "authors": "Costantino Carugno, Maurizio Ferrari Dacrema, Paolo Cremonesi",
    "abstract": "The recent availability of quantum annealers as cloud-based services has\nenabled new ways to handle machine learning problems, and several relevant\nalgorithms have been adapted to run on these devices. In a recent work, linear\nregression was formulated as a quadratic binary optimization problem that can\nbe solved via quantum annealing. Although this approach promises a\ncomputational time advantage for large datasets, the quality of the solution is\nlimited by the necessary use of a precision vector, used to approximate the\nreal-numbered regression coefficients in the quantum formulation. In this work,\nwe focus on the practical challenge of improving the precision vector encoding:\ninstead of setting an array of generic values equal for all coefficients, we\nallow each one to be expressed by its specific precision, which is tuned with a\nsimple adaptive algorithm. This approach is evaluated on synthetic datasets of\nincreasing size, and linear regression is solved using the D-Wave Advantage\nquantum annealer, as well as classical solvers. To the best of our knowledge,\nthis is the largest dataset ever evaluated for linear regression on a quantum\nannealer. The results show that our formulation is able to deliver improved\nsolution quality in all instances, and could better exploit the potential of\ncurrent quantum devices.",
    "arxiv_id": "2408.02833v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02833v1",
    "abstract_url": "http://arxiv.org/abs/2408.02833v1",
    "primary_category": "quant-ph",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Setting the duration of online A/B experiments",
    "authors": "Harrison H. Li, Chaoyu Yu",
    "abstract": "In designing an online A/B experiment, it is crucial to select a sample size\nand duration that ensure the resulting confidence interval (CI) for the\ntreatment effect is the right width to detect an effect of meaningful magnitude\nwith sufficient statistical power without wasting resources. While the\nrelationship between sample size and CI width is well understood, the effect of\nexperiment duration on CI width remains less clear. This paper provides an\nanalytical formula for the width of a CI based on a ratio treatment effect\nestimator as a function of both sample size (N) and duration (T). The formula\nis derived from a mixed effects model with two variance components. One\ncomponent, referred to as the temporal variance, persists over time for\nexperiments where the same users are kept in the same experiment arm across\ndifferent days. The remaining error variance component, by contrast, decays to\nzero as T gets large. The formula we derive introduces a key parameter that we\ncall the user-specific temporal correlation (UTC), which quantifies the\nrelative sizes of the two variance components and can be estimated from\nhistorical experiments. Higher UTC indicates a slower decay in CI width over\ntime. On the other hand, when the UTC is 0 -- as for experiments where users\nshuffle in and out of the experiment across days -- the CI width decays at the\nstandard parametric 1/T rate. We also study how access to pre-period data for\nthe users in the experiment affects the CI width decay. We show our formula\nclosely explains CI widths on real A/B experiments at YouTube.",
    "arxiv_id": "2408.02830v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02830v1",
    "abstract_url": "http://arxiv.org/abs/2408.02830v1",
    "primary_category": "stat.ME",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Wave-RVFL: A Randomized Neural Network Based on Wave Loss Function",
    "authors": "M. Sajid, A. Quadir, M. Tanveer",
    "abstract": "The random vector functional link (RVFL) network is well-regarded for its\nstrong generalization capabilities in the field of machine learning. However,\nits inherent dependencies on the square loss function make it susceptible to\nnoise and outliers. Furthermore, the calculation of RVFL's unknown parameters\nnecessitates matrix inversion of the entire training sample, which constrains\nits scalability. To address these challenges, we propose the Wave-RVFL, an RVFL\nmodel incorporating the wave loss function. We formulate and solve the proposed\noptimization problem of the Wave-RVFL using the adaptive moment estimation\n(Adam) algorithm in a way that successfully eliminates the requirement for\nmatrix inversion and significantly enhances scalability. The Wave-RVFL exhibits\nrobustness against noise and outliers by preventing over-penalization of\ndeviations, thereby maintaining a balanced approach to managing noise and\noutliers. The proposed Wave-RVFL model is evaluated on multiple UCI datasets,\nboth with and without the addition of noise and outliers, across various\ndomains and sizes. Empirical results affirm the superior performance and\nrobustness of the Wave-RVFL compared to baseline models, establishing it as a\nhighly effective and scalable classification solution.",
    "arxiv_id": "2408.02824v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02824v1",
    "abstract_url": "http://arxiv.org/abs/2408.02824v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Continuous Monitoring via Repeated Significance",
    "authors": "Eric Bax, Arundhyoti Sarkar, Alex Shtoff",
    "abstract": "Requiring statistical significance at multiple interim analyses to declare a\nstatistically significant result for an AB test allows less stringent\nrequirements for significance at each interim analysis. Repeated repeated\nsignificance competes well with methods built on assumptions about the test --\nassumptions that may be impossible to evaluate a priori and may require extra\ndata to evaluate empirically.\n  Instead, requiring repeated significance allows the data itself to prove\ndirectly that the required results are not due to chance alone. We explain how\nto apply tests with repeated significance to continuously monitor unbounded\ntests -- tests that do not have an a priori bound on running time or number of\nobservations. We show that it is impossible to maintain a constant requirement\nfor significance for unbounded tests, but that we can come arbitrarily close to\nthat goal.",
    "arxiv_id": "2408.02821v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02821v1",
    "abstract_url": "http://arxiv.org/abs/2408.02821v1",
    "primary_category": "stat.ME",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Pre-trained Encoder Inference: Revealing Upstream Encoders In Downstream Machine Learning Services",
    "authors": "Shaopeng Fu, Xuexue Sun, Ke Qing, Tianhang Zheng, Di Wang",
    "abstract": "Though pre-trained encoders can be easily accessed online to build downstream\nmachine learning (ML) services quickly, various attacks have been designed to\ncompromise the security and privacy of these encoders. While most attacks\ntarget encoders on the upstream side, it remains unknown how an encoder could\nbe threatened when deployed in a downstream ML service. This paper unveils a\nnew vulnerability: the Pre-trained Encoder Inference (PEI) attack, which posts\nprivacy threats toward encoders hidden behind downstream ML services. By only\nproviding API accesses to a targeted downstream service and a set of candidate\nencoders, the PEI attack can infer which encoder is secretly used by the\ntargeted service based on candidate ones. We evaluate the attack performance of\nPEI against real-world encoders on three downstream tasks: image\nclassification, text classification, and text-to-image generation. Experiments\nshow that the PEI attack succeeds in revealing the hidden encoder in most cases\nand seldom makes mistakes even when the hidden encoder is not in the candidate\nset. We also conducted a case study on one of the most recent vision-language\nmodels, LLaVA, to illustrate that the PEI attack is useful in assisting other\nML attacks such as adversarial attacks. The code is available at\nhttps://github.com/fshp971/encoder-inference.",
    "arxiv_id": "2408.02814v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02814v1",
    "abstract_url": "http://arxiv.org/abs/2408.02814v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Mitigating Malicious Attacks in Federated Learning via Confidence-aware Defense",
    "authors": "Qilei Li, Ahmed M. Abdelmoniem",
    "abstract": "Federated Learning (FL) is an emerging distributed machine learning paradigm\nthat allows multiple clients to collaboratively train a global model without\nsharing private local data. However, FL systems are vulnerable to attacks from\nmalicious clients, who can degrade the global model performance through data\npoisoning and model poisoning. Existing defense methods typically focus on a\nsingle type of attack, such as Byzantine attacks or backdoor attacks, and are\noften ineffective against potential data poisoning attacks like label flipping\nand label shuffling. Additionally, these methods often lack accuracy and\nrobustness in detecting and handling malicious updates. To address these\nissues, we propose a novel method based on model confidence scores, which\nevaluates the uncertainty of client model updates to detect and defend against\nmalicious clients. Our approach is comprehensively effective for both model\npoisoning and data poisoning attacks and is capable of accurately identifying\nand mitigating potential malicious updates from being aggregated. Experimental\nresults demonstrate that our method significantly improves the robustness of FL\nsystems against various types of attacks, also achieving higher model accuracy\nand stability across various scenarios.",
    "arxiv_id": "2408.02813v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02813v1",
    "abstract_url": "http://arxiv.org/abs/2408.02813v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Development of REGAI: Rubric Enabled Generative Artificial Intelligence",
    "authors": "Zach Johnson, Jeremy Straub",
    "abstract": "This paper presents and evaluates a new retrieval augmented generation (RAG)\nand large language model (LLM)-based artificial intelligence (AI) technique:\nrubric enabled generative artificial intelligence (REGAI). REGAI uses rubrics,\nwhich can be created manually or automatically by the system, to enhance the\nperformance of LLMs for evaluation purposes. REGAI improves on the performance\nof both classical LLMs and RAG-based LLM techniques. This paper describes\nREGAI, presents data regarding its performance and discusses several possible\napplication areas for the technology.",
    "arxiv_id": "2408.02811v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02811v1",
    "abstract_url": "http://arxiv.org/abs/2408.02811v1",
    "primary_category": "cs.AI",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Deciphering Air Travel Disruptions: A Machine Learning Approach",
    "authors": "Aravinda Jatavallabha, Jacob Gerlach, Aadithya Naresh",
    "abstract": "This research investigates flight delay trends by examining factors such as\ndeparture time, airline, and airport. It employs regression machine learning\nmethods to predict the contributions of various sources to delays. Time-series\nmodels, including LSTM, Hybrid LSTM, and Bi-LSTM, are compared with baseline\nregression models such as Multiple Regression, Decision Tree Regression, Random\nForest Regression, and Neural Network. Despite considerable errors in the\nbaseline models, the study aims to identify influential features in delay\nprediction, potentially informing flight planning strategies. Unlike previous\nwork, this research focuses on regression tasks and explores the use of\ntime-series models for predicting flight delays. It offers insights into\naviation operations by independently analyzing each delay component (e.g.,\nsecurity, weather).",
    "arxiv_id": "2408.02802v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02802v1",
    "abstract_url": "http://arxiv.org/abs/2408.02802v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Sparse Deep Learning Models with the $\\ell_1$ Regularization",
    "authors": "Lixin Shen, Rui Wang, Yuesheng Xu, Mingsong Yan",
    "abstract": "Sparse neural networks are highly desirable in deep learning in reducing its\ncomplexity. The goal of this paper is to study how choices of regularization\nparameters influence the sparsity level of learned neural networks. We first\nderive the $\\ell_1$-norm sparsity-promoting deep learning models including\nsingle and multiple regularization parameters models, from a statistical\nviewpoint. We then characterize the sparsity level of a regularized neural\nnetwork in terms of the choice of the regularization parameters. Based on the\ncharacterizations, we develop iterative algorithms for selecting regularization\nparameters so that the weight parameters of the resulting deep neural network\nenjoy prescribed sparsity levels. Numerical experiments are presented to\ndemonstrate the effectiveness of the proposed algorithms in choosing desirable\nregularization parameters and obtaining corresponding neural networks having\nboth of predetermined sparsity levels and satisfactory approximation accuracy.",
    "arxiv_id": "2408.02801v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02801v1",
    "abstract_url": "http://arxiv.org/abs/2408.02801v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Examining Gender and Power on Wikipedia Through Face and Politeness",
    "authors": "Adil Soubki, Shyne Choi, Owen Rambow",
    "abstract": "We propose a framework for analyzing discourse by combining two\ninterdependent concepts from sociolinguistic theory: face acts and politeness.\nWhile politeness has robust existing tools and data, face acts are less\nresourced. We introduce a new corpus created by annotating Wikipedia talk pages\nwith face acts and we use this to train a face act tagger. We then employ our\nframework to study how face and politeness interact with gender and power in\ndiscussions between Wikipedia editors. Among other findings, we observe that\nfemale Wikipedians are not only more polite, which is consistent with prior\nstudies, but that this difference corresponds with significantly more language\ndirected at humbling aspects of their own face. Interestingly, the distinction\nnearly vanishes once limiting to editors with administrative power.",
    "arxiv_id": "2408.02798v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02798v1",
    "abstract_url": "http://arxiv.org/abs/2408.02798v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Algorithm-Informed Graph Neural Networks for Leakage Detection and Localization in Water Distribution Networks",
    "authors": "Zepeng Zhang, Olga Fink",
    "abstract": "Detecting and localizing leakages is a significant challenge for the\nefficient and sustainable management of water distribution networks (WDN).\nLeveraging the inherent graph structure of WDNs, recent approaches have used\ngraph-based data-driven methods. However, these methods often learn shortcuts\nthat work well with in-distribution data but fail to generalize to\nout-of-distribution data. To address this limitation and inspired by the\nperfect generalization ability of classical algorithms, we propose an\nalgorithm-informed graph neural network (AIGNN). Recognizing that WDNs function\nas flow networks, incorporating max-flow information can be beneficial for\ninferring pressures. In the proposed framework, we first train AIGNN to emulate\nthe Ford-Fulkerson algorithm for solving max-flow problems. This algorithmic\nknowledge is then transferred to address the pressure estimation problem in\nWDNs. Two AIGNNs are deployed, one to reconstruct pressure based on the current\nmeasurements, and another to predict pressure based on previous measurements.\nLeakages are detected and localized by comparing the outputs of the\nreconstructor and the predictor. By pretraining AIGNNs to reason like\nalgorithms, they are expected to extract more task-relevant and generalizable\nfeatures. Experimental results demonstrate that the proposed algorithm-informed\napproach achieves superior results with better generalization ability compared\nto GNNs that do not incorporate algorithmic knowledge.",
    "arxiv_id": "2408.02797v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02797v1",
    "abstract_url": "http://arxiv.org/abs/2408.02797v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "4D-Var using Hessian approximation and backpropagation applied to automatically-differentiable numerical and machine learning models",
    "authors": "Kylen Solvik, Stephen G. Penny, Stephan Hoyer",
    "abstract": "Constraining a numerical weather prediction (NWP) model with observations via\n4D variational (4D-Var) data assimilation is often difficult to implement in\npractice due to the need to develop and maintain a software-based tangent\nlinear model and adjoint model. One of the most common 4D-Var algorithms uses\nan incremental update procedure, which has been shown to be an approximation of\nthe Gauss-Newton method. Here we demonstrate that when using a forecast model\nthat supports automatic differentiation, an efficient and in some cases more\naccurate alternative approximation of the Gauss-Newton method can be applied by\ncombining backpropagation of errors with Hessian approximation. This approach\ncan be used with either a conventional numerical model implemented within a\nsoftware framework that supports automatic differentiation, or a machine\nlearning (ML) based surrogate model. We test the new approach on a variety of\nLorenz-96 and quasi-geostrophic models. The results indicate potential for a\ndeeper integration of modeling, data assimilation, and new technologies in a\nnext-generation of operational forecast systems that leverage weather models\ndesigned to support automatic differentiation.",
    "arxiv_id": "2408.02767v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02767v1",
    "abstract_url": "http://arxiv.org/abs/2408.02767v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "ConDL: Detector-Free Dense Image Matching",
    "authors": "Monika Kwiatkowski, Simon Matern, Olaf Hellwich",
    "abstract": "In this work, we introduce a deep-learning framework designed for estimating\ndense image correspondences. Our fully convolutional model generates dense\nfeature maps for images, where each pixel is associated with a descriptor that\ncan be matched across multiple images. Unlike previous methods, our model is\ntrained on synthetic data that includes significant distortions, such as\nperspective changes, illumination variations, shadows, and specular highlights.\nUtilizing contrastive learning, our feature maps achieve greater invariance to\nthese distortions, enabling robust matching. Notably, our method eliminates the\nneed for a keypoint detector, setting it apart from many existing\nimage-matching techniques.",
    "arxiv_id": "2408.02766v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02766v1",
    "abstract_url": "http://arxiv.org/abs/2408.02766v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Dimensionality Reduction and Nearest Neighbors for Improving Out-of-Distribution Detection in Medical Image Segmentation",
    "authors": "McKell Woodland, Nihil Patel, Austin Castelo, Mais Al Taie, Mohamed Eltaher, Joshua P. Yung, Tucker J. Netherton, Tiffany L. Calderone, Jessica I. Sanchez, Darrel W. Cleere, Ahmed Elsaiey, Nakul Gupta, David Victor, Laura Beretta, Ankit B. Patel Kristy K. Brock",
    "abstract": "Clinically deployed deep learning-based segmentation models are known to fail\non data outside of their training distributions. While clinicians review the\nsegmentations, these models tend to perform well in most instances, which could\nexacerbate automation bias. Therefore, detecting out-of-distribution images at\ninference is critical to warn the clinicians that the model likely failed. This\nwork applied the Mahalanobis distance (MD) post hoc to the bottleneck features\nof four Swin UNETR and nnU-net models that segmented the liver on T1-weighted\nmagnetic resonance imaging and computed tomography. By reducing the dimensions\nof the bottleneck features with either principal component analysis or uniform\nmanifold approximation and projection, images the models failed on were\ndetected with high performance and minimal computational load. In addition,\nthis work explored a non-parametric alternative to the MD, a k-th nearest\nneighbors distance (KNN). KNN drastically improved scalability and performance\nover MD when both were applied to raw and average-pooled bottleneck features.",
    "arxiv_id": "2408.02761v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02761v1",
    "abstract_url": "http://arxiv.org/abs/2408.02761v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Classification of Raw MEG/EEG Data with Detach-Rocket Ensemble: An Improved ROCKET Algorithm for Multivariate Time Series Analysis",
    "authors": "Adri\u00e0 Solana, Erik Frans\u00e9n, Gonzalo Uribarri",
    "abstract": "Multivariate Time Series Classification (MTSC) is a ubiquitous problem in\nscience and engineering, particularly in neuroscience, where most data\nacquisition modalities involve the simultaneous time-dependent recording of\nbrain activity in multiple brain regions. In recent years, Random Convolutional\nKernel models such as ROCKET and MiniRocket have emerged as highly effective\ntime series classification algorithms, capable of achieving state-of-the-art\naccuracy results with low computational load. Despite their success, these\ntypes of models face two major challenges when employed in neuroscience: 1)\nthey struggle to deal with high-dimensional data such as EEG and MEG, and 2)\nthey are difficult to interpret. In this work, we present a novel ROCKET-based\nalgorithm, named Detach-Rocket Ensemble, that is specifically designed to\naddress these two problems in MTSC. Our algorithm leverages pruning to provide\nan integrated estimation of channel importance, and ensembles to achieve better\naccuracy and provide a label probability. Using a synthetic multivariate time\nseries classification dataset in which we control the amount of information\ncarried by each of the channels, we first show that our algorithm is able to\ncorrectly recover the channel importance for classification. Then, using two\nreal-world datasets, a MEG dataset and an EEG dataset, we show that\nDetach-Rocket Ensemble is able to provide both interpretable channel relevance\nand competitive classification accuracy, even when applied directly to the raw\nbrain data, without the need for feature engineering.",
    "arxiv_id": "2408.02760v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02760v1",
    "abstract_url": "http://arxiv.org/abs/2408.02760v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "A Novel Hybrid Approach for Tornado Prediction in the United States: Kalman-Convolutional BiLSTM with Multi-Head Attention",
    "authors": "Jiawei Zhou",
    "abstract": "Tornadoes are among the most intense atmospheric vortex phenomena and pose\nsignificant challenges for detection and forecasting. Conventional methods,\nwhich heavily depend on ground-based observations and radar data, are limited\nby issues such as decreased accuracy over greater distances and a high rate of\nfalse positives. To address these challenges, this study utilizes the Seamless\nHybrid Scan Reflectivity (SHSR) dataset from the Multi-Radar Multi-Sensor\n(MRMS) system, which integrates data from multiple radar sources to enhance\naccuracy. A novel hybrid model, the Kalman-Convolutional BiLSTM with Multi-Head\nAttention, is introduced to improve dynamic state estimation and capture both\nspatial and temporal dependencies within the data. This model demonstrates\nsuperior performance in precision, recall, F1-Score, and accuracy compared to\nmethods such as K-Nearest Neighbors (KNN) and LightGBM. The results highlight\nthe considerable potential of advanced machine learning techniques to improve\ntornado prediction and reduce false alarm rates. Future research will focus on\nexpanding datasets, exploring innovative model architectures, and incorporating\nlarge language models (LLMs) to provide deeper insights. This research\nintroduces a novel model for tornado prediction, offering a robust framework\nfor enhancing forecasting accuracy and public safety.",
    "arxiv_id": "2408.02751v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02751v1",
    "abstract_url": "http://arxiv.org/abs/2408.02751v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "KAN we improve on HEP classification tasks? Kolmogorov-Arnold Networks applied to an LHC physics example",
    "authors": "Johannes Erdmann, Florian Mausolf, Jan Lukas Sp\u00e4h",
    "abstract": "Recently, Kolmogorov-Arnold Networks (KANs) have been proposed as an\nalternative to multilayer perceptrons, suggesting advantages in performance and\ninterpretability. We study a typical binary event classification task in\nhigh-energy physics including high-level features and comment on the\nperformance and interpretability of KANs in this context. We find that the\nlearned activation functions of a one-layer KAN resemble the log-likelihood\nratio of the input features. In deeper KANs, the activations in the first KAN\nlayer differ from those in the one-layer KAN, which indicates that the deeper\nKANs learn more complex representations of the data. We study KANs with\ndifferent depths and widths and we compare them to multilayer perceptrons in\nterms of performance and number of trainable parameters. For the chosen\nclassification task, we do not find that KANs are more parameter efficient.\nHowever, small KANs may offer advantages in terms of interpretability that come\nat the cost of only a moderate loss in performance.",
    "arxiv_id": "2408.02743v1",
    "pdf_url": "http://arxiv.org/pdf/2408.02743v1",
    "abstract_url": "http://arxiv.org/abs/2408.02743v1",
    "primary_category": "hep-ph",
    "published_date": "2024-08-05",
    "votes": 0,
    "prompt": "LLM ",
    "model": "gpt-4-turbo"
  },
  {
    "title": "KAN 2.0: Kolmogorov-Arnold Networks Meet Science",
    "authors": "Ziming Liu, Pingchuan Ma, Yixuan Wang, Wojciech Matusik, Max Tegmark",
    "abstract": "A major challenge of AI + Science lies in their inherent incompatibility:\ntoday's AI is primarily based on connectionism, while science depends on\nsymbolism. To bridge the two worlds, we propose a framework to seamlessly\nsynergize Kolmogorov-Arnold Networks (KANs) and science. The framework\nhighlights KANs' usage for three aspects of scientific discovery: identifying\nrelevant features, revealing modular structures, and discovering symbolic\nformulas. The synergy is bidirectional: science to KAN (incorporating\nscientific knowledge into KANs), and KAN to science (extracting scientific\ninsights from KANs). We highlight major new functionalities in the pykan\npackage: (1) MultKAN: KANs with multiplication nodes. (2) kanpiler: a KAN\ncompiler that compiles symbolic formulas into KANs. (3) tree converter: convert\nKANs (or any neural networks) to tree graphs. Based on these tools, we\ndemonstrate KANs' capability to discover various types of physical laws,\nincluding conserved quantities, Lagrangians, symmetries, and constitutive laws.",
    "arxiv_id": "2408.10205v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10205v1",
    "abstract_url": "http://arxiv.org/abs/2408.10205v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": -1,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Criticality Leveraged Adversarial Training (CLAT) for Boosted Performance via Parameter Efficiency",
    "authors": "Bhavna Gopal, Huanrui Yang, Jingyang Zhang, Mark Horton, Yiran Chen",
    "abstract": "Adversarial training enhances neural network robustness but suffers from a\ntendency to overfit and increased generalization errors on clean data. This\nwork introduces CLAT, an innovative approach that mitigates adversarial\noverfitting by introducing parameter efficiency into the adversarial training\nprocess, improving both clean accuracy and adversarial robustness. Instead of\ntuning the entire model, CLAT identifies and fine-tunes robustness-critical\nlayers - those predominantly learning non-robust features - while freezing the\nremaining model to enhance robustness. It employs dynamic critical layer\nselection to adapt to changes in layer criticality throughout the fine-tuning\nprocess. Empirically, CLAT can be applied on top of existing adversarial\ntraining methods, significantly reduces the number of trainable parameters by\napproximately 95%, and achieves more than a 2% improvement in adversarial\nrobustness compared to baseline methods.",
    "arxiv_id": "2408.10204v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10204v1",
    "abstract_url": "http://arxiv.org/abs/2408.10204v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 1,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Area under the ROC Curve has the Most Consistent Evaluation for Binary Classification",
    "authors": "Jing Li",
    "abstract": "Evaluation Metrics is an important question for model evaluation and model\nselection in binary classification tasks. This study investigates how\nconsistent metrics are at evaluating different models under different data\nscenarios. Analyzing over 150 data scenarios and 18 model evaluation metrics\nusing statistical simulation, I find that for binary classification tasks,\nevaluation metrics that are less influenced by prevalence offer more consistent\nranking of a set of different models. In particular, Area Under the ROC Curve\n(AUC) has smallest variance in ranking of different models. Matthew's\ncorrelation coefficient as a more strict measure of model performance has the\nsecond smallest variance. These patterns holds across a rich set of data\nscenarios and five commonly used machine learning models as well as a naive\nrandom guess model. The results have significant implications for model\nevaluation and model selection in binary classification tasks.",
    "arxiv_id": "2408.10193v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10193v1",
    "abstract_url": "http://arxiv.org/abs/2408.10193v1",
    "primary_category": "stat.ML",
    "published_date": "2024-08-19",
    "votes": -1,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models",
    "authors": "Aviv Bick, Kevin Y. Li, Eric P. Xing, J. Zico Kolter, Albert Gu",
    "abstract": "Transformer architectures have become a dominant paradigm for domains like\nlanguage modeling but suffer in many inference settings due to their\nquadratic-time self-attention. Recently proposed subquadratic architectures,\nsuch as Mamba, have shown promise, but have been pretrained with substantially\nless computational resources than the strongest Transformer models. In this\nwork, we present a method that is able to distill a pretrained Transformer\narchitecture into alternative architectures such as state space models (SSMs).\nThe key idea to our approach is that we can view both Transformers and SSMs as\napplying different forms of mixing matrices over the token sequences. We can\nthus progressively distill the Transformer architecture by matching different\ndegrees of granularity in the SSM: first matching the mixing matrices\nthemselves, then the hidden units at each block, and finally the end-to-end\npredictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant\nbased on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid\nversion (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of the\ntraining data typically used to train models from scratch, Phi-Mamba boasts\nsubstantially stronger performance compared to all past open-source\nnon-Transformer models. MOHAWK allows models like SSMs to leverage\ncomputational resources invested in training Transformer-based architectures,\nhighlighting a new avenue for building such models.",
    "arxiv_id": "2408.10189v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10189v1",
    "abstract_url": "http://arxiv.org/abs/2408.10189v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 1,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models",
    "authors": "Anke Tang, Li Shen, Yong Luo, Shuai Xie, Han Hu, Lefei Zhang, Bo Du, Dacheng Tao",
    "abstract": "Deep model training on extensive datasets is increasingly becoming\ncost-prohibitive, prompting the widespread adoption of deep model fusion\ntechniques to leverage knowledge from pre-existing models. From simple weight\naveraging to more sophisticated methods like AdaMerging, model fusion\neffectively improves model performance and accelerates the development of new\nmodels. However, potential interference between parameters of individual models\nand the lack of interpretability in the fusion progress remain significant\nchallenges. Existing methods often try to resolve the parameter interference\nissue by evaluating attributes of parameters, such as their magnitude or sign,\nor by parameter pruning. In this study, we begin by examining the fine-tuning\nof linear layers through the lens of subspace analysis and explicitly define\nparameter interference as an optimization problem to shed light on this\nsubject. Subsequently, we introduce an innovative approach to model fusion\ncalled zero-shot Sparse MIxture of Low-rank Experts (SMILE) construction, which\nallows for the upscaling of source models into an MoE model without extra data\nor further training. Our approach relies on the observation that fine-tuning\nmostly keeps the important parts from the pre-training, but it uses less\nsignificant or unused areas to adapt to new tasks. Also, the issue of parameter\ninterference, which is intrinsically intractable in the original parameter\nspace, can be managed by expanding the dimensions. We conduct extensive\nexperiments across diverse scenarios, such as image classification and text\ngeneralization tasks, using full fine-tuning and LoRA fine-tuning, and we apply\nour method to large language models (CLIP models, Flan-T5 models, and\nMistral-7B models), highlighting the adaptability and scalability of SMILE.\nCode is available at https://github.com/tanganke/fusion_bench",
    "arxiv_id": "2408.10174v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10174v1",
    "abstract_url": "http://arxiv.org/abs/2408.10174v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 1,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Physics-Aware Combinatorial Assembly Planning using Deep Reinforcement Learning",
    "authors": "Ruixuan Liu, Alan Chen, Weiye Zhao, Changliu Liu",
    "abstract": "Combinatorial assembly uses standardized unit primitives to build objects\nthat satisfy user specifications. Lego is a widely used platform for\ncombinatorial assembly, in which people use unit primitives (ie Lego bricks) to\nbuild highly customizable 3D objects. This paper studies sequence planning for\nphysical combinatorial assembly using Lego. Given the shape of the desired\nobject, we want to find a sequence of actions for placing Lego bricks to build\nthe target object. In particular, we aim to ensure the planned assembly\nsequence is physically executable. However, assembly sequence planning (ASP)\nfor combinatorial assembly is particularly challenging due to its combinatorial\nnature, ie the vast number of possible combinations and complex constraints. To\naddress the challenges, we employ deep reinforcement learning to learn a\nconstruction policy for placing unit primitives sequentially to build the\ndesired object. Specifically, we design an online physics-aware action mask\nthat efficiently filters out invalid actions and guides policy learning. In the\nend, we demonstrate that the proposed method successfully plans physically\nvalid assembly sequences for constructing different Lego structures. The\ngenerated construction plan can be executed in real.",
    "arxiv_id": "2408.10162v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10162v1",
    "abstract_url": "http://arxiv.org/abs/2408.10162v1",
    "primary_category": "cs.RO",
    "published_date": "2024-08-19",
    "votes": -1,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Multilingual Needle in a Haystack: Investigating Long-Context Behavior of Multilingual Large Language Models",
    "authors": "Amey Hengle, Prasoon Bajpai, Soham Dan, Tanmoy Chakraborty",
    "abstract": "While recent large language models (LLMs) demonstrate remarkable abilities in\nresponding to queries in diverse languages, their ability to handle long\nmultilingual contexts is unexplored. As such, a systematic evaluation of the\nlong-context capabilities of LLMs in multilingual settings is crucial,\nspecifically in the context of information retrieval. To address this gap, we\nintroduce the MultiLingual Needle-in-a-Haystack (MLNeedle) test, designed to\nassess a model's ability to retrieve relevant information (the needle) from a\ncollection of multilingual distractor texts (the haystack). This test serves as\nan extension of the multilingual question-answering task, encompassing both\nmonolingual and cross-lingual retrieval. We evaluate four state-of-the-art LLMs\non MLNeedle. Our findings reveal that model performance can vary significantly\nwith language and needle position. Specifically, we observe that model\nperformance is the lowest when the needle is (i) in a language outside the\nEnglish language family and (ii) located in the middle of the input context.\nFurthermore, although some models claim a context size of $8k$ tokens or\ngreater, none demonstrate satisfactory cross-lingual retrieval performance as\nthe context length increases. Our analysis provides key insights into the\nlong-context behavior of LLMs in multilingual settings to guide future\nevaluation protocols. To our knowledge, this is the first study to investigate\nthe multilingual long-context behavior of LLMs.",
    "arxiv_id": "2408.10151v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10151v1",
    "abstract_url": "http://arxiv.org/abs/2408.10151v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-19",
    "votes": -1,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "In-Context Learning with Representations: Contextual Generalization of Trained Transformers",
    "authors": "Tong Yang, Yu Huang, Yingbin Liang, Yuejie Chi",
    "abstract": "In-context learning (ICL) refers to a remarkable capability of pretrained\nlarge language models, which can learn a new task given a few examples during\ninference. However, theoretical understanding of ICL is largely under-explored,\nparticularly whether transformers can be trained to generalize to unseen\nexamples in a prompt, which will require the model to acquire contextual\nknowledge of the prompt for generalization. This paper investigates the\ntraining dynamics of transformers by gradient descent through the lens of\nnon-linear regression tasks. The contextual generalization here can be attained\nvia learning the template function for each task in-context, where all template\nfunctions lie in a linear space with $m$ basis functions. We analyze the\ntraining dynamics of one-layer multi-head transformers to in-contextly predict\nunlabeled inputs given partially labeled prompts, where the labels contain\nGaussian noise and the number of examples in each prompt are not sufficient to\ndetermine the template. Under mild assumptions, we show that the training loss\nfor a one-layer multi-head transformer converges linearly to a global minimum.\nMoreover, the transformer effectively learns to perform ridge regression over\nthe basis functions. To our knowledge, this study is the first provable\ndemonstration that transformers can learn contextual (i.e., template)\ninformation to generalize to both unseen examples and tasks when prompts\ncontain only a small number of query-answer pairs.",
    "arxiv_id": "2408.10147v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10147v1",
    "abstract_url": "http://arxiv.org/abs/2408.10147v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 1,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Robust spectral clustering with rank statistics",
    "authors": "Joshua Cape, Xianshi Yu, Jonquil Z. Liao",
    "abstract": "This paper analyzes the statistical performance of a robust spectral\nclustering method for latent structure recovery in noisy data matrices. We\nconsider eigenvector-based clustering applied to a matrix of nonparametric rank\nstatistics that is derived entrywise from the raw, original data matrix. This\napproach is robust in the sense that, unlike traditional spectral clustering\nprocedures, it can provably recover population-level latent block structure\neven when the observed data matrix includes heavy-tailed entries and has a\nheterogeneous variance profile.\n  Our main theoretical contributions are threefold and hold under flexible data\ngenerating conditions. First, we establish that robust spectral clustering with\nrank statistics can consistently recover latent block structure, viewed as\ncommunities of nodes in a graph, in the sense that unobserved community\nmemberships for all but a vanishing fraction of nodes are correctly recovered\nwith high probability when the data matrix is large. Second, we refine the\nformer result and further establish that, under certain conditions, the\ncommunity membership of any individual, specified node of interest can be\nasymptotically exactly recovered with probability tending to one in the\nlarge-data limit. Third, we establish asymptotic normality results associated\nwith the truncated eigenstructure of matrices whose entries are rank\nstatistics, made possible by synthesizing contemporary entrywise matrix\nperturbation analysis with the classical nonparametric theory of so-called\nsimple linear rank statistics. Collectively, these results demonstrate the\nstatistical utility of rank-based data transformations when paired with\nspectral techniques for dimensionality reduction. Additionally, for a dataset\nof human connectomes, our approach yields parsimonious dimensionality reduction\nand improved recovery of ground-truth neuroanatomical cluster structure.",
    "arxiv_id": "2408.10136v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10136v1",
    "abstract_url": "http://arxiv.org/abs/2408.10136v1",
    "primary_category": "stat.ML",
    "published_date": "2024-08-19",
    "votes": -1,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a Low-Resource Language",
    "authors": "Manjil Karki, Pratik Shakya, Sandesh Acharya, Ravi Pandit, Dinesh Gothe",
    "abstract": "Voice cloning is a prominent feature in personalized speech interfaces. A\nneural vocal cloning system can mimic someone's voice using just a few audio\nsamples. Both speaker encoding and speaker adaptation are topics of research in\nthe field of voice cloning. Speaker adaptation relies on fine-tuning a\nmulti-speaker generative model, which involves training a separate model to\ninfer a new speaker embedding used for speaker encoding. Both methods can\nachieve excellent performance, even with a small number of cloning audios, in\nterms of the speech's naturalness and similarity to the original speaker.\nSpeaker encoding approaches are more appropriate for low-resource deployment\nsince they require significantly less memory and have a faster cloning time\nthan speaker adaption, which can offer slightly greater naturalness and\nsimilarity. The main goal is to create a vocal cloning system that produces\naudio output with a Nepali accent or that sounds like Nepali. For the further\nadvancement of TTS, the idea of transfer learning was effectively used to\naddress several issues that were encountered in the development of this system,\nincluding the poor audio quality and the lack of available data.",
    "arxiv_id": "2408.10128v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10128v1",
    "abstract_url": "http://arxiv.org/abs/2408.10128v1",
    "primary_category": "cs.SD",
    "published_date": "2024-08-19",
    "votes": 1,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Learning Brave Assumption-Based Argumentation Frameworks via ASP",
    "authors": "Emanuele De Angelis, Maurizio Proietti, Francesca Toni",
    "abstract": "Assumption-based Argumentation (ABA) is advocated as a unifying formalism for\nvarious forms of non-monotonic reasoning, including logic programming. It\nallows capturing defeasible knowledge, subject to argumentative debate. While,\nin much existing work, ABA frameworks are given up-front, in this paper we\nfocus on the problem of automating their learning from background knowledge and\npositive/negative examples. Unlike prior work, we newly frame the problem in\nterms of brave reasoning under stable extensions for ABA. We present a novel\nalgorithm based on transformation rules (such as Rote Learning, Folding,\nAssumption Introduction and Fact Subsumption) and an implementation thereof\nthat makes use of Answer Set Programming. Finally, we compare our technique to\nstate-of-the-art ILP systems that learn defeasible knowledge.",
    "arxiv_id": "2408.10126v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10126v1",
    "abstract_url": "http://arxiv.org/abs/2408.10126v1",
    "primary_category": "cs.AI",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models",
    "authors": "Tianyu Zhang, Yuxiang Ren, Chengbin Hou, Hairong Lv, Xuegong Zhang",
    "abstract": "Molecular property prediction is a crucial foundation for drug discovery. In\nrecent years, pre-trained deep learning models have been widely applied to this\ntask. Some approaches that incorporate prior biological domain knowledge into\nthe pre-training framework have achieved impressive results. However, these\nmethods heavily rely on biochemical experts, and retrieving and summarizing\nvast amounts of domain knowledge literature is both time-consuming and\nexpensive. Large Language Models (LLMs) have demonstrated remarkable\nperformance in understanding and efficiently providing general knowledge.\nNevertheless, they occasionally exhibit hallucinations and lack precision in\ngenerating domain-specific knowledge. Conversely, Domain-specific Small Models\n(DSMs) possess rich domain knowledge and can accurately calculate molecular\ndomain-related metrics. However, due to their limited model size and singular\nfunctionality, they lack the breadth of knowledge necessary for comprehensive\nrepresentation learning. To leverage the advantages of both approaches in\nmolecular property prediction, we propose a novel Molecular Graph\nrepresentation learning framework that integrates Large language models and\nDomain-specific small models (MolGraph-LarDo). Technically, we design a\ntwo-stage prompt strategy where DSMs are introduced to calibrate the knowledge\nprovided by LLMs, enhancing the accuracy of domain-specific information and\nthus enabling LLMs to generate more precise textual descriptions for molecular\nsamples. Subsequently, we employ a multi-modal alignment method to coordinate\nvarious modalities, including molecular graphs and their corresponding\ndescriptive texts, to guide the pre-training of molecular representations.\nExtensive experiments demonstrate the effectiveness of the proposed method.",
    "arxiv_id": "2408.10124v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10124v1",
    "abstract_url": "http://arxiv.org/abs/2408.10124v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "PLUTUS: A Well Pre-trained Large Unified Transformer can Unveil Financial Time Series Regularities",
    "authors": "Yuanjian Xu, Anxian Liu, Jianing Hao, Zhenzhuo Li, Shichang Meng, Guang Zhang",
    "abstract": "Financial time series modeling is crucial for understanding and predicting\nmarket behaviors but faces challenges such as non-linearity, non-stationarity,\nand high noise levels. Traditional models struggle to capture complex patterns\ndue to these issues, compounded by limitations in computational resources and\nmodel capacity. Inspired by the success of large language models in NLP, we\nintroduce \\textbf{PLUTUS}, a \\textbf{P}re-trained \\textbf{L}arge\n\\textbf{U}nified \\textbf{T}ransformer-based model that \\textbf{U}nveils\nregularities in financial time \\textbf{S}eries. PLUTUS uses an invertible\nembedding module with contrastive learning and autoencoder techniques to create\nan approximate one-to-one mapping between raw data and patch embeddings.\nTimeFormer, an attention based architecture, forms the core of PLUTUS,\neffectively modeling high-noise time series. We incorporate a novel attention\nmechanisms to capture features across both variable and temporal dimensions.\nPLUTUS is pre-trained on an unprecedented dataset of 100 billion observations,\ndesigned to thrive in noisy financial environments. To our knowledge, PLUTUS is\nthe first open-source, large-scale, pre-trained financial time series model\nwith over one billion parameters. It achieves state-of-the-art performance in\nvarious tasks, demonstrating strong transferability and establishing a robust\nfoundational model for finance. Our research provides technical guidance for\npre-training financial time series data, setting a new standard in the field.",
    "arxiv_id": "2408.10111v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10111v1",
    "abstract_url": "http://arxiv.org/abs/2408.10111v1",
    "primary_category": "cs.AI",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Perturb-and-Compare Approach for Detecting Out-of-Distribution Samples in Constrained Access Environments",
    "authors": "Heeyoung Lee, Hoyoon Byun, Changdae Oh, JinYeong Bak, Kyungwoo Song",
    "abstract": "Accessing machine learning models through remote APIs has been gaining\nprevalence following the recent trend of scaling up model parameters for\nincreased performance. Even though these models exhibit remarkable ability,\ndetecting out-of-distribution (OOD) samples remains a crucial safety concern\nfor end users as these samples may induce unreliable outputs from the model. In\nthis work, we propose an OOD detection framework, MixDiff, that is applicable\neven when the model's parameters or its activations are not accessible to the\nend user. To bypass the access restriction, MixDiff applies an identical\ninput-level perturbation to a given target sample and a similar in-distribution\n(ID) sample, then compares the relative difference in the model outputs of\nthese two samples. MixDiff is model-agnostic and compatible with existing\noutput-based OOD detection methods. We provide theoretical analysis to\nillustrate MixDiff's effectiveness in discerning OOD samples that induce\noverconfident outputs from the model and empirically demonstrate that MixDiff\nconsistently enhances the OOD detection performance on various datasets in\nvision and text domains.",
    "arxiv_id": "2408.10107v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10107v1",
    "abstract_url": "http://arxiv.org/abs/2408.10107v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Federated Frank-Wolfe Algorithm",
    "authors": "Ali Dadras, Sourasekhar Banerjee, Karthik Prakhya, Alp Yurtsever",
    "abstract": "Federated learning (FL) has gained a lot of attention in recent years for\nbuilding privacy-preserving collaborative learning systems. However, FL\nalgorithms for constrained machine learning problems are still limited,\nparticularly when the projection step is costly. To this end, we propose a\nFederated Frank-Wolfe Algorithm (FedFW). FedFW features data privacy, low\nper-iteration cost, and communication of sparse signals. In the deterministic\nsetting, FedFW achieves an $\\varepsilon$-suboptimal solution within\n$O(\\varepsilon^{-2})$ iterations for smooth and convex objectives, and\n$O(\\varepsilon^{-3})$ iterations for smooth but non-convex objectives.\nFurthermore, we present a stochastic variant of FedFW and show that it finds a\nsolution within $O(\\varepsilon^{-3})$ iterations in the convex setting. We\ndemonstrate the empirical performance of FedFW on several machine learning\ntasks.",
    "arxiv_id": "2408.10090v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10090v1",
    "abstract_url": "http://arxiv.org/abs/2408.10090v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "MASALA: Model-Agnostic Surrogate Explanations by Locality Adaptation",
    "authors": "Saif Anwar, Nathan Griffiths, Abhir Bhalerao, Thomas Popham",
    "abstract": "Existing local Explainable AI (XAI) methods, such as LIME, select a region of\nthe input space in the vicinity of a given input instance, for which they\napproximate the behaviour of a model using a simpler and more interpretable\nsurrogate model. The size of this region is often controlled by a user-defined\nlocality hyperparameter. In this paper, we demonstrate the difficulties\nassociated with defining a suitable locality size to capture impactful model\nbehaviour, as well as the inadequacy of using a single locality size to explain\nall predictions. We propose a novel method, MASALA, for generating\nexplanations, which automatically determines the appropriate local region of\nimpactful model behaviour for each individual instance being explained. MASALA\napproximates the local behaviour used by a complex model to make a prediction\nby fitting a linear surrogate model to a set of points which experience similar\nmodel behaviour. These points are found by clustering the input space into\nregions of linear behavioural trends exhibited by the model. We compare the\nfidelity and consistency of explanations generated by our method with existing\nlocal XAI methods, namely LIME and CHILLI. Experiments on the PHM08 and MIDAS\ndatasets show that our method produces more faithful and consistent\nexplanations than existing methods, without the need to define any sensitive\nlocality hyperparameters.",
    "arxiv_id": "2408.10085v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10085v1",
    "abstract_url": "http://arxiv.org/abs/2408.10085v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "TANGO: Clustering with Typicality-Aware Nonlocal Mode-Seeking and Graph-Cut Optimization",
    "authors": "Haowen Ma, Zhiguo Long, Hua Meng",
    "abstract": "Density-based clustering methods by mode-seeking usually achieve clustering\nby using local density estimation to mine structural information, such as local\ndependencies from lower density points to higher neighbors. However, they often\nrely too heavily on \\emph{local} structures and neglect \\emph{global}\ncharacteristics, which can lead to significant errors in peak selection and\ndependency establishment. Although introducing more hyperparameters that revise\ndependencies can help mitigate this issue, tuning them is challenging and even\nimpossible on real-world datasets. In this paper, we propose a new algorithm\n(TANGO) to establish local dependencies by exploiting a global-view\n\\emph{typicality} of points, which is obtained by mining further the density\ndistributions and initial dependencies. TANGO then obtains sub-clusters with\nthe help of the adjusted dependencies, and characterizes the similarity between\nsub-clusters by incorporating path-based connectivity. It achieves final\nclustering by employing graph-cut on sub-clusters, thus avoiding the\nchallenging selection of cluster centers. Moreover, this paper provides\ntheoretical analysis and an efficient method for the calculation of typicality.\nExperimental results on several synthetic and $16$ real-world datasets\ndemonstrate the effectiveness and superiority of TANGO.",
    "arxiv_id": "2408.10084v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10084v1",
    "abstract_url": "http://arxiv.org/abs/2408.10084v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "No Screening is More Efficient with Multiple Objects",
    "authors": "Shunya Noda, Genta Okada",
    "abstract": "We study efficient mechanism design for allocating multiple heterogeneous\nobjects. We aim to maximize the residual surplus, the total value generated\nfrom an allocation minus the costs for screening agents' values. We discover a\nrobust trend indicating that no-screening mechanisms such as serial\ndictatorship with exogenous priority order tend to perform better as the\nvariety of goods increases. We analyze the underlying reasons by characterizing\nefficient mechanisms in a stylized environment. We also apply an automated\nmechanism design approach to numerically derive efficient mechanisms and\nvalidate the trend in general environments. Building on this implication, we\npropose the register-invite-book system (RIB) as an efficient system for\nscheduling vaccination against pandemic diseases.",
    "arxiv_id": "2408.10077v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10077v1",
    "abstract_url": "http://arxiv.org/abs/2408.10077v1",
    "primary_category": "econ.TH",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning",
    "authors": "Sriyash Poddar, Yanming Wan, Hamish Ivison, Abhishek Gupta, Natasha Jaques",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm for\naligning foundation models to human values and preferences. However, current\nRLHF techniques cannot account for the naturally occurring differences in\nindividual human preferences across a diverse population. When these\ndifferences arise, traditional RLHF frameworks simply average over them,\nleading to inaccurate rewards and poor performance for individual subgroups. To\naddress the need for pluralistic alignment, we develop a class of multimodal\nRLHF methods. Our proposed techniques are based on a latent variable\nformulation - inferring a novel user-specific latent and learning reward models\nand policies conditioned on this latent without additional user-specific data.\nWhile conceptually simple, we show that in practice, this reward modeling\nrequires careful algorithmic considerations around model architecture and\nreward scaling. To empirically validate our proposed technique, we first show\nthat it can provide a way to combat underspecification in simulated control\nproblems, inferring and optimizing user-specific reward functions. Next, we\nconduct experiments on pluralistic language datasets representing diverse user\npreferences and demonstrate improved reward function accuracy. We additionally\nshow the benefits of this probabilistic framework in terms of measuring\nuncertainty, and actively learning user preferences. This work enables learning\nfrom diverse populations of users with divergent preferences, an important\nchallenge that naturally occurs in problems from robot learning to foundation\nmodel alignment.",
    "arxiv_id": "2408.10075v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10075v1",
    "abstract_url": "http://arxiv.org/abs/2408.10075v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Facial Wrinkle Segmentation for Cosmetic Dermatology: Pretraining with Texture Map-Based Weak Supervision",
    "authors": "Junho Moon, Haejun Chung, Ikbeom Jang",
    "abstract": "Facial wrinkle detection plays a crucial role in cosmetic dermatology.\nPrecise manual segmentation of facial wrinkles is challenging and\ntime-consuming, with inherent subjectivity leading to inconsistent results\namong graders. To address this issue, we propose two solutions. First, we build\nand release the first public facial wrinkle dataset, `FFHQ-Wrinkle', an\nextension of the NVIDIA FFHQ dataset. This dataset includes 1,000 images with\nhuman labels and 50,000 images with automatically generated weak labels. This\ndataset can foster the research community to develop advanced wrinkle detection\nalgorithms. Second, we introduce a training strategy for U-Net-like\nencoder-decoder models to detect wrinkles across the face automatically. Our\nmethod employs a two-stage training strategy: texture map pretraining and\nfinetuning on human-labeled data. Initially, we pretrain models on a large\ndataset with weak labels (N=50k) or masked texture maps generated through\ncomputer vision techniques, without human intervention. Subsequently, we\nfinetune the models using human-labeled data (N=1k), which consists of manually\nlabeled wrinkle masks. During finetuning, the network inputs a combination of\nRGB and masked texture maps, comprising four channels. We effectively combine\nlabels from multiple annotators to minimize subjectivity in manual labeling.\nOur strategies demonstrate improved segmentation performance in facial wrinkle\nsegmentation both quantitatively and visually compared to existing pretraining\nmethods.",
    "arxiv_id": "2408.10060v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10060v1",
    "abstract_url": "http://arxiv.org/abs/2408.10060v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Efficient Exploration in Deep Reinforcement Learning: A Novel Bayesian Actor-Critic Algorithm",
    "authors": "Nikolai Rozanov",
    "abstract": "Reinforcement learning (RL) and Deep Reinforcement Learning (DRL), in\nparticular, have the potential to disrupt and are already changing the way we\ninteract with the world. One of the key indicators of their applicability is\ntheir ability to scale and work in real-world scenarios, that is in large-scale\nproblems. This scale can be achieved via a combination of factors, the\nalgorithm's ability to make use of large amounts of data and computational\nresources and the efficient exploration of the environment for viable solutions\n(i.e. policies).\n  In this work, we investigate and motivate some theoretical foundations for\ndeep reinforcement learning. We start with exact dynamic programming and work\nour way up to stochastic approximations and stochastic approximations for a\nmodel-free scenario, which forms the theoretical basis of modern reinforcement\nlearning. We present an overview of this highly varied and rapidly changing\nfield from the perspective of Approximate Dynamic Programming. We then focus\nour study on the short-comings with respect to exploration of the cornerstone\napproaches (i.e. DQN, DDQN, A2C) in deep reinforcement learning. On the theory\nside, our main contribution is the proposal of a novel Bayesian actor-critic\nalgorithm. On the empirical side, we evaluate Bayesian exploration as well as\nactor-critic algorithms on standard benchmarks as well as state-of-the-art\nevaluation suites and show the benefits of both of these approaches over\ncurrent state-of-the-art deep RL methods. We release all the implementations\nand provide a full python library that is easy to install and hopefully will\nserve the reinforcement learning community in a meaningful way, and provide a\nstrong foundation for future work.",
    "arxiv_id": "2408.10055v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10055v1",
    "abstract_url": "http://arxiv.org/abs/2408.10055v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Exploiting Fine-Grained Prototype Distribution for Boosting Unsupervised Class Incremental Learning",
    "authors": "Jiaming Liu, Hongyuan Liu, Zhili Qin, Wei Han, Yulu Fan, Qinli Yang, Junming Shao",
    "abstract": "The dynamic nature of open-world scenarios has attracted more attention to\nclass incremental learning (CIL). However, existing CIL methods typically\npresume the availability of complete ground-truth labels throughout the\ntraining process, an assumption rarely met in practical applications.\nConsequently, this paper explores a more challenging problem of unsupervised\nclass incremental learning (UCIL). The essence of addressing this problem lies\nin effectively capturing comprehensive feature representations and discovering\nunknown novel classes. To achieve this, we first model the knowledge of class\ndistribution by exploiting fine-grained prototypes. Subsequently, a granularity\nalignment technique is introduced to enhance the unsupervised class discovery.\nAdditionally, we proposed a strategy to minimize overlap between novel and\nexisting classes, thereby preserving historical knowledge and mitigating the\nphenomenon of catastrophic forgetting. Extensive experiments on the five\ndatasets demonstrate that our approach significantly outperforms current\nstate-of-the-art methods, indicating the effectiveness of the proposed method.",
    "arxiv_id": "2408.10046v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10046v1",
    "abstract_url": "http://arxiv.org/abs/2408.10046v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "PinnDE: Physics-Informed Neural Networks for Solving Differential Equations",
    "authors": "Jason Matthews, Alex Bihlo",
    "abstract": "In recent years the study of deep learning for solving differential equations\nhas grown substantially. The use of physics-informed neural networks (PINNs)\nand deep operator networks (DeepONets) have emerged as two of the most useful\napproaches in approximating differential equation solutions using machine\nlearning. Here, we propose PinnDE, an open-source python library for solving\ndifferential equations with both PINNs and DeepONets. We give a brief review of\nboth PINNs and DeepONets, introduce PinnDE along with the structure and usage\nof the package, and present worked examples to show PinnDE's effectiveness in\napproximating solutions with both PINNs and DeepONets.",
    "arxiv_id": "2408.10011v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10011v1",
    "abstract_url": "http://arxiv.org/abs/2408.10011v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Unlocking the Power of LSTM for Long Term Time Series Forecasting",
    "authors": "Yaxuan Kong, Zepu Wang, Yuqi Nie, Tian Zhou, Stefan Zohren, Yuxuan Liang, Peng Sun, Qingsong Wen",
    "abstract": "Traditional recurrent neural network architectures, such as long short-term\nmemory neural networks (LSTM), have historically held a prominent role in time\nseries forecasting (TSF) tasks. While the recently introduced sLSTM for Natural\nLanguage Processing (NLP) introduces exponential gating and memory mixing that\nare beneficial for long term sequential learning, its potential short memory\nissue is a barrier to applying sLSTM directly in TSF. To address this, we\npropose a simple yet efficient algorithm named P-sLSTM, which is built upon\nsLSTM by incorporating patching and channel independence. These modifications\nsubstantially enhance sLSTM's performance in TSF, achieving state-of-the-art\nresults. Furthermore, we provide theoretical justifications for our design, and\nconduct extensive comparative and analytical experiments to fully validate the\nefficiency and superior performance of our model.",
    "arxiv_id": "2408.10006v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10006v1",
    "abstract_url": "http://arxiv.org/abs/2408.10006v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "The Fairness-Quality Trade-off in Clustering",
    "authors": "Rashida Hakim, Ana-Andreea Stoica, Christos H. Papadimitriou, Mihalis Yannakakis",
    "abstract": "Fairness in clustering has been considered extensively in the past; however,\nthe trade-off between the two objectives -- e.g., can we sacrifice just a\nlittle in the quality of the clustering to significantly increase fairness, or\nvice-versa? -- has rarely been addressed. We introduce novel algorithms for\ntracing the complete trade-off curve, or Pareto front, between quality and\nfairness in clustering problems; that is, computing all clusterings that are\nnot dominated in both objectives by other clusterings. Unlike previous work\nthat deals with specific objectives for quality and fairness, we deal with all\nobjectives for fairness and quality in two general classes encompassing most of\nthe special cases addressed in previous work. Our algorithm must take\nexponential time in the worst case as the Pareto front itself can be\nexponential. Even when the Pareto front is polynomial, our algorithm may take\nexponential time, and we prove that this is inevitable unless P = NP. However,\nwe also present a new polynomial-time algorithm for computing the entire Pareto\nfront when the cluster centers are fixed, and for perhaps the most natural\nfairness objective: minimizing the sum, over all clusters, of the imbalance\nbetween the two groups in each cluster.",
    "arxiv_id": "2408.10002v1",
    "pdf_url": "http://arxiv.org/pdf/2408.10002v1",
    "abstract_url": "http://arxiv.org/abs/2408.10002v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Uniting contrastive and generative learning for event sequences models",
    "authors": "Aleksandr Yugay, Alexey Zaytsev",
    "abstract": "High-quality representation of transactional sequences is vital for modern\nbanking applications, including risk management, churn prediction, and\npersonalized customer offers. Different tasks require distinct representation\nproperties: local tasks benefit from capturing the client's current state,\nwhile global tasks rely on general behavioral patterns. Previous research has\ndemonstrated that various self-supervised approaches yield representations that\nbetter capture either global or local qualities.\n  This study investigates the integration of two self-supervised learning\ntechniques - instance-wise contrastive learning and a generative approach based\non restoring masked events in latent space. The combined approach creates\nrepresentations that balance local and global transactional data\ncharacteristics. Experiments conducted on several public datasets, focusing on\nsequence classification and next-event type prediction, show that the\nintegrated method achieves superior performance compared to individual\napproaches and demonstrates synergistic effects. These findings suggest that\nthe proposed approach offers a robust framework for advancing event sequences\nrepresentation learning in the financial sector.",
    "arxiv_id": "2408.09995v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09995v1",
    "abstract_url": "http://arxiv.org/abs/2408.09995v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Parseval Convolution Operators and Neural Networks",
    "authors": "Michael Unser, Stanislas Ducotterd",
    "abstract": "We first establish a kernel theorem that characterizes all linear\nshift-invariant (LSI) operators acting on discrete multicomponent signals. This\nresult naturally leads to the identification of the Parseval convolution\noperators as the class of energy-preserving filterbanks. We then present a\nconstructive approach for the design/specification of such filterbanks via the\nchaining of elementary Parseval modules, each of which being parameterized by\nan orthogonal matrix or a 1-tight frame. Our analysis is complemented with\nexplicit formulas for the Lipschitz constant of all the components of a\nconvolutional neural network (CNN), which gives us a handle on their stability.\nFinally, we demonstrate the usage of those tools with the design of a CNN-based\nalgorithm for the iterative reconstruction of biomedical images. Our algorithm\nfalls within the plug-and-play framework for the resolution of inverse\nproblems. It yields better-quality results than the sparsity-based methods used\nin compressed sensing, while offering essentially the same convergence and\nrobustness guarantees.",
    "arxiv_id": "2408.09981v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09981v1",
    "abstract_url": "http://arxiv.org/abs/2408.09981v1",
    "primary_category": "eess.SP",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Preference-Optimized Pareto Set Learning for Blackbox Optimization",
    "authors": "Zhang Haishan, Diptesh Das, Koji Tsuda",
    "abstract": "Multi-Objective Optimization (MOO) is an important problem in real-world\napplications. However, for a non-trivial problem, no single solution exists\nthat can optimize all the objectives simultaneously. In a typical MOO problem,\nthe goal is to find a set of optimum solutions (Pareto set) that trades off the\npreferences among objectives. Scalarization in MOO is a well-established method\nfor finding a finite set approximation of the whole Pareto set (PS). However,\nin real-world experimental design scenarios, it's beneficial to obtain the\nwhole PS for flexible exploration of the design space. Recently Pareto set\nlearning (PSL) has been introduced to approximate the whole PS. PSL involves\ncreating a manifold representing the Pareto front of a multi-objective\noptimization problem. A naive approach includes finding discrete points on the\nPareto front through randomly generated preference vectors and connecting them\nby regression. However, this approach is computationally expensive and leads to\na poor PS approximation. We propose to optimize the preference points to be\ndistributed evenly on the Pareto front. Our formulation leads to a bilevel\noptimization problem that can be solved by e.g. differentiable cross-entropy\nmethods. We demonstrated the efficacy of our method for complex and difficult\nblack-box MOO problems using both synthetic and real-world benchmark data.",
    "arxiv_id": "2408.09976v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09976v1",
    "abstract_url": "http://arxiv.org/abs/2408.09976v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "The Exploration-Exploitation Dilemma Revisited: An Entropy Perspective",
    "authors": "Renye Yan, Yaozhong Gan, You Wu, Ling Liang, Junliang Xing, Yimao Cai, Ru Huang",
    "abstract": "The imbalance of exploration and exploitation has long been a significant\nchallenge in reinforcement learning. In policy optimization, excessive reliance\non exploration reduces learning efficiency, while over-dependence on\nexploitation might trap agents in local optima. This paper revisits the\nexploration-exploitation dilemma from the perspective of entropy by revealing\nthe relationship between entropy and the dynamic adaptive process of\nexploration and exploitation. Based on this theoretical insight, we establish\nan end-to-end adaptive framework called AdaZero, which automatically determines\nwhether to explore or to exploit as well as their balance of strength.\nExperiments show that AdaZero significantly outperforms baseline models across\nvarious Atari and MuJoCo environments with only a single setting. Especially in\nthe challenging environment of Montezuma, AdaZero boosts the final returns by\nup to fifteen times. Moreover, we conduct a series of visualization analyses to\nreveal the dynamics of our self-adaptive mechanism, demonstrating how entropy\nreflects and changes with respect to the agent's performance and adaptive\nprocess.",
    "arxiv_id": "2408.09974v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09974v1",
    "abstract_url": "http://arxiv.org/abs/2408.09974v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Unsupervised Machine Learning Hybrid Approach Integrating Linear Programming in Loss Function: A Robust Optimization Technique",
    "authors": "Andrew Kiruluta, Andreas Lemos",
    "abstract": "This paper presents a novel hybrid approach that integrates linear\nprogramming (LP) within the loss function of an unsupervised machine learning\nmodel. By leveraging the strengths of both optimization techniques and machine\nlearning, this method introduces a robust framework for solving complex\noptimization problems where traditional methods may fall short. The proposed\napproach encapsulates the constraints and objectives of a linear programming\nproblem directly into the loss function, guiding the learning process to adhere\nto these constraints while optimizing the desired outcomes. This technique not\nonly preserves the interpretability of linear programming but also benefits\nfrom the flexibility and adaptability of machine learning, making it\nparticularly well-suited for unsupervised or semi-supervised learning\nscenarios.",
    "arxiv_id": "2408.09967v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09967v1",
    "abstract_url": "http://arxiv.org/abs/2408.09967v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Mask in the Mirror: Implicit Sparsification",
    "authors": "Tom Jacobs, Rebekka Burkholz",
    "abstract": "Sparsifying deep neural networks to reduce their inference cost is an NP-hard\nproblem and difficult to optimize due to its mixed discrete and continuous\nnature. Yet, as we prove, continuous sparsification has already an implicit\nbias towards sparsity that would not require common projections of relaxed mask\nvariables. While implicit rather than explicit regularization induces benefits,\nit usually does not provide enough flexibility in practice, as only a specific\ntarget sparsity is obtainable. To exploit its potential for continuous\nsparsification, we propose a way to control the strength of the implicit bias.\nBased on the mirror flow framework, we derive resulting convergence and\noptimality guarantees in the context of underdetermined linear regression and\ndemonstrate the utility of our insights in more general neural network\nsparsification experiments, achieving significant performance gains,\nparticularly in the high-sparsity regime. Our theoretical contribution might be\nof independent interest, as we highlight a way to enter the rich regime and\nshow that implicit bias is controllable by a time-dependent Bregman potential.",
    "arxiv_id": "2408.09966v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09966v1",
    "abstract_url": "http://arxiv.org/abs/2408.09966v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "AdaResNet: Enhancing Residual Networks with Dynamic Weight Adjustment for Improved Feature Integration",
    "authors": "Hong Su",
    "abstract": "In very deep neural networks, gradients can become extremely small during\nbackpropagation, making it challenging to train the early layers. ResNet\n(Residual Network) addresses this issue by enabling gradients to flow directly\nthrough the network via skip connections, facilitating the training of much\ndeeper networks. However, in these skip connections, the input ipd is directly\nadded to the transformed data tfd, treating ipd and tfd equally, without\nadapting to different scenarios. In this paper, we propose AdaResNet\n(Auto-Adapting Residual Network), which automatically adjusts the ratio between\nipd and tfd based on the training data. We introduce a variable,\nweight}_{tfd}^{ipd, to represent this ratio. This variable is dynamically\nadjusted during backpropagation, allowing it to adapt to the training data\nrather than remaining fixed. Experimental results demonstrate that AdaResNet\nachieves a maximum accuracy improvement of over 50\\% compared to traditional\nResNet.",
    "arxiv_id": "2408.09958v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09958v1",
    "abstract_url": "http://arxiv.org/abs/2408.09958v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Weakly Supervised Pretraining and Multi-Annotator Supervised Finetuning for Facial Wrinkle Detection",
    "authors": "Ik Jun Moon, Junho Moon, Ikbeom Jang",
    "abstract": "1. Research question: With the growing interest in skin diseases and skin\naesthetics, the ability to predict facial wrinkles is becoming increasingly\nimportant. This study aims to evaluate whether a computational model,\nconvolutional neural networks (CNN), can be trained for automated facial\nwrinkle segmentation. 2. Findings: Our study presents an effective technique\nfor integrating data from multiple annotators and illustrates that transfer\nlearning can enhance performance, resulting in dependable segmentation of\nfacial wrinkles. 3. Meaning: This approach automates intricate and\ntime-consuming tasks of wrinkle analysis with a deep learning framework. It\ncould be used to facilitate skin treatments and diagnostics.",
    "arxiv_id": "2408.09952v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09952v1",
    "abstract_url": "http://arxiv.org/abs/2408.09952v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "The curse of random quantum data",
    "authors": "Kaining Zhang, Junyu Liu, Liu Liu, Liang Jiang, Min-Hsiu Hsieh, Dacheng Tao",
    "abstract": "Quantum machine learning, which involves running machine learning algorithms\non quantum devices, may be one of the most significant flagship applications\nfor these devices. Unlike its classical counterparts, the role of data in\nquantum machine learning has not been fully understood. In this work, we\nquantify the performances of quantum machine learning in the landscape of\nquantum data. Provided that the encoding of quantum data is sufficiently\nrandom, the performance, we find that the training efficiency and\ngeneralization capabilities in quantum machine learning will be exponentially\nsuppressed with the increase in the number of qubits, which we call \"the curse\nof random quantum data\". Our findings apply to both the quantum kernel method\nand the large-width limit of quantum neural networks. Conversely, we highlight\nthat through meticulous design of quantum datasets, it is possible to avoid\nthese curses, thereby achieving efficient convergence and robust\ngeneralization. Our conclusions are corroborated by extensive numerical\nsimulations.",
    "arxiv_id": "2408.09937v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09937v1",
    "abstract_url": "http://arxiv.org/abs/2408.09937v1",
    "primary_category": "quant-ph",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Electron-nucleus cross sections from transfer learning",
    "authors": "Krzysztof M. Graczyk, Beata E. Kowal, Artur M. Ankowski, Rwik Dharmapal Banerjee, Jose Luis Bonilla, Hemant Prasad, Jan T. Sobczyk",
    "abstract": "Transfer learning (TL) allows a deep neural network (DNN) trained on one type\nof data to be adapted for new problems with limited information. We propose to\nuse the TL technique in physics. The DNN learns the physics of one process, and\nafter fine-tuning, it makes predictions for related processes. We consider the\nDNNs, trained on inclusive electron-carbon scattering data, and show that after\nfine-tuning, they accurately predict cross sections for electron interactions\nwith nuclear targets ranging from lithium to iron. The method works even when\nthe DNN is fine-tuned on a small dataset.",
    "arxiv_id": "2408.09936v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09936v1",
    "abstract_url": "http://arxiv.org/abs/2408.09936v1",
    "primary_category": "hep-ph",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Data Augmentation of Contrastive Learning is Estimating Positive-incentive Noise",
    "authors": "Hongyuan Zhang, Yanchen Xu, Sida Huang, Xuelong Li",
    "abstract": "Inspired by the idea of Positive-incentive Noise (Pi-Noise or $\\pi$-Noise)\nthat aims at learning the reliable noise beneficial to tasks, we scientifically\ninvestigate the connection between contrastive learning and $\\pi$-noise in this\npaper. By converting the contrastive loss to an auxiliary Gaussian distribution\nto quantitatively measure the difficulty of the specific contrastive model\nunder the information theory framework, we properly define the task entropy,\nthe core concept of $\\pi$-noise, of contrastive learning. It is further proved\nthat the predefined data augmentation in the standard contrastive learning\nparadigm can be regarded as a kind of point estimation of $\\pi$-noise. Inspired\nby the theoretical study, a framework that develops a $\\pi$-noise generator to\nlearn the beneficial noise (instead of estimation) as data augmentations for\ncontrast is proposed. The designed framework can be applied to diverse types of\ndata and is also completely compatible with the existing contrastive models.\nFrom the visualization, we surprisingly find that the proposed method\nsuccessfully learns effective augmentations.",
    "arxiv_id": "2408.09929v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09929v1",
    "abstract_url": "http://arxiv.org/abs/2408.09929v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Expressive Power of Temporal Message Passing",
    "authors": "Przemys\u0142aw Andrzej Wa\u0142\u0119ga, Michael Rawson",
    "abstract": "Graph neural networks (GNNs) have recently been adapted to temporal settings,\noften employing temporal versions of the message-passing mechanism known from\nGNNs. We divide temporal message passing mechanisms from literature into two\nmain types: global and local, and establish Weisfeiler-Leman characterisations\nfor both. This allows us to formally analyse expressive power of temporal\nmessage-passing models. We show that global and local temporal message-passing\nmechanisms have incomparable expressive power when applied to arbitrary\ntemporal graphs. However, the local mechanism is strictly more expressive than\nthe global mechanism when applied to colour-persistent temporal graphs, whose\nnode colours are initially the same in all time points. Our theoretical\nfindings are supported by experimental evidence, underlining practical\nimplications of our analysis.",
    "arxiv_id": "2408.09918v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09918v1",
    "abstract_url": "http://arxiv.org/abs/2408.09918v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Active Learning for Identifying Disaster-Related Tweets: A Comparison with Keyword Filtering and Generic Fine-Tuning",
    "authors": "David Hanny, Sebastian Schmidt, Bernd Resch",
    "abstract": "Information from social media can provide essential information for emergency\nresponse during natural disasters in near real-time. However, it is difficult\nto identify the disaster-related posts among the large amounts of unstructured\ndata available. Previous methods often use keyword filtering, topic modelling\nor classification-based techniques to identify such posts. Active Learning (AL)\npresents a promising sub-field of Machine Learning (ML) that has not been used\nmuch in the field of text classification of social media content. This study\ntherefore investigates the potential of AL for identifying disaster-related\nTweets. We compare a keyword filtering approach, a RoBERTa model fine-tuned\nwith generic data from CrisisLex, a base RoBERTa model trained with AL and a\nfine-tuned RoBERTa model trained with AL regarding classification performance.\nFor testing, data from CrisisLex and manually labelled data from the 2021 flood\nin Germany and the 2023 Chile forest fires were considered. The results show\nthat generic fine-tuning combined with 10 rounds of AL outperformed all other\napproaches. Consequently, a broadly applicable model for the identification of\ndisaster-related Tweets could be trained with very little labelling effort. The\nmodel can be applied to use cases beyond this study and provides a useful tool\nfor further research in social media analysis.",
    "arxiv_id": "2408.09914v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09914v1",
    "abstract_url": "http://arxiv.org/abs/2408.09914v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "$p$SVM: Soft-margin SVMs with $p$-norm Hinge Loss",
    "authors": "Haoxiang Sun",
    "abstract": "Support Vector Machines (SVMs) based on hinge loss have been extensively\ndiscussed and applied to various binary classification tasks. These SVMs\nachieve a balance between margin maximization and the minimization of slack due\nto outliers. Although many efforts have been dedicated to enhancing the\nperformance of SVMs with hinge loss, studies on $p$SVMs, soft-margin SVMs with\n$p$-norm hinge loss, remain relatively scarce. In this paper, we explore the\nproperties, performance, and training algorithms of $p$SVMs. We first derive\nthe generalization bound of $p$SVMs, then formulate the dual optimization\nproblem, comparing it with the traditional approach. Furthermore, we discuss a\ngeneralized version of the Sequential Minimal Optimization (SMO) algorithm,\n$p$SMO, to train our $p$SVM model. Comparative experiments on various datasets,\nincluding binary and multi-class classification tasks, demonstrate the\neffectiveness and advantages of our $p$SVM model and the $p$SMO method.",
    "arxiv_id": "2408.09908v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09908v1",
    "abstract_url": "http://arxiv.org/abs/2408.09908v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Instruction-Based Molecular Graph Generation with Unified Text-Graph Diffusion Model",
    "authors": "Yuran Xiang, Haiteng Zhao, Chang Ma, Zhi-Hong Deng",
    "abstract": "Recent advancements in computational chemistry have increasingly focused on\nsynthesizing molecules based on textual instructions. Integrating graph\ngeneration with these instructions is complex, leading most current methods to\nuse molecular sequences with pre-trained large language models. In response to\nthis challenge, we propose a novel framework, named $\\textbf{UTGDiff (Unified\nText-Graph Diffusion Model)}$, which utilizes language models for discrete\ngraph diffusion to generate molecular graphs from instructions. UTGDiff\nfeatures a unified text-graph transformer as the denoising network, derived\nfrom pre-trained language models and minimally modified to process graph data\nthrough attention bias. Our experimental results demonstrate that UTGDiff\nconsistently outperforms sequence-based baselines in tasks involving\ninstruction-based molecule generation and editing, achieving superior\nperformance with fewer parameters given an equivalent level of pretraining\ncorpus. Our code is availble at https://github.com/ran1812/UTGDiff.",
    "arxiv_id": "2408.09896v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09896v1",
    "abstract_url": "http://arxiv.org/abs/2408.09896v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Performance Law of Large Language Models",
    "authors": "Chuhan Wu, Ruiming Tang",
    "abstract": "Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments.",
    "arxiv_id": "2408.09895v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09895v1",
    "abstract_url": "http://arxiv.org/abs/2408.09895v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Differential Private Stochastic Optimization with Heavy-tailed Data: Towards Optimal Rates",
    "authors": "Puning Zhao, Jiafei Wu, Zhe Liu, Chong Wang, Rongfei Fan, Qingming Li",
    "abstract": "We study convex optimization problems under differential privacy (DP). With\nheavy-tailed gradients, existing works achieve suboptimal rates. The main\nobstacle is that existing gradient estimators have suboptimal tail properties,\nresulting in a superfluous factor of $d$ in the union bound. In this paper, we\nexplore algorithms achieving optimal rates of DP optimization with heavy-tailed\ngradients. Our first method is a simple clipping approach. Under bounded $p$-th\norder moments of gradients, with $n$ samples, it achieves\n$\\tilde{O}(\\sqrt{d/n}+\\sqrt{d}(\\sqrt{d}/n\\epsilon)^{1-1/p})$ population risk\nwith $\\epsilon\\leq 1/\\sqrt{d}$. We then propose an iterative updating method,\nwhich is more complex but achieves this rate for all $\\epsilon\\leq 1$. The\nresults significantly improve over existing methods. Such improvement relies on\na careful treatment of the tail behavior of gradient estimators. Our results\nmatch the minimax lower bound in \\cite{kamath2022improved}, indicating that the\ntheoretical limit of stochastic convex optimization under DP is achievable.",
    "arxiv_id": "2408.09891v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09891v1",
    "abstract_url": "http://arxiv.org/abs/2408.09891v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "GINO-Q: Learning an Asymptotically Optimal Index Policy for Restless Multi-armed Bandits",
    "authors": "Gongpu Chen, Soung Chang Liew, Deniz Gunduz",
    "abstract": "The restless multi-armed bandit (RMAB) framework is a popular model with\napplications across a wide variety of fields. However, its solution is hindered\nby the exponentially growing state space (with respect to the number of arms)\nand the combinatorial action space, making traditional reinforcement learning\nmethods infeasible for large-scale instances. In this paper, we propose GINO-Q,\na three-timescale stochastic approximation algorithm designed to learn an\nasymptotically optimal index policy for RMABs. GINO-Q mitigates the curse of\ndimensionality by decomposing the RMAB into a series of subproblems, each with\nthe same dimension as a single arm, ensuring that complexity increases linearly\nwith the number of arms. Unlike recently developed Whittle-index-based\nalgorithms, GINO-Q does not require RMABs to be indexable, enhancing its\nflexibility and applicability. Our experimental results demonstrate that GINO-Q\nconsistently learns near-optimal policies, even for non-indexable RMABs where\nWhittle-index-based algorithms perform poorly, and it converges significantly\nfaster than existing baselines.",
    "arxiv_id": "2408.09882v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09882v1",
    "abstract_url": "http://arxiv.org/abs/2408.09882v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "New spectral imaging biomarkers for sepsis and mortality in intensive care",
    "authors": "Silvia Seidlitz, Katharina H\u00f6lzl, Ayca von Garrel, Jan Sellner, Stephan Katzenschlager, Tobias H\u00f6lle, Dania Fischer, Maik von der Forst, Felix C. F. Schmitt, Markus A. Weigand, Lena Maier-Hein, Maximilian Dietrich",
    "abstract": "With sepsis remaining a leading cause of mortality, early identification of\nseptic patients and those at high risk of death is a challenge of high\nsocioeconomic importance. The driving hypothesis of this study was that\nhyperspectral imaging (HSI) could provide novel biomarkers for sepsis diagnosis\nand treatment management due to its potential to monitor microcirculatory\nalterations. We conducted a comprehensive study involving HSI data of the palm\nand fingers from more than 480 patients on the day of their intensive care unit\n(ICU) admission. The findings demonstrate that HSI measurements can predict\nsepsis with an area under the receiver operating characteristic curve (AUROC)\nof 0.80 (95 % confidence interval (CI) [0.76; 0.84]) and mortality with an\nAUROC of 0.72 (95 % CI [0.65; 0.79]). The predictive performance improves\nsubstantially when additional clinical data is incorporated, leading to an\nAUROC of up to 0.94 (95 % CI [0.92; 0.96]) for sepsis and 0.84 (95 % CI [0.78;\n0.89]) for mortality. We conclude that HSI presents novel imaging biomarkers\nfor the rapid, non-invasive prediction of sepsis and mortality, suggesting its\npotential as an important modality for guiding diagnosis and treatment.",
    "arxiv_id": "2408.09873v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09873v1",
    "abstract_url": "http://arxiv.org/abs/2408.09873v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in Explainable Recommendation",
    "authors": "Ching-Wen Yang, Che Wei Chen, Kun-da Wu, Hao Xu, Jui-Feng Yao, Hung-Yu Kao",
    "abstract": "Explainable Recommendation task is designed to receive a pair of user and\nitem and output explanations to justify why an item is recommended to a user.\nMany models treat review-generation as a proxy of explainable recommendation.\nAlthough they are able to generate fluent and grammatical sentences, they\nsuffer from generality and hallucination issues. We propose a personalized,\naspect-controlled model called Multi-Aspect Prompt LEarner (MAPLE), in which it\nintegrates aspect category as another input dimension to facilitate the\nmemorization of fine-grained aspect terms. Experiments on two real-world review\ndatasets in restaurant domain show that MAPLE outperforms the baseline\nreview-generation models in terms of text and feature diversity while\nmaintaining excellent coherence and factual relevance. We further treat MAPLE\nas a retriever component in the retriever-reader framework and employ a\nLarge-Language Model (LLM) as the reader, showing that MAPLE's explanation\nalong with the LLM's comprehension ability leads to enriched and personalized\nexplanation as a result. We will release the code and data in this http upon\nacceptance.",
    "arxiv_id": "2408.09865v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09865v1",
    "abstract_url": "http://arxiv.org/abs/2408.09865v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "3D-Aware Instance Segmentation and Tracking in Egocentric Videos",
    "authors": "Yash Bhalgat, Vadim Tschernezki, Iro Laina, Jo\u00e3o F. Henriques, Andrea Vedaldi, Andrew Zisserman",
    "abstract": "Egocentric videos present unique challenges for 3D scene understanding due to\nrapid camera motion, frequent object occlusions, and limited object visibility.\nThis paper introduces a novel approach to instance segmentation and tracking in\nfirst-person video that leverages 3D awareness to overcome these obstacles. Our\nmethod integrates scene geometry, 3D object centroid tracking, and instance\nsegmentation to create a robust framework for analyzing dynamic egocentric\nscenes. By incorporating spatial and temporal cues, we achieve superior\nperformance compared to state-of-the-art 2D approaches. Extensive evaluations\non the challenging EPIC Fields dataset demonstrate significant improvements\nacross a range of tracking and segmentation consistency metrics. Specifically,\nour method outperforms the next best performing approach by $7$ points in\nAssociation Accuracy (AssA) and $4.5$ points in IDF1 score, while reducing the\nnumber of ID switches by $73\\%$ to $80\\%$ across various object categories.\nLeveraging our tracked instance segmentations, we showcase downstream\napplications in 3D object reconstruction and amodal video object segmentation\nin these egocentric settings.",
    "arxiv_id": "2408.09860v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09860v1",
    "abstract_url": "http://arxiv.org/abs/2408.09860v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "ShortCircuit: AlphaZero-Driven Circuit Design",
    "authors": "Dimitrios Tsaras, Antoine Grosnit, Lei Chen, Zhiyao Xie, Haitham Bou-Ammar, Mingxuan Yuan",
    "abstract": "Chip design relies heavily on generating Boolean circuits, such as\nAND-Inverter Graphs (AIGs), from functional descriptions like truth tables.\nWhile recent advances in deep learning have aimed to accelerate circuit design,\nthese efforts have mostly focused on tasks other than synthesis, and\ntraditional heuristic methods have plateaued. In this paper, we introduce\nShortCircuit, a novel transformer-based architecture that leverages the\nstructural properties of AIGs and performs efficient space exploration.\nContrary to prior approaches attempting end-to-end generation of logic circuits\nusing deep networks, ShortCircuit employs a two-phase process combining\nsupervised with reinforcement learning to enhance generalization to unseen\ntruth tables. We also propose an AlphaZero variant to handle the double\nexponentially large state space and the sparsity of the rewards, enabling the\ndiscovery of near-optimal designs. To evaluate the generative performance of\nour trained model , we extract 500 truth tables from a benchmark set of 20\nreal-world circuits. ShortCircuit successfully generates AIGs for 84.6% of the\n8-input test truth tables, and outperforms the state-of-the-art logic synthesis\ntool, ABC, by 14.61% in terms of circuits size.",
    "arxiv_id": "2408.09858v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09858v1",
    "abstract_url": "http://arxiv.org/abs/2408.09858v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Machine Learning with Physics Knowledge for Prediction: A Survey",
    "authors": "Joe Watson, Chen Song, Oliver Weeger, Theo Gruner, An T. Le, Kay Hansel, Ahmed Hendawy, Oleg Arenz, Will Trojak, Miles Cranmer, Carlo D'Eramo, Fabian B\u00fclow, Tanmay Goyal, Jan Peters, Martin W. Hoffman",
    "abstract": "This survey examines the broad suite of methods and models for combining\nmachine learning with physics knowledge for prediction and forecast, with a\nfocus on partial differential equations. These methods have attracted\nsignificant interest due to their potential impact on advancing scientific\nresearch and industrial practices by improving predictive models with small- or\nlarge-scale datasets and expressive predictive models with useful inductive\nbiases. The survey has two parts. The first considers incorporating physics\nknowledge on an architectural level through objective functions, structured\npredictive models, and data augmentation. The second considers data as physics\nknowledge, which motivates looking at multi-task, meta, and contextual learning\nas an alternative approach to incorporating physics knowledge in a data-driven\nfashion. Finally, we also provide an industrial perspective on the application\nof these methods and a survey of the open-source ecosystem for physics-informed\nmachine learning.",
    "arxiv_id": "2408.09840v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09840v1",
    "abstract_url": "http://arxiv.org/abs/2408.09840v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Mitigating the Stability-Plasticity Dilemma in Adaptive Train Scheduling with Curriculum-Driven Continual DQN Expansion",
    "authors": "Achref Jaziri, Etienne K\u00fcnzel, Visvanathan Ramesh",
    "abstract": "A continual learning agent builds on previous experiences to develop\nincreasingly complex behaviors by adapting to non-stationary and dynamic\nenvironments while preserving previously acquired knowledge. However, scaling\nthese systems presents significant challenges, particularly in balancing the\npreservation of previous policies with the adaptation of new ones to current\nenvironments. This balance, known as the stability-plasticity dilemma, is\nespecially pronounced in complex multi-agent domains such as the train\nscheduling problem, where environmental and agent behaviors are constantly\nchanging, and the search space is vast. In this work, we propose addressing\nthese challenges in the train scheduling problem using curriculum learning. We\ndesign a curriculum with adjacent skills that build on each other to improve\ngeneralization performance. Introducing a curriculum with distinct tasks\nintroduces non-stationarity, which we address by proposing a new algorithm:\nContinual Deep Q-Network (DQN) Expansion (CDE). Our approach dynamically\ngenerates and adjusts Q-function subspaces to handle environmental changes and\ntask requirements. CDE mitigates catastrophic forgetting through EWC while\nensuring high plasticity using adaptive rational activation functions.\nExperimental results demonstrate significant improvements in learning\nefficiency and adaptability compared to RL baselines and other adapted methods\nfor continual learning, highlighting the potential of our method in managing\nthe stability-plasticity dilemma in the adaptive train scheduling setting.",
    "arxiv_id": "2408.09838v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09838v1",
    "abstract_url": "http://arxiv.org/abs/2408.09838v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Symplectic Neural Networks Based on Dynamical Systems",
    "authors": "Benjamin K Tapley",
    "abstract": "We present and analyze a framework for designing symplectic neural networks\n(SympNets) based on geometric integrators for Hamiltonian differential\nequations. The SympNets are universal approximators in the space of Hamiltonian\ndiffeomorphisms, interpretable and have a non-vanishing gradient property. We\nalso give a representation theory for linear systems, meaning the proposed\nP-SympNets can exactly parameterize any symplectic map corresponding to\nquadratic Hamiltonians. Extensive numerical tests demonstrate increased\nexpressiveness and accuracy -- often several orders of magnitude better -- for\nlower training cost over existing architectures. Lastly, we show how to perform\nsymbolic Hamiltonian regression with SympNets for polynomial systems using\nbackward error analysis.",
    "arxiv_id": "2408.09821v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09821v1",
    "abstract_url": "http://arxiv.org/abs/2408.09821v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Liquid Fourier Latent Dynamics Networks for fast GPU-based numerical simulations in computational cardiology",
    "authors": "Matteo Salvador, Alison L. Marsden",
    "abstract": "Scientific Machine Learning (ML) is gaining momentum as a cost-effective\nalternative to physics-based numerical solvers in many engineering\napplications. In fact, scientific ML is currently being used to build accurate\nand efficient surrogate models starting from high-fidelity numerical\nsimulations, effectively encoding the parameterized temporal dynamics\nunderlying Ordinary Differential Equations (ODEs), or even the spatio-temporal\nbehavior underlying Partial Differential Equations (PDEs), in appropriately\ndesigned neural networks. We propose an extension of Latent Dynamics Networks\n(LDNets), namely Liquid Fourier LDNets (LFLDNets), to create parameterized\nspace-time surrogate models for multiscale and multiphysics sets of highly\nnonlinear differential equations on complex geometries. LFLDNets employ a\nneurologically-inspired, sparse, liquid neural network for temporal dynamics,\nrelaxing the requirement of a numerical solver for time advancement and leading\nto superior performance in terms of tunable parameters, accuracy, efficiency\nand learned trajectories with respect to neural ODEs based on feedforward\nfully-connected neural networks. Furthermore, in our implementation of\nLFLDNets, we use a Fourier embedding with a tunable kernel in the\nreconstruction network to learn high-frequency functions better and faster than\nusing space coordinates directly as input. We challenge LFLDNets in the\nframework of computational cardiology and evaluate their capabilities on two\n3-dimensional test cases arising from multiscale cardiac electrophysiology and\ncardiovascular hemodynamics. This paper illustrates the capability to run\nArtificial Intelligence-based numerical simulations on single or multiple GPUs\nin a matter of minutes and represents a significant step forward in the\ndevelopment of physics-informed digital twins.",
    "arxiv_id": "2408.09818v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09818v1",
    "abstract_url": "http://arxiv.org/abs/2408.09818v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "A Population-to-individual Tuning Framework for Adapting Pretrained LM to On-device User Intent Prediction",
    "authors": "Jiahui Gong, Jingtao Ding, Fanjin Meng, Guilong Chen, Hong Chen, Shen Zhao, Haisheng Lu, Yong Li",
    "abstract": "Mobile devices, especially smartphones, can support rich functions and have\ndeveloped into indispensable tools in daily life. With the rise of generative\nAI services, smartphones can potentially transform into personalized\nassistants, anticipating user needs and scheduling services accordingly.\nPredicting user intents on smartphones, and reflecting anticipated activities\nbased on past interactions and context, remains a pivotal step towards this\nvision. Existing research predominantly focuses on specific domains, neglecting\nthe challenge of modeling diverse event sequences across dynamic contexts.\nLeveraging pre-trained language models (PLMs) offers a promising avenue, yet\nadapting PLMs to on-device user intent prediction presents significant\nchallenges. To address these challenges, we propose PITuning, a\nPopulation-to-Individual Tuning framework. PITuning enhances common pattern\nextraction through dynamic event-to-intent transition modeling and addresses\nlong-tailed preferences via adaptive unlearning strategies. Experimental\nresults on real-world datasets demonstrate PITuning's superior intent\nprediction performance, highlighting its ability to capture long-tailed\npreferences and its practicality for on-device prediction scenarios.",
    "arxiv_id": "2408.09815v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09815v1",
    "abstract_url": "http://arxiv.org/abs/2408.09815v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Enhance Modality Robustness in Text-Centric Multimodal Alignment with Adversarial Prompting",
    "authors": "Yun-Da Tsai, Ting-Yu Yen, Keng-Te Liao, Shou-De Lin",
    "abstract": "Converting different modalities into generalized text, which then serves as\ninput prompts for large language models (LLMs), is a common approach for\naligning multimodal models, particularly when pairwise data is limited.\nText-centric alignment method leverages the unique properties of text as a\nmodality space, transforming diverse inputs into a unified textual\nrepresentation, thereby enabling downstream models to effectively interpret\nvarious modal inputs. This study evaluates the quality and robustness of\nmultimodal representations in the face of noise imperfections, dynamic input\norder permutations, and missing modalities, revealing that current text-centric\nalignment methods can compromise downstream robustness. To address this issue,\nwe propose a new text-centric adversarial training approach that significantly\nenhances robustness compared to traditional robust training methods and\npre-trained multimodal foundation models. Our findings underscore the potential\nof this approach to improve the robustness and adaptability of multimodal\nrepresentations, offering a promising solution for dynamic and real-world\napplications.",
    "arxiv_id": "2408.09798v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09798v1",
    "abstract_url": "http://arxiv.org/abs/2408.09798v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Unsupervised Composable Representations for Audio",
    "authors": "Giovanni Bindi, Philippe Esling",
    "abstract": "Current generative models are able to generate high-quality artefacts but\nhave been shown to struggle with compositional reasoning, which can be defined\nas the ability to generate complex structures from simpler elements. In this\npaper, we focus on the problem of compositional representation learning for\nmusic data, specifically targeting the fully-unsupervised setting. We propose a\nsimple and extensible framework that leverages an explicit compositional\ninductive bias, defined by a flexible auto-encoding objective that can leverage\nany of the current state-of-art generative models. We demonstrate that our\nframework, used with diffusion models, naturally addresses the task of\nunsupervised audio source separation, showing that our model is able to perform\nhigh-quality separation. Our findings reveal that our proposal achieves\ncomparable or superior performance with respect to other blind source\nseparation methods and, furthermore, it even surpasses current state-of-art\nsupervised baselines on signal-to-interference ratio metrics. Additionally, by\nlearning an a-posteriori masking diffusion model in the space of composable\nrepresentations, we achieve a system capable of seamlessly performing\nunsupervised source separation, unconditional generation, and variation\ngeneration. Finally, as our proposal works in the latent space of pre-trained\nneural audio codecs, it also provides a lower computational cost with respect\nto other neural baselines.",
    "arxiv_id": "2408.09792v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09792v1",
    "abstract_url": "http://arxiv.org/abs/2408.09792v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "ALTBI: Constructing Improved Outlier Detection Models via Optimization of Inlier-Memorization Effect",
    "authors": "Seoyoung Cho, Jaesung Hwang, Kwan-Young Bak, Dongha Kim",
    "abstract": "Outlier detection (OD) is the task of identifying unusual observations (or\noutliers) from a given or upcoming data by learning unique patterns of normal\nobservations (or inliers). Recently, a study introduced a powerful unsupervised\nOD (UOD) solver based on a new observation of deep generative models, called\ninlier-memorization (IM) effect, which suggests that generative models memorize\ninliers before outliers in early learning stages. In this study, we aim to\ndevelop a theoretically principled method to address UOD tasks by maximally\nutilizing the IM effect. We begin by observing that the IM effect is observed\nmore clearly when the given training data contain fewer outliers. This finding\nindicates a potential for enhancing the IM effect in UOD regimes if we can\neffectively exclude outliers from mini-batches when designing the loss\nfunction. To this end, we introduce two main techniques: 1) increasing the\nmini-batch size as the model training proceeds and 2) using an adaptive\nthreshold to calculate the truncated loss function. We theoretically show that\nthese two techniques effectively filter out outliers from the truncated loss\nfunction, allowing us to utilize the IM effect to the fullest. Coupled with an\nadditional ensemble strategy, we propose our method and term it Adaptive Loss\nTruncation with Batch Increment (ALTBI). We provide extensive experimental\nresults to demonstrate that ALTBI achieves state-of-the-art performance in\nidentifying outliers compared to other recent methods, even with significantly\nlower computation costs. Additionally, we show that our method yields robust\nperformances when combined with privacy-preserving algorithms.",
    "arxiv_id": "2408.09791v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09791v1",
    "abstract_url": "http://arxiv.org/abs/2408.09791v1",
    "primary_category": "stat.ML",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Structure-enhanced Contrastive Learning for Graph Clustering",
    "authors": "Xunlian Wu, Jingqi Hu, Anqi Zhang, Yining Quan, Qiguang Miao, Peng Gang Sun",
    "abstract": "Graph clustering is a crucial task in network analysis with widespread\napplications, focusing on partitioning nodes into distinct groups with stronger\nintra-group connections than inter-group ones. Recently, contrastive learning\nhas achieved significant progress in graph clustering. However, most methods\nsuffer from the following issues: 1) an over-reliance on meticulously designed\ndata augmentation strategies, which can undermine the potential of contrastive\nlearning. 2) overlooking cluster-oriented structural information, particularly\nthe higher-order cluster(community) structure information, which could unveil\nthe mesoscopic cluster structure information of the network. In this study,\nStructure-enhanced Contrastive Learning (SECL) is introduced to addresses these\nissues by leveraging inherent network structures. SECL utilizes a cross-view\ncontrastive learning mechanism to enhance node embeddings without elaborate\ndata augmentations, a structural contrastive learning module for ensuring\nstructural consistency, and a modularity maximization strategy for harnessing\nclustering-oriented information. This comprehensive approach results in robust\nnode representations that greatly enhance clustering performance. Extensive\nexperiments on six datasets confirm SECL's superiority over current\nstate-of-the-art methods, indicating a substantial improvement in the domain of\ngraph clustering.",
    "arxiv_id": "2408.09790v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09790v1",
    "abstract_url": "http://arxiv.org/abs/2408.09790v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Faster Adaptive Decentralized Learning Algorithms",
    "authors": "Feihu Huang, Jianyu Zhao",
    "abstract": "Decentralized learning recently has received increasing attention in machine\nlearning due to its advantages in implementation simplicity and system\nrobustness, data privacy. Meanwhile, the adaptive gradient methods show\nsuperior performances in many machine learning tasks such as training neural\nnetworks. Although some works focus on studying decentralized optimization\nalgorithms with adaptive learning rates, these adaptive decentralized\nalgorithms still suffer from high sample complexity. To fill these gaps, we\npropose a class of faster adaptive decentralized algorithms (i.e., AdaMDOS and\nAdaMDOF) for distributed nonconvex stochastic and finite-sum optimization,\nrespectively. Moreover, we provide a solid convergence analysis framework for\nour methods. In particular, we prove that our AdaMDOS obtains a near-optimal\nsample complexity of $\\tilde{O}(\\epsilon^{-3})$ for finding an\n$\\epsilon$-stationary solution of nonconvex stochastic optimization. Meanwhile,\nour AdaMDOF obtains a near-optimal sample complexity of\n$O(\\sqrt{n}\\epsilon^{-2})$ for finding an $\\epsilon$-stationary solution of\nnonconvex finite-sum optimization, where $n$ denotes the sample size. To the\nbest of our knowledge, our AdaMDOF algorithm is the first adaptive\ndecentralized algorithm for nonconvex finite-sum optimization. Some\nexperimental results demonstrate efficiency of our algorithms.",
    "arxiv_id": "2408.09775v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09775v1",
    "abstract_url": "http://arxiv.org/abs/2408.09775v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Baby Bear: Seeking a Just Right Rating Scale for Scalar Annotations",
    "authors": "Xu Han, Felix Yu, Joao Sedoc, Benjamin Van Durme",
    "abstract": "Our goal is a mechanism for efficiently assigning scalar ratings to each of a\nlarge set of elements. For example, \"what percent positive or negative is this\nproduct review?\" When sample sizes are small, prior work has advocated for\nmethods such as Best Worst Scaling (BWS) as being more robust than direct\nordinal annotation (\"Likert scales\"). Here we first introduce IBWS, which\niteratively collects annotations through Best-Worst Scaling, resulting in\nrobustly ranked crowd-sourced data. While effective, IBWS is too expensive for\nlarge-scale tasks. Using the results of IBWS as a best-desired outcome, we\nevaluate various direct assessment methods to determine what is both\ncost-efficient and best correlating to a large scale BWS annotation strategy.\nFinally, we illustrate in the domains of dialogue and sentiment how these\nannotations can support robust learning-to-rank models.",
    "arxiv_id": "2408.09765v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09765v1",
    "abstract_url": "http://arxiv.org/abs/2408.09765v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Sequential Federated Learning in Hierarchical Architecture on Non-IID Datasets",
    "authors": "Xingrun Yan, Shiyuan Zuo, Rongfei Fan, Han Hu, Li Shen, Puning Zhao, Yong Luo",
    "abstract": "In a real federated learning (FL) system, communication overhead for passing\nmodel parameters between the clients and the parameter server (PS) is often a\nbottleneck. Hierarchical federated learning (HFL) that poses multiple edge\nservers (ESs) between clients and the PS can partially alleviate communication\npressure but still needs the aggregation of model parameters from multiple ESs\nat the PS. To further reduce communication overhead, we bring sequential FL\n(SFL) into HFL for the first time, which removes the central PS and enables the\nmodel training to be completed only through passing the global model between\ntwo adjacent ESs for each iteration, and propose a novel algorithm adaptive to\nsuch a combinational framework, referred to as Fed-CHS. Convergence results are\nderived for strongly convex and non-convex loss functions under various data\nheterogeneity setups, which show comparable convergence performance with the\nalgorithms for HFL or SFL solely. Experimental results provide evidence of the\nsuperiority of our proposed Fed-CHS on both communication overhead saving and\ntest accuracy over baseline methods.",
    "arxiv_id": "2408.09762v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09762v1",
    "abstract_url": "http://arxiv.org/abs/2408.09762v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning",
    "authors": "Jingyu Hu, Weiru Liu, Mengnan Du",
    "abstract": "Recent studies highlight the effectiveness of using in-context learning (ICL)\nto steer large language models (LLMs) in processing tabular data, a challenging\ntask given the structured nature of such data. Despite advancements in\nperformance, the fairness implications of these methods are less understood.\nThis study investigates how varying demonstrations within ICL prompts influence\nthe fairness outcomes of LLMs. Our findings reveal that deliberately including\nminority group samples in prompts significantly boosts fairness without\nsacrificing predictive accuracy. Further experiments demonstrate that the\nproportion of minority to majority samples in demonstrations affects the\ntrade-off between fairness and prediction accuracy. Based on these insights, we\nintroduce a mitigation technique that employs clustering and evolutionary\nstrategies to curate a diverse and representative sample set from the training\ndata. This approach aims to enhance both predictive performance and fairness in\nICL applications. Experimental results validate that our proposed method\ndramatically improves fairness across various metrics, showing its efficacy in\nreal-world scenarios.",
    "arxiv_id": "2408.09757v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09757v1",
    "abstract_url": "http://arxiv.org/abs/2408.09757v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Parallel-in-Time Solutions with Random Projection Neural Networks",
    "authors": "Marta M. Betcke, Lisa Maria Kreusser, Davide Murari",
    "abstract": "This paper considers one of the fundamental parallel-in-time methods for the\nsolution of ordinary differential equations, Parareal, and extends it by\nadopting a neural network as a coarse propagator. We provide a theoretical\nanalysis of the convergence properties of the proposed algorithm and show its\neffectiveness for several examples, including Lorenz and Burgers' equations. In\nour numerical simulations, we further specialize the underpinning neural\narchitecture to Random Projection Neural Networks (RPNNs), a 2-layer neural\nnetwork where the first layer weights are drawn at random rather than\noptimized. This restriction substantially increases the efficiency of fitting\nRPNN's weights in comparison to a standard feedforward network without\nnegatively impacting the accuracy, as demonstrated in the SIR system example.",
    "arxiv_id": "2408.09756v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09756v1",
    "abstract_url": "http://arxiv.org/abs/2408.09756v1",
    "primary_category": "math.NA",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Icing on the Cake: Automatic Code Summarization at Ericsson",
    "authors": "Giriprasad Sridhara, Sujoy Roychowdhury, Sumit Soman, Ranjani H G, Ricardo Britto",
    "abstract": "This paper presents our findings on the automatic summarization of Java\nmethods within Ericsson, a global telecommunications company. We evaluate the\nperformance of an approach called Automatic Semantic Augmentation of Prompts\n(ASAP), which uses a Large Language Model (LLM) to generate leading summary\ncomments for Java methods. ASAP enhances the $LLM's$ prompt context by\nintegrating static program analysis and information retrieval techniques to\nidentify similar exemplar methods along with their developer-written Javadocs,\nand serves as the baseline in our study. In contrast, we explore and compare\nthe performance of four simpler approaches that do not require static program\nanalysis, information retrieval, or the presence of exemplars as in the ASAP\nmethod. Our methods rely solely on the Java method body as input, making them\nlightweight and more suitable for rapid deployment in commercial software\ndevelopment environments. We conducted experiments on an Ericsson software\nproject and replicated the study using two widely-used open-source Java\nprojects, Guava and Elasticsearch, to ensure the reliability of our results.\nPerformance was measured across eight metrics that capture various aspects of\nsimilarity. Notably, one of our simpler approaches performed as well as or\nbetter than the ASAP method on both the Ericsson project and the open-source\nprojects. Additionally, we performed an ablation study to examine the impact of\nmethod names on Javadoc summary generation across our four proposed approaches\nand the ASAP method. By masking the method names and observing the generated\nsummaries, we found that our approaches were statistically significantly less\ninfluenced by the absence of method names compared to the baseline. This\nsuggests that our methods are more robust to variations in method names and may\nderive summaries more comprehensively from the method body than the ASAP\napproach.",
    "arxiv_id": "2408.09735v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09735v1",
    "abstract_url": "http://arxiv.org/abs/2408.09735v1",
    "primary_category": "cs.SE",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "sTransformer: A Modular Approach for Extracting Inter-Sequential and Temporal Information for Time-Series Forecasting",
    "authors": "Jiaheng Yin, Zhengxin Shi, Jianshen Zhang, Xiaomin Lin, Yulin Huang, Yongzhi Qi, Wei Qi",
    "abstract": "In recent years, numerous Transformer-based models have been applied to\nlong-term time-series forecasting (LTSF) tasks. However, recent studies with\nlinear models have questioned their effectiveness, demonstrating that simple\nlinear layers can outperform sophisticated Transformer-based models. In this\nwork, we review and categorize existing Transformer-based models into two main\ntypes: (1) modifications to the model structure and (2) modifications to the\ninput data. The former offers scalability but falls short in capturing\ninter-sequential information, while the latter preprocesses time-series data\nbut is challenging to use as a scalable module. We propose\n$\\textbf{sTransformer}$, which introduces the Sequence and Temporal\nConvolutional Network (STCN) to fully capture both sequential and temporal\ninformation. Additionally, we introduce a Sequence-guided Mask Attention\nmechanism to capture global feature information. Our approach ensures the\ncapture of inter-sequential information while maintaining module scalability.\nWe compare our model with linear models and existing forecasting models on\nlong-term time-series forecasting, achieving new state-of-the-art results. We\nalso conducted experiments on other time-series tasks, achieving strong\nperformance. These demonstrate that Transformer-based structures remain\neffective and our model can serve as a viable baseline for time-series tasks.",
    "arxiv_id": "2408.09723v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09723v1",
    "abstract_url": "http://arxiv.org/abs/2408.09723v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Towards Few-Shot Learning in the Open World: A Review and Beyond",
    "authors": "Hui Xue, Yuexuan An, Yongchun Qin, Wenqian Li, Yixin Wu, Yongjuan Che, Pengfei Fang, Minling Zhang",
    "abstract": "Human intelligence is characterized by our ability to absorb and apply\nknowledge from the world around us, especially in rapidly acquiring new\nconcepts from minimal examples, underpinned by prior knowledge. Few-shot\nlearning (FSL) aims to mimic this capacity by enabling significant\ngeneralizations and transferability. However, traditional FSL frameworks often\nrely on assumptions of clean, complete, and static data, conditions that are\nseldom met in real-world environments. Such assumptions falter in the\ninherently uncertain, incomplete, and dynamic contexts of the open world. This\npaper presents a comprehensive review of recent advancements designed to adapt\nFSL for use in open-world settings. We categorize existing methods into three\ndistinct types of open-world few-shot learning: those involving varying\ninstances, varying classes, and varying distributions. Each category is\ndiscussed in terms of its specific challenges and methods, as well as its\nstrengths and weaknesses. We standardize experimental settings and metric\nbenchmarks across scenarios, and provide a comparative analysis of the\nperformance of various methods. In conclusion, we outline potential future\nresearch directions for this evolving field. It is our hope that this review\nwill catalyze further development of effective solutions to these complex\nchallenges, thereby advancing the field of artificial intelligence.",
    "arxiv_id": "2408.09722v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09722v1",
    "abstract_url": "http://arxiv.org/abs/2408.09722v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Confirmation Bias in Gaussian Mixture Models",
    "authors": "Amnon Balanov, Tamir Bendory, Wasim Huleihel",
    "abstract": "Confirmation bias, the tendency to interpret information in a way that aligns\nwith one's preconceptions, can profoundly impact scientific research, leading\nto conclusions that reflect the researcher's hypotheses even when the\nobservational data do not support them. This issue is especially critical in\nscientific fields involving highly noisy observations, such as cryo-electron\nmicroscopy.\n  This study investigates confirmation bias in Gaussian mixture models. We\nconsider the following experiment: A team of scientists assumes they are\nanalyzing data drawn from a Gaussian mixture model with known signals\n(hypotheses) as centroids. However, in reality, the observations consist\nentirely of noise without any informative structure. The researchers use a\nsingle iteration of the K-means or expectation-maximization algorithms, two\npopular algorithms to estimate the centroids. Despite the observations being\npure noise, we show that these algorithms yield biased estimates that resemble\nthe initial hypotheses, contradicting the unbiased expectation that averaging\nthese noise observations would converge to zero. Namely, the algorithms\ngenerate estimates that mirror the postulated model, although the hypotheses\n(the presumed centroids of the Gaussian mixture) are not evident in the\nobservations. Specifically, among other results, we prove a positive\ncorrelation between the estimates produced by the algorithms and the\ncorresponding hypotheses. We also derive explicit closed-form expressions of\nthe estimates for a finite and infinite number of hypotheses. This study\nunderscores the risks of confirmation bias in low signal-to-noise environments,\nprovides insights into potential pitfalls in scientific methodologies, and\nhighlights the importance of prudent data interpretation.",
    "arxiv_id": "2408.09718v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09718v1",
    "abstract_url": "http://arxiv.org/abs/2408.09718v1",
    "primary_category": "stat.ML",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "HYDEN: Hyperbolic Density Representations for Medical Images and Reports",
    "authors": "Zhi Qiao, Linbin Han, Xiantong Zhen, Jia-Hong Gao, Zhen Qian",
    "abstract": "In light of the inherent entailment relations between images and text,\nhyperbolic point vector embeddings, leveraging the hierarchical modeling\nadvantages of hyperbolic space, have been utilized for visual semantic\nrepresentation learning. However, point vector embedding approaches fail to\naddress the issue of semantic uncertainty, where an image may have multiple\ninterpretations, and text may refer to different images, a phenomenon\nparticularly prevalent in the medical domain. Therefor, we propose\n\\textbf{HYDEN}, a novel hyperbolic density embedding based image-text\nrepresentation learning approach tailored for specific medical domain data.\nThis method integrates text-aware local features alongside global features from\nimages, mapping image-text features to density features in hyperbolic space via\nusing hyperbolic pseudo-Gaussian distributions. An encapsulation loss function\nis employed to model the partial order relations between image-text density\ndistributions. Experimental results demonstrate the interpretability of our\napproach and its superior performance compared to the baseline methods across\nvarious zero-shot tasks and different datasets.",
    "arxiv_id": "2408.09715v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09715v1",
    "abstract_url": "http://arxiv.org/abs/2408.09715v1",
    "primary_category": "cs.AI",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Community-Centric Graph Unlearning",
    "authors": "Yi Li, Shichao Zhang, Guixian Zhang, Debo Cheng",
    "abstract": "Graph unlearning technology has become increasingly important since the\nadvent of the `right to be forgotten' and the growing concerns about the\nprivacy and security of artificial intelligence. Graph unlearning aims to\nquickly eliminate the effects of specific data on graph neural networks (GNNs).\nHowever, most existing deterministic graph unlearning frameworks follow a\nbalanced partition-submodel training-aggregation paradigm, resulting in a lack\nof structural information between subgraph neighborhoods and redundant\nunlearning parameter calculations. To address this issue, we propose a novel\nGraph Structure Mapping Unlearning paradigm (GSMU) and a novel method based on\nit named Community-centric Graph Eraser (CGE). CGE maps community subgraphs to\nnodes, thereby enabling the reconstruction of a node-level unlearning operation\nwithin a reduced mapped graph. CGE makes the exponential reduction of both the\namount of training data and the number of unlearning parameters. Extensive\nexperiments conducted on five real-world datasets and three widely used GNN\nbackbones have verified the high performance and efficiency of our CGE method,\nhighlighting its potential in the field of graph unlearning.",
    "arxiv_id": "2408.09705v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09705v1",
    "abstract_url": "http://arxiv.org/abs/2408.09705v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "LightWeather: Harnessing Absolute Positional Encoding to Efficient and Scalable Global Weather Forecasting",
    "authors": "Yisong Fu, Fei Wang, Zezhi Shao, Chengqing Yu, Yujie Li, Zhao Chen, Zhulin An, Yongjun Xu",
    "abstract": "Recently, Transformers have gained traction in weather forecasting for their\ncapability to capture long-term spatial-temporal correlations. However, their\ncomplex architectures result in large parameter counts and extended training\ntimes, limiting their practical application and scalability to global-scale\nforecasting. This paper aims to explore the key factor for accurate weather\nforecasting and design more efficient solutions. Interestingly, our empirical\nfindings reveal that absolute positional encoding is what really works in\nTransformer-based weather forecasting models, which can explicitly model the\nspatial-temporal correlations even without attention mechanisms. We\ntheoretically prove that its effectiveness stems from the integration of\ngeographical coordinates and real-world time features, which are intrinsically\nrelated to the dynamics of weather. Based on this, we propose LightWeather, a\nlightweight and effective model for station-based global weather forecasting.\nWe employ absolute positional encoding and a simple MLP in place of other\ncomponents of Transformer. With under 30k parameters and less than one hour of\ntraining time, LightWeather achieves state-of-the-art performance on global\nweather datasets compared to other advanced DL methods. The results underscore\nthe superiority of integrating spatial-temporal knowledge over complex\narchitectures, providing novel insights for DL in weather forecasting.",
    "arxiv_id": "2408.09695v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09695v1",
    "abstract_url": "http://arxiv.org/abs/2408.09695v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Regularization for Adversarial Robust Learning",
    "authors": "Jie Wang, Rui Gao, Yao Xie",
    "abstract": "Despite the growing prevalence of artificial neural networks in real-world\napplications, their vulnerability to adversarial attacks remains to be a\nsignificant concern, which motivates us to investigate the robustness of\nmachine learning models. While various heuristics aim to optimize the\ndistributionally robust risk using the $\\infty$-Wasserstein metric, such a\nnotion of robustness frequently encounters computation intractability. To\ntackle the computational challenge, we develop a novel approach to adversarial\ntraining that integrates $\\phi$-divergence regularization into the\ndistributionally robust risk function. This regularization brings a notable\nimprovement in computation compared with the original formulation. We develop\nstochastic gradient methods with biased oracles to solve this problem\nefficiently, achieving the near-optimal sample complexity. Moreover, we\nestablish its regularization effects and demonstrate it is asymptotic\nequivalence to a regularized empirical risk minimization (ERM) framework, by\nconsidering various scaling regimes of the regularization parameter $\\eta$ and\nrobustness level $\\rho$. These regimes yield gradient norm regularization,\nvariance regularization, or a smoothed gradient norm regularization that\ninterpolates between these extremes. We numerically validate our proposed\nmethod in supervised learning, reinforcement learning, and contextual learning\nand showcase its state-of-the-art performance against various adversarial\nattacks.",
    "arxiv_id": "2408.09672v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09672v1",
    "abstract_url": "http://arxiv.org/abs/2408.09672v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Contextual Bandits for Unbounded Context Distributions",
    "authors": "Puning Zhao, Jiafei Wu, Zhe Liu, Huiwen Wu",
    "abstract": "Nonparametric contextual bandit is an important model of sequential decision\nmaking problems. Under $\\alpha$-Tsybakov margin condition, existing research\nhas established a regret bound of\n$\\tilde{O}\\left(T^{1-\\frac{\\alpha+1}{d+2}}\\right)$ for bounded supports.\nHowever, the optimal regret with unbounded contexts has not been analyzed. The\nchallenge of solving contextual bandit problems with unbounded support is to\nachieve both exploration-exploitation tradeoff and bias-variance tradeoff\nsimultaneously. In this paper, we solve the nonparametric contextual bandit\nproblem with unbounded contexts. We propose two nearest neighbor methods\ncombined with UCB exploration. The first method uses a fixed $k$. Our analysis\nshows that this method achieves minimax optimal regret under a weak margin\ncondition and relatively light-tailed context distributions. The second method\nuses adaptive $k$. By a proper data-driven selection of $k$, this method\nachieves an expected regret of\n$\\tilde{O}\\left(T^{1-\\frac{(\\alpha+1)\\beta}{\\alpha+(d+2)\\beta}}+T^{1-\\beta}\\right)$,\nin which $\\beta$ is a parameter describing the tail strength. This bound\nmatches the minimax lower bound up to logarithm factors, indicating that the\nsecond method is approximately optimal.",
    "arxiv_id": "2408.09655v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09655v1",
    "abstract_url": "http://arxiv.org/abs/2408.09655v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Meta-Learning on Augmented Gene Expression Profiles for Enhanced Lung Cancer Detection",
    "authors": "Arya Hadizadeh Moghaddam, Mohsen Nayebi Kerdabadi, Cuncong Zhong, Zijun Yao",
    "abstract": "Gene expression profiles obtained through DNA microarray have proven\nsuccessful in providing critical information for cancer detection classifiers.\nHowever, the limited number of samples in these datasets poses a challenge to\nemploy complex methodologies such as deep neural networks for sophisticated\nanalysis. To address this \"small data\" dilemma, Meta-Learning has been\nintroduced as a solution to enhance the optimization of machine learning models\nby utilizing similar datasets, thereby facilitating a quicker adaptation to\ntarget datasets without the requirement of sufficient samples. In this study,\nwe present a meta-learning-based approach for predicting lung cancer from gene\nexpression profiles. We apply this framework to well-established deep learning\nmethodologies and employ four distinct datasets for the meta-learning tasks,\nwhere one as the target dataset and the rest as source datasets. Our approach\nis evaluated against both traditional and deep learning methodologies, and the\nresults show the superior performance of meta-learning on augmented source data\ncompared to the baselines trained on single datasets. Moreover, we conduct the\ncomparative analysis between meta-learning and transfer learning methodologies\nto highlight the efficiency of the proposed approach in addressing the\nchallenges associated with limited sample sizes. Finally, we incorporate the\nexplainability study to illustrate the distinctiveness of decisions made by\nmeta-learning.",
    "arxiv_id": "2408.09635v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09635v1",
    "abstract_url": "http://arxiv.org/abs/2408.09635v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Branch and Bound to Assess Stability of Regression Coefficients in Uncertain Models",
    "authors": "Brian Knaeble, R. Mitchell Hughes, George Rudolph, Mark A. Abramson, Daniel Razo",
    "abstract": "It can be difficult to interpret a coefficient of an uncertain model. A slope\ncoefficient of a regression model may change as covariates are added or removed\nfrom the model. In the context of high-dimensional data, there are too many\nmodel extensions to check. However, as we show here, it is possible to\nefficiently search, with a branch and bound algorithm, for maximum and minimum\nvalues of that adjusted slope coefficient over a discrete space of regularized\nregression models. Here we introduce our algorithm, along with supporting\nmathematical results, an example application, and a link to our computer code,\nto help researchers summarize high-dimensional data and assess the stability of\nregression coefficients in uncertain models.",
    "arxiv_id": "2408.09634v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09634v1",
    "abstract_url": "http://arxiv.org/abs/2408.09634v1",
    "primary_category": "stat.ME",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "MoDeGPT: Modular Decomposition for Large Language Model Compression",
    "authors": "Chi-Heng Lin, Shangqian Gao, James Seale Smith, Abhishek Patel, Shikhar Tuli, Yilin Shen, Hongxia Jin, Yen-Chang Hsu",
    "abstract": "Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%.",
    "arxiv_id": "2408.09632v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09632v1",
    "abstract_url": "http://arxiv.org/abs/2408.09632v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Attention is a smoothed cubic spline",
    "authors": "Zehua Lai, Lek-Heng Lim, Yucong Liu",
    "abstract": "We highlight a perhaps important but hitherto unobserved insight: The\nattention module in a transformer is a smoothed cubic spline. Viewed in this\nmanner, this mysterious but critical component of a transformer becomes a\nnatural development of an old notion deeply entrenched in classical\napproximation theory. More precisely, we show that with ReLU-activation,\nattention, masked attention, encoder-decoder attention are all cubic splines.\nAs every component in a transformer is constructed out of compositions of\nvarious attention modules (= cubic splines) and feed forward neural networks (=\nlinear splines), all its components -- encoder, decoder, and encoder-decoder\nblocks; multilayered encoders and decoders; the transformer itself -- are cubic\nor higher-order splines. If we assume the Pierce-Birkhoff conjecture, then the\nconverse also holds, i.e., every spline is a ReLU-activated encoder. Since a\nspline is generally just $C^2$, one way to obtain a smoothed $C^\\infty$-version\nis by replacing ReLU with a smooth activation; and if this activation is chosen\nto be SoftMax, we recover the original transformer as proposed by Vaswani et\nal. This insight sheds light on the nature of the transformer by casting it\nentirely in terms of splines, one of the best known and thoroughly understood\nobjects in applied mathematics.",
    "arxiv_id": "2408.09624v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09624v1",
    "abstract_url": "http://arxiv.org/abs/2408.09624v1",
    "primary_category": "cs.AI",
    "published_date": "2024-08-19",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Circuit design in biology and machine learning. I. Random networks and dimensional reduction",
    "authors": "Steven A. Frank",
    "abstract": "A biological circuit is a neural or biochemical cascade, taking inputs and\nproducing outputs. How have biological circuits learned to solve environmental\nchallenges over the history of life? The answer certainly follows Dobzhansky's\nfamous quote that ``nothing in biology makes sense except in the light of\nevolution.'' But that quote leaves out the mechanistic basis by which natural\nselection's trial-and-error learning happens, which is exactly what we have to\nunderstand. How does the learning process that designs biological circuits\nactually work? How much insight can we gain about the form and function of\nbiological circuits by studying the processes that have made those circuits?\nBecause life's circuits must often solve the same problems as those faced by\nmachine learning, such as environmental tracking, homeostatic control,\ndimensional reduction, or classification, we can begin by considering how\nmachine learning designs computational circuits to solve problems. We can then\nask: How much insight do those computational circuits provide about the design\nof biological circuits? How much does biology differ from computers in the\nparticular circuit designs that it uses to solve problems? This article steps\nthrough two classic machine learning models to set the foundation for analyzing\nbroad questions about the design of biological circuits. One insight is the\nsurprising power of randomly connected networks. Another is the central role of\ninternal models of the environment embedded within biological circuits,\nillustrated by a model of dimensional reduction and trend prediction. Overall,\nmany challenges in biology have machine learning analogs, suggesting hypotheses\nabout how biology's circuits are designed.",
    "arxiv_id": "2408.09604v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09604v1",
    "abstract_url": "http://arxiv.org/abs/2408.09604v1",
    "primary_category": "q-bio.PE",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "On the Necessity of World Knowledge for Mitigating Missing Labels in Extreme Classification",
    "authors": "Jatin Prakash, Anirudh Buvanesh, Bishal Santra, Deepak Saini, Sachin Yadav, Jian Jiao, Yashoteja Prabhu, Amit Sharma, Manik Varma",
    "abstract": "Extreme Classification (XC) aims to map a query to the most relevant\ndocuments from a very large document set. XC algorithms used in real-world\napplications learn this mapping from datasets curated from implicit feedback,\nsuch as user clicks. However, these datasets inevitably suffer from missing\nlabels. In this work, we observe that systematic missing labels lead to missing\nknowledge, which is critical for accurately modelling relevance between queries\nand documents. We formally show that this absence of knowledge cannot be\nrecovered using existing methods such as propensity weighting and data\nimputation strategies that solely rely on the training dataset. While LLMs\nprovide an attractive solution to augment the missing knowledge, leveraging\nthem in applications with low latency requirements and large document sets is\nchallenging. To incorporate missing knowledge at scale, we propose SKIM\n(Scalable Knowledge Infusion for Missing Labels), an algorithm that leverages a\ncombination of small LM and abundant unstructured meta-data to effectively\nmitigate the missing label problem. We show the efficacy of our method on\nlarge-scale public datasets through exhaustive unbiased evaluation ranging from\nhuman annotations to simulations inspired from industrial settings. SKIM\noutperforms existing methods on Recall@100 by more than 10 absolute points.\nAdditionally, SKIM scales to proprietary query-ad retrieval datasets containing\n10 million documents, outperforming contemporary methods by 12% in offline\nevaluation and increased ad click-yield by 1.23% in an online A/B test\nconducted on a popular search engine. We release our code, prompts, trained XC\nmodels and finetuned SLMs at: https://github.com/bicycleman15/skim",
    "arxiv_id": "2408.09585v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09585v1",
    "abstract_url": "http://arxiv.org/abs/2408.09585v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Convolutional Conditional Neural Processes",
    "authors": "Wessel P. Bruinsma",
    "abstract": "Neural processes are a family of models which use neural networks to directly\nparametrise a map from data sets to predictions. Directly parametrising this\nmap enables the use of expressive neural networks in small-data problems where\nneural networks would traditionally overfit. Neural processes can produce\nwell-calibrated uncertainties, effectively deal with missing data, and are\nsimple to train. These properties make this family of models appealing for a\nbreadth of applications areas, such as healthcare or environmental sciences.\n  This thesis advances neural processes in three ways.\n  First, we propose convolutional neural processes (ConvNPs). ConvNPs improve\ndata efficiency of neural processes by building in a symmetry called\ntranslation equivariance. ConvNPs rely on convolutional neural networks rather\nthan multi-layer perceptrons.\n  Second, we propose Gaussian neural processes (GNPs). GNPs directly\nparametrise dependencies in the predictions of a neural process. Current\napproaches to modelling dependencies in the predictions depend on a latent\nvariable, which consequently requires approximate inference, undermining the\nsimplicity of the approach.\n  Third, we propose autoregressive conditional neural processes (AR CNPs). AR\nCNPs train a neural process without any modifications to the model or training\nprocedure and, at test time, roll out the model in an autoregressive fashion.\nAR CNPs equip the neural process framework with a new knob where modelling\ncomplexity and computational expense at training time can be traded for\ncomputational expense at test time.\n  In addition to methodological advancements, this thesis also proposes a\nsoftware abstraction that enables a compositional approach to implementing\nneural processes. This approach allows the user to rapidly explore the space of\nneural process models by putting together elementary building blocks in\ndifferent ways.",
    "arxiv_id": "2408.09583v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09583v1",
    "abstract_url": "http://arxiv.org/abs/2408.09583v1",
    "primary_category": "stat.ML",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "A Markov Random Field Multi-Modal Variational AutoEncoder",
    "authors": "Fouad Oubari, Mohamed El Baha, Raphael Meunier, Rodrigue D\u00e9catoire, Mathilde Mougeot",
    "abstract": "Recent advancements in multimodal Variational AutoEncoders (VAEs) have\nhighlighted their potential for modeling complex data from multiple modalities.\nHowever, many existing approaches use relatively straightforward aggregating\nschemes that may not fully capture the complex dynamics present between\ndifferent modalities. This work introduces a novel multimodal VAE that\nincorporates a Markov Random Field (MRF) into both the prior and posterior\ndistributions. This integration aims to capture complex intermodal interactions\nmore effectively. Unlike previous models, our approach is specifically designed\nto model and leverage the intricacies of these relationships, enabling a more\nfaithful representation of multimodal data. Our experiments demonstrate that\nour model performs competitively on the standard PolyMNIST dataset and shows\nsuperior performance in managing complex intermodal dependencies in a specially\ndesigned synthetic dataset, intended to test intricate relationships.",
    "arxiv_id": "2408.09576v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09576v1",
    "abstract_url": "http://arxiv.org/abs/2408.09576v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Say My Name: a Model's Bias Discovery Framework",
    "authors": "Massimiliano Ciranni, Luca Molinaro, Carlo Alberto Barbano, Attilio Fiandrotti, Vittorio Murino, Vito Paolo Pastore, Enzo Tartaglione",
    "abstract": "In the last few years, due to the broad applicability of deep learning to\ndownstream tasks and end-to-end training capabilities, increasingly more\nconcerns about potential biases to specific, non-representative patterns have\nbeen raised. Many works focusing on unsupervised debiasing usually leverage the\ntendency of deep models to learn ``easier'' samples, for example by clustering\nthe latent space to obtain bias pseudo-labels. However, the interpretation of\nsuch pseudo-labels is not trivial, especially for a non-expert end user, as it\ndoes not provide semantic information about the bias features. To address this\nissue, we introduce ``Say My Name'' (SaMyNa), the first tool to identify biases\nwithin deep models semantically. Unlike existing methods, our approach focuses\non biases learned by the model. Our text-based pipeline enhances explainability\nand supports debiasing efforts: applicable during either training or post-hoc\nvalidation, our method can disentangle task-related information and proposes\nitself as a tool to analyze biases. Evaluation on traditional benchmarks\ndemonstrates its effectiveness in detecting biases and even disclaiming them,\nshowcasing its broad applicability for model diagnosis.",
    "arxiv_id": "2408.09570v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09570v1",
    "abstract_url": "http://arxiv.org/abs/2408.09570v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Security Concerns in Quantum Machine Learning as a Service",
    "authors": "Satwik Kundu, Swaroop Ghosh",
    "abstract": "Quantum machine learning (QML) is a category of algorithms that employ\nvariational quantum circuits (VQCs) to tackle machine learning tasks. Recent\ndiscoveries have shown that QML models can effectively generalize from limited\ntraining data samples. This capability has sparked increased interest in\ndeploying these models to address practical, real-world challenges, resulting\nin the emergence of Quantum Machine Learning as a Service (QMLaaS). QMLaaS\nrepresents a hybrid model that utilizes both classical and quantum computing\nresources. Classical computers play a crucial role in this setup, handling\ninitial pre-processing and subsequent post-processing of data to compensate for\nthe current limitations of quantum hardware. Since this is a new area, very\nlittle work exists to paint the whole picture of QMLaaS in the context of known\nsecurity threats in the domain of classical and quantum machine learning. This\nSoK paper is aimed to bridge this gap by outlining the complete QMLaaS\nworkflow, which encompasses both the training and inference phases and\nhighlighting significant security concerns involving untrusted classical or\nquantum providers. QML models contain several sensitive assets, such as the\nmodel architecture, training/testing data, encoding techniques, and trained\nparameters. Unauthorized access to these components could compromise the\nmodel's integrity and lead to intellectual property (IP) theft. We pinpoint the\ncritical security issues that must be considered to pave the way for a secure\nQMLaaS deployment.",
    "arxiv_id": "2408.09562v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09562v1",
    "abstract_url": "http://arxiv.org/abs/2408.09562v1",
    "primary_category": "quant-ph",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Addressing Heterogeneity in Federated Learning: Challenges and Solutions for a Shared Production Environment",
    "authors": "Tatjana Legler, Vinit Hegiste, Ahmed Anwar, Martin Ruskowski",
    "abstract": "Federated learning (FL) has emerged as a promising approach to training\nmachine learning models across decentralized data sources while preserving data\nprivacy, particularly in manufacturing and shared production environments.\nHowever, the presence of data heterogeneity variations in data distribution,\nquality, and volume across different or clients and production sites, poses\nsignificant challenges to the effectiveness and efficiency of FL. This paper\nprovides a comprehensive overview of heterogeneity in FL within the context of\nmanufacturing, detailing the types and sources of heterogeneity, including\nnon-independent and identically distributed (non-IID) data, unbalanced data,\nvariable data quality, and statistical heterogeneity. We discuss the impact of\nthese types of heterogeneity on model training and review current methodologies\nfor mitigating their adverse effects. These methodologies include personalized\nand customized models, robust aggregation techniques, and client selection\ntechniques. By synthesizing existing research and proposing new strategies,\nthis paper aims to provide insight for effectively managing data heterogeneity\nin FL, enhancing model robustness, and ensuring fair and efficient training\nacross diverse environments. Future research directions are also identified,\nhighlighting the need for adaptive and scalable solutions to further improve\nthe FL paradigm in the context of Industry 4.0.",
    "arxiv_id": "2408.09556v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09556v1",
    "abstract_url": "http://arxiv.org/abs/2408.09556v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Seamless Integration: Sampling Strategies in Federated Learning Systems",
    "authors": "Tatjana Legler, Vinit Hegiste, Martin Ruskowski",
    "abstract": "Federated Learning (FL) represents a paradigm shift in the field of machine\nlearning, offering an approach for a decentralized training of models across a\nmultitude of devices while maintaining the privacy of local data. However, the\ndynamic nature of FL systems, characterized by the ongoing incorporation of new\nclients with potentially diverse data distributions and computational\ncapabilities, poses a significant challenge to the stability and efficiency of\nthese distributed learning networks. The seamless integration of new clients is\nimperative to sustain and enhance the performance and robustness of FL systems.\nThis paper looks into the complexities of integrating new clients into existing\nFL systems and explores how data heterogeneity and varying data distribution\n(not independent and identically distributed) among them can affect model\ntraining, system efficiency, scalability and stability. Despite these\nchallenges, the integration of new clients into FL systems presents\nopportunities to enhance data diversity, improve learning performance, and\nleverage distributed computational power. In contrast to other fields of\napplication such as the distributed optimization of word predictions on Gboard\n(where federated learning once originated), there are usually only a few\nclients in the production environment, which is why information from each new\nclient becomes all the more valuable. This paper outlines strategies for\neffective client selection strategies and solutions for ensuring system\nscalability and stability. Using the example of images from optical quality\ninspection, it offers insights into practical approaches. In conclusion, this\npaper proposes that addressing the challenges presented by new client\nintegration is crucial to the advancement and efficiency of distributed\nlearning networks, thus paving the way for the adoption of Federated Learning\nin production environments.",
    "arxiv_id": "2408.09545v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09545v1",
    "abstract_url": "http://arxiv.org/abs/2408.09545v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Byzantine-resilient Federated Learning Employing Normalized Gradients on Non-IID Datasets",
    "authors": "Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Li Shen, Puning Zhao, Jie Xu, Han Hu",
    "abstract": "In practical federated learning (FL) systems, the presence of malicious\nByzantine attacks and data heterogeneity often introduces biases into the\nlearning process. However, existing Byzantine-robust methods typically only\nachieve a compromise between adaptability to different loss function types\n(including both strongly convex and non-convex) and robustness to heterogeneous\ndatasets, but with non-zero optimality gap. Moreover, this compromise often\ncomes at the cost of high computational complexity for aggregation, which\nsignificantly slows down the training speed. To address this challenge, we\npropose a federated learning approach called Federated Normalized Gradients\nAlgorithm (Fed-NGA). Fed-NGA simply normalizes the uploaded local gradients to\nbe unit vectors before aggregation, achieving a time complexity of\n$\\mathcal{O}(pM)$, where $p$ represents the dimension of model parameters and\n$M$ is the number of participating clients. This complexity scale achieves the\nbest level among all the existing Byzantine-robust methods. Furthermore,\nthrough rigorous proof, we demonstrate that Fed-NGA transcends the trade-off\nbetween adaptability to loss function type and data heterogeneity and the\nlimitation of non-zero optimality gap in existing literature. Specifically,\nFed-NGA can adapt to both non-convex loss functions and non-IID datasets\nsimultaneously, with zero optimality gap at a rate of $\\mathcal{O}\n(1/T^{\\frac{1}{2} - \\delta})$, where T is the iteration number and $\\delta \\in\n(0,\\frac{1}{2})$. In cases where the loss function is strongly convex, the zero\noptimality gap achieving rate can be improved to be linear. Experimental\nresults provide evidence of the superiority of our proposed Fed-NGA on time\ncomplexity and convergence performance over baseline methods.",
    "arxiv_id": "2408.09539v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09539v1",
    "abstract_url": "http://arxiv.org/abs/2408.09539v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Sample-Optimal Large-Scale Optimal Subset Selection",
    "authors": "Zaile Li, Weiwei Fan, L. Jeff Hong",
    "abstract": "Ranking and selection (R&S) conventionally aims to select the unique best\nalternative with the largest mean performance from a finite set of\nalternatives. However, for better supporting decision making, it may be more\ninformative to deliver a small menu of alternatives whose mean performances are\namong the top $m$. Such problem, called optimal subset selection (OSS), is\ngenerally more challenging to address than the conventional R&S. This challenge\nbecomes even more significant when the number of alternatives is considerably\nlarge. Thus, the focus of this paper is on addressing the large-scale OSS\nproblem. To achieve this goal, we design a top-$m$ greedy selection mechanism\nthat keeps sampling the current top $m$ alternatives with top $m$ running\nsample means and propose the explore-first top-$m$ greedy (EFG-$m$) procedure.\nThrough an extended boundary-crossing framework, we prove that the EFG-$m$\nprocedure is both sample optimal and consistent in terms of the probability of\ngood selection, confirming its effectiveness in solving large-scale OSS\nproblem. Surprisingly, we also demonstrate that the EFG-$m$ procedure enables\nto achieve an indifference-based ranking within the selected subset of\nalternatives at no extra cost. This is highly beneficial as it delivers deeper\ninsights to decision-makers, enabling more informed decision-makings. Lastly,\nnumerical experiments validate our results and demonstrate the efficiency of\nour procedures.",
    "arxiv_id": "2408.09537v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09537v1",
    "abstract_url": "http://arxiv.org/abs/2408.09537v1",
    "primary_category": "stat.ML",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Deep Limit Model-free Prediction in Regression",
    "authors": "Kejin Wu, Dimitris N. Politis",
    "abstract": "In this paper, we provide a novel Model-free approach based on Deep Neural\nNetwork (DNN) to accomplish point prediction and prediction interval under a\ngeneral regression setting. Usually, people rely on parametric or\nnon-parametric models to bridge dependent and independent variables (Y and X).\nHowever, this classical method relies heavily on the correct model\nspecification. Even for the non-parametric approach, some additive form is\noften assumed. A newly proposed Model-free prediction principle sheds light on\na prediction procedure without any model assumption. Previous work regarding\nthis principle has shown better performance than other standard alternatives.\nRecently, DNN, one of the machine learning methods, has received increasing\nattention due to its great performance in practice. Guided by the Model-free\nprediction idea, we attempt to apply a fully connected forward DNN to map X and\nsome appropriate reference random variable Z to Y. The targeted DNN is trained\nby minimizing a specially designed loss function so that the randomness of Y\nconditional on X is outsourced to Z through the trained DNN. Our method is more\nstable and accurate compared to other DNN-based counterparts, especially for\noptimal point predictions. With a specific prediction procedure, our prediction\ninterval can capture the estimation variability so that it can render a better\ncoverage rate for finite sample cases. The superior performance of our method\nis verified by simulation and empirical studies.",
    "arxiv_id": "2408.09532v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09532v1",
    "abstract_url": "http://arxiv.org/abs/2408.09532v1",
    "primary_category": "stat.ML",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Fine-gained air quality inference based on low-quality sensing data using self-supervised learning",
    "authors": "Meng Xu, Ke Han, Weijian Hu, Wen Ji",
    "abstract": "Fine-grained air quality (AQ) mapping is made possible by the proliferation\nof cheap AQ micro-stations (MSs). However, their measurements are often\ninaccurate and sensitive to local disturbances, in contrast to standardized\nstations (SSs) that provide accurate readings but fall short in number. To\nsimultaneously address the issues of low data quality (MSs) and high label\nsparsity (SSs), a multi-task spatio-temporal network (MTSTN) is proposed, which\nemploys self-supervised learning to utilize massive unlabeled data, aided by\nseasonal and trend decomposition of MS data offering reliable information as\nfeatures. The MTSTN is applied to infer NO$_2$, O$_3$ and PM$_{2.5}$\nconcentrations in a 250 km$^2$ area in Chengdu, China, at a resolution of\n500m$\\times$500m$\\times$1hr. Data from 55 SSs and 323 MSs were used, along with\nmeteorological, traffic, geographic and timestamp data as features. The MTSTN\nexcels in accuracy compared to several benchmarks, and its performance is\ngreatly enhanced by utilizing low-quality MS data. A series of ablation and\npressure tests demonstrate the results' robustness and interpretability,\nshowcasing the MTSTN's practical value for accurate and affordable AQ\ninference.",
    "arxiv_id": "2408.09526v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09526v1",
    "abstract_url": "http://arxiv.org/abs/2408.09526v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Enhancing Quantum Memory Lifetime with Measurement-Free Local Error Correction and Reinforcement Learning",
    "authors": "Mincheol Park, Nishad Maskara, Marcin Kalinowski, Mikhail D. Lukin",
    "abstract": "Reliable quantum computation requires systematic identification and\ncorrection of errors that occur and accumulate in quantum hardware. To diagnose\nand correct such errors, standard quantum error-correcting protocols utilize\n$\\textit{global}$ error information across the system obtained by mid-circuit\nreadout of ancillary qubits. We investigate circuit-level error-correcting\nprotocols that are measurement-free and based on $\\textit{local}$ error\ninformation. Such a local error correction (LEC) circuit consists of faulty\nmulti-qubit gates to perform both syndrome extraction and ancilla-controlled\nerror removal. We develop and implement a reinforcement learning framework that\ntakes a fixed set of faulty gates as inputs and outputs an optimized LEC\ncircuit. To evaluate this approach, we quantitatively characterize an extension\nof logical qubit lifetime by a noisy LEC circuit. For the 2D classical Ising\nmodel and 4D toric code, our optimized LEC circuit performs better at extending\na memory lifetime compared to a conventional LEC circuit based on Toom's rule\nin a sub-threshold gate error regime. We further show that such circuits can be\nused to reduce the rate of mid-circuit readouts to preserve a 2D toric code\nmemory. Finally, we discuss the application of the LEC protocol on dissipative\npreparation of quantum states with topological phases.",
    "arxiv_id": "2408.09524v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09524v1",
    "abstract_url": "http://arxiv.org/abs/2408.09524v1",
    "primary_category": "quant-ph",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "A Unified Framework for Interpretable Transformers Using PDEs and Information Theory",
    "authors": "Yukun Zhang",
    "abstract": "This paper presents a novel unified theoretical framework for understanding\nTransformer architectures by integrating Partial Differential Equations (PDEs),\nNeural Information Flow Theory, and Information Bottleneck Theory. We model\nTransformer information dynamics as a continuous PDE process, encompassing\ndiffusion, self-attention, and nonlinear residual components. Our comprehensive\nexperiments across image and text modalities demonstrate that the PDE model\neffectively captures key aspects of Transformer behavior, achieving high\nsimilarity (cosine similarity > 0.98) with Transformer attention distributions\nacross all layers. While the model excels in replicating general information\nflow patterns, it shows limitations in fully capturing complex, non-linear\ntransformations. This work provides crucial theoretical insights into\nTransformer mechanisms, offering a foundation for future optimizations in deep\nlearning architectural design. We discuss the implications of our findings,\npotential applications in model interpretability and efficiency, and outline\ndirections for enhancing PDE models to better mimic the intricate behaviors\nobserved in Transformers, paving the way for more transparent and optimized AI\nsystems.",
    "arxiv_id": "2408.09523v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09523v1",
    "abstract_url": "http://arxiv.org/abs/2408.09523v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Out-of-distribution generalization via composition: a lens through induction heads in Transformers",
    "authors": "Jiajun Song, Zhuoyan Xu, Yiqiao Zhong",
    "abstract": "Large language models (LLMs) such as GPT-4 sometimes appear to be creative,\nsolving novel tasks often with a few demonstrations in the prompt. These tasks\nrequire the models to generalize on distributions different from those from\ntraining data -- which is known as out-of-distribution (OOD) generalization.\nDespite the tremendous success of LLMs, how they approach OOD generalization\nremains an open and underexplored question. We examine OOD generalization in\nsettings where instances are generated according to hidden rules, including\nin-context learning with symbolic reasoning. Models are required to infer the\nhidden rules behind input prompts without any fine-tuning.\n  We empirically examined the training dynamics of Transformers on a synthetic\nexample and conducted extensive experiments on a variety of pretrained LLMs,\nfocusing on a type of components known as induction heads. We found that OOD\ngeneralization and composition are tied together -- models can learn rules by\ncomposing two self-attention layers, thereby achieving OOD generalization.\nFurthermore, a shared latent subspace in the embedding (or feature) space acts\nas a bridge for composition by aligning early layers and later layers, which we\nrefer to as the common bridge representation hypothesis.",
    "arxiv_id": "2408.09503v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09503v1",
    "abstract_url": "http://arxiv.org/abs/2408.09503v1",
    "primary_category": "cs.CL",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Directed Exploration in Reinforcement Learning from Linear Temporal Logic",
    "authors": "Marco Bagatella, Andreas Krause, Georg Martius",
    "abstract": "Linear temporal logic (LTL) is a powerful language for task specification in\nreinforcement learning, as it allows describing objectives beyond the\nexpressivity of conventional discounted return formulations. Nonetheless,\nrecent works have shown that LTL formulas can be translated into a variable\nrewarding and discounting scheme, whose optimization produces a policy\nmaximizing a lower bound on the probability of formula satisfaction. However,\nthe synthesized reward signal remains fundamentally sparse, making exploration\nchallenging. We aim to overcome this limitation, which can prevent current\nalgorithms from scaling beyond low-dimensional, short-horizon problems. We show\nhow better exploration can be achieved by further leveraging the LTL\nspecification and casting its corresponding Limit Deterministic B\\\"uchi\nAutomaton (LDBA) as a Markov reward process, thus enabling a form of high-level\nvalue estimation. By taking a Bayesian perspective over LDBA dynamics and\nproposing a suitable prior distribution, we show that the values estimated\nthrough this procedure can be treated as a shaping potential and mapped to\ninformative intrinsic rewards. Empirically, we demonstrate applications of our\nmethod from tabular settings to high-dimensional continuous systems, which have\nso far represented a significant challenge for LTL-based reinforcement learning\nalgorithms.",
    "arxiv_id": "2408.09495v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09495v1",
    "abstract_url": "http://arxiv.org/abs/2408.09495v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Ancestral Reinforcement Learning: Unifying Zeroth-Order Optimization and Genetic Algorithms for Reinforcement Learning",
    "authors": "So Nakashima, Tetsuya J. Kobayashi",
    "abstract": "Reinforcement Learning (RL) offers a fundamental framework for discovering\noptimal action strategies through interactions within unknown environments.\nRecent advancement have shown that the performance and applicability of RL can\nsignificantly be enhanced by exploiting a population of agents in various ways.\nZeroth-Order Optimization (ZOO) leverages an agent population to estimate the\ngradient of the objective function, enabling robust policy refinement even in\nnon-differentiable scenarios. As another application, Genetic Algorithms (GA)\nboosts the exploration of policy landscapes by mutational generation of policy\ndiversity in an agent population and its refinement by selection. A natural\nquestion is whether we can have the best of two worlds that the agent\npopulation can have. In this work, we propose Ancestral Reinforcement Learning\n(ARL), which synergistically combines the robust gradient estimation of ZOO\nwith the exploratory power of GA. The key idea in ARL is that each agent within\na population infers gradient by exploiting the history of its ancestors, i.e.,\nthe ancestor population in the past, while maintaining the diversity of\npolicies in the current population as in GA. We also theoretically reveal that\nthe populational search in ARL implicitly induces the KL-regularization of the\nobjective function, resulting in the enhanced exploration. Our results extend\nthe applicability of populational algorithms for RL.",
    "arxiv_id": "2408.09493v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09493v1",
    "abstract_url": "http://arxiv.org/abs/2408.09493v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Leveraging Invariant Principle for Heterophilic Graph Structure Distribution Shifts",
    "authors": "Jinluan Yang, Zhengyu Chen, Teng Xiao, Wenqiao Zhang, Yong Lin, Kun Kuang",
    "abstract": "Heterophilic Graph Neural Networks (HGNNs) have shown promising results for\nsemi-supervised learning tasks on graphs. Notably, most real-world heterophilic\ngraphs are composed of a mixture of nodes with different neighbor patterns,\nexhibiting local node-level homophilic and heterophilic structures. However,\nexisting works are only devoted to designing better HGNN backbones or\narchitectures for node classification tasks on heterophilic and homophilic\ngraph benchmarks simultaneously, and their analyses of HGNN performance with\nrespect to nodes are only based on the determined data distribution without\nexploring the effect caused by this structural difference between training and\ntesting nodes. How to learn invariant node representations on heterophilic\ngraphs to handle this structure difference or distribution shifts remains\nunexplored. In this paper, we first discuss the limitations of previous\ngraph-based invariant learning methods from the perspective of data\naugmentation. Then, we propose \\textbf{HEI}, a framework capable of generating\ninvariant node representations through incorporating heterophily information to\ninfer latent environments without augmentation, which are then used for\ninvariant prediction, under heterophilic graph structure distribution shifts.\nWe theoretically show that our proposed method can achieve guaranteed\nperformance under heterophilic graph structure distribution shifts. Extensive\nexperiments on various benchmarks and backbones can also demonstrate the\neffectiveness of our method compared with existing state-of-the-art baselines.",
    "arxiv_id": "2408.09490v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09490v1",
    "abstract_url": "http://arxiv.org/abs/2408.09490v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Mitigating Noise Detriment in Differentially Private Federated Learning with Model Pre-training",
    "authors": "Huitong Jin, Yipeng Zhou, Laizhong Cui, Quan Z. Sheng",
    "abstract": "Pre-training exploits public datasets to pre-train an advanced machine\nlearning model, so that the model can be easily tuned to adapt to various\ndownstream tasks. Pre-training has been extensively explored to mitigate\ncomputation and communication resource consumption. Inspired by these\nadvantages, we are the first to explore how model pre-training can mitigate\nnoise detriment in differentially private federated learning (DPFL). DPFL is\nupgraded from federated learning (FL), the de-facto standard for privacy\npreservation when training the model across multiple clients owning private\ndata. DPFL introduces differentially private (DP) noises to obfuscate model\ngradients exposed in FL, which however can considerably impair model accuracy.\nIn our work, we compare head fine-tuning (HT) and full fine-tuning (FT), which\nare based on pre-training, with scratch training (ST) in DPFL through a\ncomprehensive empirical study. Our experiments tune pre-trained models\n(obtained by pre-training on ImageNet-1K) with CIFAR-10, CHMNIST and\nFashion-MNIST (FMNIST) datasets, respectively. The results demonstrate that HT\nand FT can significantly mitigate noise influence by diminishing gradient\nexposure times. In particular, HT outperforms FT when the privacy budget is\ntight or the model size is large. Visualization and explanation study further\nsubstantiates our findings. Our pioneering study introduces a new perspective\non enhancing DPFL and expanding its practical applications.",
    "arxiv_id": "2408.09478v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09478v1",
    "abstract_url": "http://arxiv.org/abs/2408.09478v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Advances in Multiple Instance Learning for Whole Slide Image Analysis: Techniques, Challenges, and Future Directions",
    "authors": "Jun Wang, Yu Mao, Nan Guan, Chun Jason Xue",
    "abstract": "Whole slide images (WSIs) are gigapixel-scale digital images of H\\&E-stained\ntissue samples widely used in pathology. The substantial size and complexity of\nWSIs pose unique analytical challenges. Multiple Instance Learning (MIL) has\nemerged as a powerful approach for addressing these challenges, particularly in\ncancer classification and detection. This survey provides a comprehensive\noverview of the challenges and methodologies associated with applying MIL to\nWSI analysis, including attention mechanisms, pseudo-labeling, transformers,\npooling functions, and graph neural networks. Additionally, it explores the\npotential of MIL in discovering cancer cell morphology, constructing\ninterpretable machine learning models, and quantifying cancer grading. By\nsummarizing the current challenges, methodologies, and potential applications\nof MIL in WSI analysis, this survey aims to inform researchers about the state\nof the field and inspire future research directions.",
    "arxiv_id": "2408.09476v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09476v1",
    "abstract_url": "http://arxiv.org/abs/2408.09476v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Advancements in Molecular Property Prediction: A Survey of Single and Multimodal Approaches",
    "authors": "Tanya Liyaqat, Tanvir Ahmad, Chandni Saxena",
    "abstract": "Molecular Property Prediction (MPP) plays a pivotal role across diverse\ndomains, spanning drug discovery, material science, and environmental\nchemistry. Fueled by the exponential growth of chemical data and the evolution\nof artificial intelligence, recent years have witnessed remarkable strides in\nMPP. However, the multifaceted nature of molecular data, such as molecular\nstructures, SMILES notation, and molecular images, continues to pose a\nfundamental challenge in its effective representation. To address this,\nrepresentation learning techniques are instrumental as they acquire informative\nand interpretable representations of molecular data. This article explores\nrecent AI/-based approaches in MPP, focusing on both single and multiple\nmodality representation techniques. It provides an overview of various molecule\nrepresentations and encoding schemes, categorizes MPP methods by their use of\nmodalities, and outlines datasets and tools available for feature generation.\nThe article also analyzes the performance of recent methods and suggests future\nresearch directions to advance the field of MPP.",
    "arxiv_id": "2408.09461v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09461v1",
    "abstract_url": "http://arxiv.org/abs/2408.09461v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "In-Memory Learning Automata Architecture using Y-Flash Cell",
    "authors": "Omar Ghazal, Tian Lan, Shalman Ojukwu, Komal Krishnamurthy, Alex Yakovlev, Rishad Shafik",
    "abstract": "The modern implementation of machine learning architectures faces significant\nchallenges due to frequent data transfer between memory and processing units.\nIn-memory computing, primarily through memristor-based analog computing, offers\na promising solution to overcome this von Neumann bottleneck. In this\ntechnology, data processing and storage are located inside the memory. Here, we\nintroduce a novel approach that utilizes floating-gate Y-Flash memristive\ndevices manufactured with a standard 180 nm CMOS process. These devices offer\nattractive features, including analog tunability and moderate device-to-device\nvariation; such characteristics are essential for reliable decision-making in\nML applications. This paper uses a new machine learning algorithm, the Tsetlin\nMachine (TM), for in-memory processing architecture. The TM's learning element,\nAutomaton, is mapped into a single Y-Flash cell, where the Automaton's range is\ntransferred into the Y-Flash's conductance scope. Through comprehensive\nsimulations, the proposed hardware implementation of the learning automata,\nparticularly for Tsetlin machines, has demonstrated enhanced scalability and\non-edge learning capabilities.",
    "arxiv_id": "2408.09456v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09456v1",
    "abstract_url": "http://arxiv.org/abs/2408.09456v1",
    "primary_category": "cs.AR",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Reparameterized Multi-Resolution Convolutions for Long Sequence Modelling",
    "authors": "Harry Jake Cunningham, Giorgio Giannone, Mingtian Zhang, Marc Peter Deisenroth",
    "abstract": "Global convolutions have shown increasing promise as powerful general-purpose\nsequence models. However, training long convolutions is challenging, and kernel\nparameterizations must be able to learn long-range dependencies without\noverfitting. This work introduces reparameterized multi-resolution convolutions\n($\\texttt{MRConv}$), a novel approach to parameterizing global convolutional\nkernels for long-sequence modelling. By leveraging multi-resolution\nconvolutions, incorporating structural reparameterization and introducing\nlearnable kernel decay, $\\texttt{MRConv}$ learns expressive long-range kernels\nthat perform well across various data modalities. Our experiments demonstrate\nstate-of-the-art performance on the Long Range Arena, Sequential CIFAR, and\nSpeech Commands tasks among convolution models and linear-time transformers.\nMoreover, we report improved performance on ImageNet classification by\nreplacing 2D convolutions with 1D $\\texttt{MRConv}$ layers.",
    "arxiv_id": "2408.09453v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09453v1",
    "abstract_url": "http://arxiv.org/abs/2408.09453v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "GraphSPNs: Sum-Product Networks Benefit From Canonical Orderings",
    "authors": "Milan Pape\u017e, Martin Rektoris, V\u00e1clav \u0160m\u00eddl, Tom\u00e1\u0161 Pevn\u00fd",
    "abstract": "Deep generative models have recently made a remarkable progress in capturing\ncomplex probability distributions over graphs. However, they are intractable\nand thus unable to answer even the most basic probabilistic inference queries\nwithout resorting to approximations. Therefore, we propose graph sum-product\nnetworks (GraphSPNs), a tractable deep generative model which provides exact\nand efficient inference over (arbitrary parts of) graphs. We investigate\ndifferent principles to make SPNs permutation invariant. We demonstrate that\nGraphSPNs are able to (conditionally) generate novel and chemically valid\nmolecular graphs, being competitive to, and sometimes even better than,\nexisting intractable models. We find out that (Graph)SPNs benefit from ensuring\nthe permutation invariance via canonical ordering.",
    "arxiv_id": "2408.09451v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09451v1",
    "abstract_url": "http://arxiv.org/abs/2408.09451v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Attention Is Not What You Need: Revisiting Multi-Instance Learning for Whole Slide Image Classification",
    "authors": "Xin Liu, Weijia Zhang, Min-Ling Zhang",
    "abstract": "Although attention-based multi-instance learning algorithms have achieved\nimpressive performances on slide-level whole slide image (WSI) classification\ntasks, they are prone to mistakenly focus on irrelevant patterns such as\nstaining conditions and tissue morphology, leading to incorrect patch-level\npredictions and unreliable interpretability. Moreover, these attention-based\nMIL algorithms tend to focus on salient instances and struggle to recognize\nhard-to-classify instances. In this paper, we first demonstrate that\nattention-based WSI classification methods do not adhere to the standard MIL\nassumptions. From the standard MIL assumptions, we propose a surprisingly\nsimple yet effective instance-based MIL method for WSI classification\n(FocusMIL) based on max-pooling and forward amortized variational inference. We\nargue that synergizing the standard MIL assumption with variational inference\nencourages the model to focus on tumour morphology instead of spurious\ncorrelations. Our experimental evaluations show that FocusMIL significantly\noutperforms the baselines in patch-level classification tasks on the Camelyon16\nand TCGA-NSCLC benchmarks. Visualization results show that our method also\nachieves better classification boundaries for identifying hard instances and\nmitigates the effect of spurious correlations between bags and labels.",
    "arxiv_id": "2408.09449v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09449v1",
    "abstract_url": "http://arxiv.org/abs/2408.09449v1",
    "primary_category": "cs.CV",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  },
  {
    "title": "Parameterized Physics-informed Neural Networks for Parameterized PDEs",
    "authors": "Woojin Cho, Minju Jo, Haksoo Lim, Kookjin Lee, Dongeun Lee, Sanghyun Hong, Noseong Park",
    "abstract": "Complex physical systems are often described by partial differential\nequations (PDEs) that depend on parameters such as the Reynolds number in fluid\nmechanics. In applications such as design optimization or uncertainty\nquantification, solutions of those PDEs need to be evaluated at numerous points\nin the parameter space. While physics-informed neural networks (PINNs) have\nemerged as a new strong competitor as a surrogate, their usage in this scenario\nremains underexplored due to the inherent need for repetitive and\ntime-consuming training. In this paper, we address this problem by proposing a\nnovel extension, parameterized physics-informed neural networks (P$^2$INNs).\nP$^2$INNs enable modeling the solutions of parameterized PDEs via explicitly\nencoding a latent representation of PDE parameters. With the extensive\nempirical evaluation, we demonstrate that P$^2$INNs outperform the baselines\nboth in accuracy and parameter efficiency on benchmark 1D and 2D parameterized\nPDEs and are also effective in overcoming the known \"failure modes\".",
    "arxiv_id": "2408.09446v1",
    "pdf_url": "http://arxiv.org/pdf/2408.09446v1",
    "abstract_url": "http://arxiv.org/abs/2408.09446v1",
    "primary_category": "cs.LG",
    "published_date": "2024-08-18",
    "votes": 0,
    "prompt": "LLM reasoning",
    "model": "gpt-4-turbo"
  }
]